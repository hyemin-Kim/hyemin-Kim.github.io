<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Python &gt;&gt; sklearn - (4) 앙상블 (Ensemble) | Hyemin Kim</title><meta name="keywords" content="Python,sklearn,Machine Learning,앙상블"><meta name="author" content="Hyemin Kim"><meta name="copyright" content="Hyemin Kim"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Voting, Bagging, Boosting, Stacking, Cross Validation">
<meta property="og:type" content="article">
<meta property="og:title" content="Python &gt;&gt; sklearn - (4) 앙상블 (Ensemble)">
<meta property="og:url" content="https://hyemin-kim.github.io/2020/08/04/S-Python-sklearn4/index.html">
<meta property="og:site_name" content="Hyemin Kim">
<meta property="og:description" content="Voting, Bagging, Boosting, Stacking, Cross Validation">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://ohiing.com/wp-content/uploads/2020/02/scikit-learn-2.jpg">
<meta property="article:published_time" content="2020-08-04T11:40:35.000Z">
<meta property="article:modified_time" content="2020-11-06T05:20:39.672Z">
<meta property="article:author" content="Hyemin Kim">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="sklearn">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="앙상블">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ohiing.com/wp-content/uploads/2020/02/scikit-learn-2.jpg"><link rel="shortcut icon" href="/img/favicon_m.png"><link rel="canonical" href="https://hyemin-kim.github.io/2020/08/04/S-Python-sklearn4/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-XKJP2G3X9E"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XKJP2G3X9E');
</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '4.2.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":false,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: Hyemin Kim","link":"Link: ","source":"Source: Hyemin Kim","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-11-06 14:20:39'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Hyemin Kim" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">102</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">36</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">29</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#앙상블-ensemble"><span class="toc-text"> 앙상블 (Ensemble)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-데이터-셋"><span class="toc-text"> 0. 데이터 셋</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#0-1-데이터-로드"><span class="toc-text"> 0-1. 데이터 로드</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-2-데이터프레임-만들기"><span class="toc-text"> 0-2. 데이터프레임 만들기</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-training-set-test-set-나누기"><span class="toc-text"> 1. Training set &#x2F; Test set 나누기</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-평가-지표-만들기"><span class="toc-text"> 2. 평가 지표 만들기</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-평가-지표"><span class="toc-text"> 2-1. 평가 지표</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-모델-성능-확인을-위한-함수"><span class="toc-text"> 2-2. 모델 성능 확인을 위한 함수</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-단일-회귀-모델-지난-시간"><span class="toc-text"> 3. 단일 회귀 모델 (지난 시간)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-linear-regression"><span class="toc-text"> (1)  Linear Regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-ridge"><span class="toc-text"> (2)  Ridge</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-lasso"><span class="toc-text"> (3)  LASSO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-elasticnet"><span class="toc-text"> (4) ElasticNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-with-standard-scaling"><span class="toc-text"> (5) With Standard Scaling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-polynomial-features"><span class="toc-text"> (6) Polynomial Features</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-앙상블-ensemble-알고리즘"><span class="toc-text"> 4. 앙상블 (Ensemble)  알고리즘</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-보팅-voting"><span class="toc-text"> 4-1. 보팅 (Voting)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#회귀-regression"><span class="toc-text"> &gt;&gt; 회귀 (Regression)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#분류-classification"><span class="toc-text"> &gt;&gt; 분류 (Classification)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-배깅-bagging"><span class="toc-text"> 4-2. 배깅 (Bagging)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#random-forest"><span class="toc-text"> &gt;&gt; Random Forest</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-부스팅-boosting"><span class="toc-text"> 4-3. 부스팅 (Boosting)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-gradient-boost"><span class="toc-text"> 4-3-1. Gradient Boost</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-xgboost"><span class="toc-text"> 4-3-2. XGBoost</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-3-lightgbm"><span class="toc-text"> 4-3-3. LightGBM</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-스태킹-stacking"><span class="toc-text"> 4-4. 스태킹 (Stacking)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-weighted-blending"><span class="toc-text"> 4-5. Weighted Blending</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-앙상블-모델-정리"><span class="toc-text"> 4-6. 앙상블 모델 정리</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-cross-validation"><span class="toc-text"> 5. Cross Validation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-cross-validation-소개"><span class="toc-text"> 5-1. Cross Validation 소개</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-hyper-parameter-튜닝"><span class="toc-text"> 5-2. Hyper-parameter 튜닝</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-randomizedsearchcv"><span class="toc-text"> (1) RandomizedSearchCV</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-gridserchcv"><span class="toc-text"> (2) GridSerchCV</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://ohiing.com/wp-content/uploads/2020/02/scikit-learn-2.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Hyemin Kim</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Python &gt;&gt; sklearn - (4) 앙상블 (Ensemble)</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-08-04T11:40:35.000Z" title="Created 2020-08-04 20:40:35">2020-08-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-11-06T05:20:39.672Z" title="Updated 2020-11-06 14:20:39">2020-11-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E3%80%90STUDY-Python%E3%80%91/">【STUDY - Python】</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E3%80%90STUDY-Python%E3%80%91/Python-Machine-Learning/">Python - Machine Learning</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="앙상블-ensemble"><a class="markdownIt-Anchor" href="#앙상블-ensemble"></a> 앙상블 (Ensemble)</h1>
<p></p><ul class="markdownIt-TOC">
<li><a href="#0-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%85%8B"><strong>0. 데이터 셋</strong></a>
<ul>
<li><a href="#0-1-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%A1%9C%EB%93%9C">0-1. 데이터 로드</a></li>
<li><a href="#0-2-%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%94%84%EB%A0%88%EC%9E%84-%EB%A7%8C%EB%93%A4%EA%B8%B0">0-2. 데이터프레임 만들기</a></li>
</ul>
</li>
<li><a href="#1-training-set-test-set-%EB%82%98%EB%88%84%EA%B8%B0"><strong>1. Training set / Test set 나누기</strong></a></li>
<li><a href="#2-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-%EB%A7%8C%EB%93%A4%EA%B8%B0"><strong>2. 평가 지표 만들기</strong></a>
<ul>
<li><a href="#2-1-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C">2-1. 평가 지표</a></li>
<li><a href="#2-2-%EB%AA%A8%EB%8D%B8-%EC%84%B1%EB%8A%A5-%ED%99%95%EC%9D%B8%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%95%A8%EC%88%98">2-2. 모델 성능 확인을 위한 함수</a></li>
</ul>
</li>
<li><a href="#3-%EB%8B%A8%EC%9D%BC-%ED%9A%8C%EA%B7%80-%EB%AA%A8%EB%8D%B8-%EC%A7%80%EB%82%9C-%EC%8B%9C%EA%B0%84"><strong>3. 단일 회귀 모델 (지난 시간)</strong></a>
<ul>
<li><a href="#1-linear-regression">(1)  Linear Regression</a></li>
<li><a href="#2-ridge">(2)  Ridge</a></li>
<li><a href="#3-lasso">(3)  LASSO</a></li>
<li><a href="#4-elasticnet">(4) ElasticNet</a></li>
<li><a href="#5-with-standard-scaling">(5) With Standard Scaling</a></li>
<li><a href="#6-polynomial-features">(6) Polynomial Features</a></li>
</ul>
</li>
<li><a href="#4-%EC%95%99%EC%83%81%EB%B8%94-ensemble-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98"><strong>4. 앙상블 (Ensemble)  알고리즘</strong></a>
<ul>
<li><a href="#4-1-%EB%B3%B4%ED%8C%85-voting">4-1. 보팅 (Voting)</a>
<ul>
<li><a href="#%ED%9A%8C%EA%B7%80-regression">&gt;&gt; 회귀 (Regression)</a></li>
<li><a href="#%EB%B6%84%EB%A5%98-classification">&gt;&gt; 분류 (Classification)</a></li>
</ul>
</li>
<li><a href="#4-2-%EB%B0%B0%EA%B9%85-bagging">4-2. 배깅 (Bagging)</a>
<ul>
<li><a href="#random-forest">&gt;&gt; Random Forest</a></li>
</ul>
</li>
<li><a href="#4-3-%EB%B6%80%EC%8A%A4%ED%8C%85-boosting">4-3. 부스팅 (Boosting)</a>
<ul>
<li><a href="#4-3-1-gradient-boost">4-3-1. Gradient Boost</a></li>
<li><a href="#4-3-2-xgboost">4-3-2. XGBoost</a></li>
<li><a href="#4-3-3-lightgbm">4-3-3. LightGBM</a></li>
</ul>
</li>
<li><a href="#4-4-%EC%8A%A4%ED%83%9C%ED%82%B9-stacking">4-4. 스태킹 (Stacking)</a></li>
<li><a href="#4-5-weighted-blending">4-5. Weighted Blending</a></li>
<li><a href="#4-6-%EC%95%99%EC%83%81%EB%B8%94-%EB%AA%A8%EB%8D%B8-%EC%A0%95%EB%A6%AC">4-6. 앙상블 모델 정리</a></li>
</ul>
</li>
<li><a href="#5-cross-validation"><strong>5. Cross Validation</strong></a>
<ul>
<li><a href="#5-1-cross-validation-%EC%86%8C%EA%B0%9C">5-1. Cross Validation 소개</a></li>
<li><a href="#5-2-hyper-parameter-%ED%8A%9C%EB%8B%9D">5-2. Hyper-parameter 튜닝</a>
<ul>
<li><a href="#1-randomizedsearchcv">(1) RandomizedSearchCV</a></li>
<li><a href="#2-gridserchcv">(2) GridSerchCV</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p></p>
<br>
<p>머신러닝 앙상블이란 <strong>여러 개의 머신러닝 모델을 이용해 최적의 답을 찾아내는 기법</strong>이다.<br>
(여러 모델을 이용하여 데이터를 학습하고, 모든 모델의 예측결과를 평균하여 예측)</p>
<br>
<p><strong>앙상블 기법의 종류</strong></p>
<ul>
<li>보팅 (Voting): 투표를 통해 결과 도출</li>
<li>배깅 (Bagging): 샘플 중복 생성을 통해 결과 도출</li>
<li>부스팅 (Boosting): 이전 오차를 보완하면서 가중치 부여</li>
<li>스태킹 (Stacking): 여러 모델을 기반으로 예측된 결과를 통해 meta 모델이 다시 한번 예측</li>
</ul>
<br>
<p><strong>참고자료 (블로그)</strong></p>
<ul>
<li>
<p><a href="https://blog.naver.com/winddori2002/221848433173" target="_blank" rel="noopener">보팅 (Voting)</a></p>
</li>
<li>
<p><a href="https://teddylee777.github.io/machine-learning/ensemble%EA%B8%B0%EB%B2%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4%EC%99%80-%EC%A2%85%EB%A5%98-2" target="_blank" rel="noopener">배경 (Bagging)</a></p>
</li>
<li>
<p><a href="https://teddylee777.github.io/machine-learning/ensemble%EA%B8%B0%EB%B2%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4%EC%99%80-%EC%A2%85%EB%A5%98-3" target="_blank" rel="noopener">부스팅 (Boosting)</a></p>
<br>
<br>
</li>
</ul>
<h2 id="0-데이터-셋"><a class="markdownIt-Anchor" href="#0-데이터-셋"></a> <strong>0. 데이터 셋</strong></h2>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">np.set_printoptions(suppress=<span class="literal">True</span>) <span class="comment"># If True, print floating point numbers instead of scientific notation</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br></pre></td></tr></tbody></table></figure>
 <br> 
<h3 id="0-1-데이터-로드"><a class="markdownIt-Anchor" href="#0-1-데이터-로드"></a> 0-1. 데이터 로드</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = load_boston()</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(data[<span class="string">'DESCR'</span>])</span><br></pre></td></tr></tbody></table></figure>
<pre><code>.. _boston_dataset:

Boston house prices dataset
---------------------------

**Data Set Characteristics:**  

    :Number of Instances: 506 

    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.

    :Attribute Information (in order):
        - CRIM     per capita crime rate by town
        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
        - INDUS    proportion of non-retail business acres per town
        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
        - NOX      nitric oxides concentration (parts per 10 million)
        - RM       average number of rooms per dwelling
        - AGE      proportion of owner-occupied units built prior to 1940
        - DIS      weighted distances to five Boston employment centres
        - RAD      index of accessibility to radial highways
        - TAX      full-value property-tax rate per $10,000
        - PTRATIO  pupil-teacher ratio by town
        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
        - LSTAT    % lower status of the population
        - MEDV     Median value of owner-occupied homes in $1000's

    :Missing Attribute Values: None

    :Creator: Harrison, D. and Rubinfeld, D.L.

This is a copy of UCI ML housing dataset.
https://archive.ics.uci.edu/ml/machine-learning-databases/housing/
</code></pre>
<p>​</p>
<pre><code>This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.

The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic
prices and the demand for clean air', J. Environ. Economics &amp; Management,
vol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, 'Regression diagnostics
...', Wiley, 1980.   N.B. Various transformations are used in the table on
pages 244-261 of the latter.

The Boston house-price data has been used in many machine learning papers that address regression
problems.   
     
.. topic:: References

   - Belsley, Kuh &amp; Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.
   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.
</code></pre>
<p>​</p>
 <br> 
<h3 id="0-2-데이터프레임-만들기"><a class="markdownIt-Anchor" href="#0-2-데이터프레임-만들기"></a> 0-2. 데이터프레임 만들기</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(data[<span class="string">'data'</span>], columns = data[<span class="string">'feature_names'</span>])</span><br><span class="line">df[<span class="string">'MEDV'</span>] = data[<span class="string">'target'</span>]</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.head()</span><br></pre></td></tr></tbody></table></figure>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table>
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div>
 <br> 
<p><strong>컬럼 소게</strong> (feature 13 + target 1):</p>
<ul>
<li>
<p><strong>CRIM</strong>: 범죄율</p>
</li>
<li>
<p><strong>ZN</strong>: 25,000 square feet 당 주거용 토지의 비율</p>
</li>
<li>
<p><strong>INDUS</strong>: 비소매(non-retail) 비즈니스 면적 비율</p>
</li>
<li>
<p><strong>CHAS</strong>: 찰스 강 더미 변수 (통로가 하천을 향하면 1; 그렇지 않으면 0)</p>
</li>
<li>
<p><strong>NOX</strong>: 산화 질소 농도 (천만 분의 1)</p>
</li>
<li>
<p><strong>RM</strong>:주거 당 평균 객실 수</p>
</li>
<li>
<p><strong>AGE</strong>: 1940 년 이전에 건축된 자가 소유 점유 비율</p>
</li>
<li>
<p><strong>DIS</strong>: 5 개의 보스턴 고용 센터까지의 가중 거리</p>
</li>
<li>
<p><strong>RAD</strong>: 고속도로 접근성 지수</p>
</li>
<li>
<p><strong>TAX</strong>: 10,000 달러 당 전체 가치 재산 세율</p>
</li>
<li>
<p><strong>PTRATIO</strong>  도시 별 학생-교사 비율</p>
</li>
<li>
<p><strong>B</strong>: 1000 (Bk-0.63) ^ 2 여기서 Bk는 도시 별 검정 비율입니다.</p>
</li>
<li>
<p><strong>LSTAT</strong>: 인구의 낮은 지위</p>
</li>
<li>
<p><strong>MEDV</strong>: 자가 주택의 중앙값 (1,000 달러 단위)</p>
<br>
</li>
</ul>
<h2 id="1-training-set-test-set-나누기"><a class="markdownIt-Anchor" href="#1-training-set-test-set-나누기"></a> <strong>1. Training set / Test set 나누기</strong></h2>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train, x_test, y_train, y_test = train_test_split(df.drop(<span class="string">'MEDV'</span>, <span class="number">1</span>), df[<span class="string">'MEDV'</span>], random_state=<span class="number">23</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train.shape, y_train.shape</span><br></pre></td></tr></tbody></table></figure>
<pre><code>((379, 13), (379,))
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_test.shape, y_test.shape</span><br></pre></td></tr></tbody></table></figure>
<pre><code>((127, 13), (127,))
</code></pre>
  <br>
<h2 id="2-평가-지표-만들기"><a class="markdownIt-Anchor" href="#2-평가-지표-만들기"></a> <strong>2. 평가 지표 만들기</strong></h2>
<h3 id="2-1-평가-지표"><a class="markdownIt-Anchor" href="#2-1-평가-지표"></a> 2-1. 평가 지표</h3>
<p><strong>(1) MAE (Mean Absolute Error)</strong></p>
<p>MAE (평균 절대 오차): 에측값과 실제값의 차이의 <strong>절대값</strong>에 대하여 평균을 낸 것</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mi>A</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mo fence="true">∣</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true">^</mo></mover><mo fence="true">∣</mo></mrow></mrow><annotation encoding="application/x-tex">MAE = \frac{1}{n} \sum_{i=1}^n \left\vert y_i - \widehat{y_i} \right\vert
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.67056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span class="svg-align" style="top:-3.43056em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width="100%" height="0.24em" viewBox="0 0 1062 239" preserveAspectRatio="none"><path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span></span></span></span></span></p>
<p><strong>(2) MSE (Mean Squared Error)</strong></p>
<p>MSE (평균 제곱 오차): 예측값과 실제값의 차이의 <strong>제곱</strong>에 대하여 평균을 낸 것</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mi>S</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo fence="true">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true">^</mo></mover><mo fence="true">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">MSE = \frac{1}{n} \sum_{i=1}^n \left( y_i - \widehat{y_i} \right)^2
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.67056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span class="svg-align" style="top:-3.43056em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width="100%" height="0.24em" viewBox="0 0 1062 239" preserveAspectRatio="none"><path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954008em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p><strong>(3) RMSE (Root Mean Squared Error)</strong></p>
<p>RMSE (평균 제곱근 오차): 예측값과 실제값의 차이의 <strong>제곱</strong>에 대하여 평균을 낸 뒤 <strong>루트</strong>를 씌운 것</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mi>M</mi><mi>S</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo fence="true">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true">^</mo></mover><mo fence="true">)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n \left( y_i - \widehat{y_i} \right)^2}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.1568160000000005em;vertical-align:-1.277669em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8791470000000006em;"><span class="svg-align" style="top:-5.116816em;"><span class="pstrut" style="height:5.116816em;"></span><span class="mord" style="padding-left:1.056em;"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.67056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span class="svg-align" style="top:-3.43056em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width="100%" height="0.24em" viewBox="0 0 1062 239" preserveAspectRatio="none"><path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954008em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.8391470000000005em;"><span class="pstrut" style="height:5.116816em;"></span><span class="hide-tail" style="min-width:0.742em;height:3.196816em;"><svg width="400em" height="3.196816em" viewBox="0 0 400000 3196" preserveAspectRatio="xMinYMin slice"><path d="M702 80H400000v40H742v3062l-4 4-4 4c-.667.7
-2 1.5-4 2.5s-4.167 1.833-6.5 2.5-5.5 1-9.5 1h-12l-28-84c-16.667-52-96.667
-294.333-240-727l-212 -643 -85 170c-4-3.333-8.333-7.667-13 -13l-13-13l77-155
 77-156c66 199.333 139 419.667 219 661 l218 661zM702 80H400000v40H742z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span></span></span></span></span></p>
  <br>
<h3 id="2-2-모델-성능-확인을-위한-함수"><a class="markdownIt-Anchor" href="#2-2-모델-성능-확인을-위한-함수"></a> 2-2. 모델 성능 확인을 위한 함수</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sklearn 평가지표 활용</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error, mean_squared_error</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">my_predictions = {}</span><br><span class="line"></span><br><span class="line">colors = [<span class="string">'r'</span>, <span class="string">'c'</span>, <span class="string">'m'</span>, <span class="string">'y'</span>, <span class="string">'k'</span>, <span class="string">'khaki'</span>, <span class="string">'teal'</span>, <span class="string">'orchid'</span>, <span class="string">'sandybrown'</span>,</span><br><span class="line">          <span class="string">'greenyellow'</span>, <span class="string">'dodgerblue'</span>, <span class="string">'deepskyblue'</span>, <span class="string">'rosybrown'</span>, <span class="string">'firebrick'</span>,</span><br><span class="line">          <span class="string">'deeppink'</span>, <span class="string">'crimson'</span>, <span class="string">'salmon'</span>, <span class="string">'darkred'</span>, <span class="string">'olivedrab'</span>, <span class="string">'olive'</span>, </span><br><span class="line">          <span class="string">'forestgreen'</span>, <span class="string">'royalblue'</span>, <span class="string">'indigo'</span>, <span class="string">'navy'</span>, <span class="string">'mediumpurple'</span>, <span class="string">'chocolate'</span>,</span><br><span class="line">          <span class="string">'gold'</span>, <span class="string">'darkorange'</span>, <span class="string">'seagreen'</span>, <span class="string">'turquoise'</span>, <span class="string">'steelblue'</span>, <span class="string">'slategray'</span>, </span><br><span class="line">          <span class="string">'peru'</span>, <span class="string">'midnightblue'</span>, <span class="string">'slateblue'</span>, <span class="string">'dimgray'</span>, <span class="string">'cadetblue'</span>, <span class="string">'tomato'</span></span><br><span class="line">         ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># prediction plot</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_predictions</span><span class="params">(name_, actual, pred)</span>:</span></span><br><span class="line">    df = pd.DataFrame({<span class="string">'actual'</span>: y_test, <span class="string">'prediction'</span>: pred})</span><br><span class="line">    df = df.sort_values(by=<span class="string">'actual'</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">9</span>))</span><br><span class="line">    plt.scatter(df.index, df[<span class="string">'prediction'</span>], marker=<span class="string">'x'</span>, color=<span class="string">'r'</span>)</span><br><span class="line">    plt.scatter(df.index, df[<span class="string">'actual'</span>], alpha=<span class="number">0.7</span>, marker=<span class="string">'o'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    plt.title(name_, fontsize=<span class="number">15</span>)</span><br><span class="line">    plt.legend([<span class="string">'prediction'</span>, <span class="string">'actual'</span>], fontsize=<span class="number">12</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluation plot</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse_eval</span><span class="params">(name_, actual, pred)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> predictions</span><br><span class="line">    <span class="keyword">global</span> colors</span><br><span class="line"></span><br><span class="line">    plot_predictions(name_, actual, pred)</span><br><span class="line"></span><br><span class="line">    mse = mean_squared_error(actual, pred)</span><br><span class="line">    my_predictions[name_] = mse</span><br><span class="line"></span><br><span class="line">    y_value = sorted(my_predictions.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(y_value, columns=[<span class="string">'model'</span>, <span class="string">'mse'</span>])</span><br><span class="line">    print(df)</span><br><span class="line">    min_ = df[<span class="string">'mse'</span>].min() - <span class="number">10</span></span><br><span class="line">    max_ = df[<span class="string">'mse'</span>].max() + <span class="number">10</span></span><br><span class="line">    </span><br><span class="line">    length = len(df)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, length))</span><br><span class="line">    ax = plt.subplot()</span><br><span class="line">    ax.set_yticks(np.arange(len(df)))</span><br><span class="line">    ax.set_yticklabels(df[<span class="string">'model'</span>], fontsize=<span class="number">15</span>)</span><br><span class="line">    bars = ax.barh(np.arange(len(df)), df[<span class="string">'mse'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, v <span class="keyword">in</span> enumerate(df[<span class="string">'mse'</span>]):</span><br><span class="line">        idx = np.random.choice(len(colors))</span><br><span class="line">        bars[i].set_color(colors[idx])</span><br><span class="line">        ax.text(v + <span class="number">2</span>, i, str(round(v, <span class="number">3</span>)), color=<span class="string">'k'</span>, fontsize=<span class="number">15</span>, fontweight=<span class="string">'bold'</span>)</span><br><span class="line">        </span><br><span class="line">    plt.title(<span class="string">'MSE Error'</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">    plt.xlim(min_, max_)</span><br><span class="line">    </span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_model</span><span class="params">(name_)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> my_predictions</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">del</span> my_predictions[name_]</span><br><span class="line">    <span class="keyword">except</span> KeyError:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># coefficients visulization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_coef</span><span class="params">(columns, coef)</span>:</span></span><br><span class="line">    coef_df = pd.DataFrame(list(zip(columns, coef)))</span><br><span class="line">    coef_df.columns=[<span class="string">'feature'</span>, <span class="string">'coef'</span>]</span><br><span class="line">    coef_df = coef_df.sort_values(<span class="string">'coef'</span>, ascending=<span class="literal">False</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">9</span>, <span class="number">7</span>))</span><br><span class="line">    ax.barh(np.arange(len(coef_df)), coef_df[<span class="string">'coef'</span>])</span><br><span class="line">    idx = np.arange(len(coef_df))</span><br><span class="line">    ax.set_yticks(idx)</span><br><span class="line">    ax.set_yticklabels(coef_df[<span class="string">'feature'</span>])</span><br><span class="line">    fig.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><br> <br></p>
<h2 id="3-단일-회귀-모델-지난-시간"><a class="markdownIt-Anchor" href="#3-단일-회귀-모델-지난-시간"></a> <strong>3. 단일 회귀 모델 (지난 시간)</strong></h2>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, MinMaxScaler, RobustScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br></pre></td></tr></tbody></table></figure>
  <br>
<h3 id="1-linear-regression"><a class="markdownIt-Anchor" href="#1-linear-regression"></a> (1)  Linear Regression</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">linear_reg = LinearRegression(n_jobs=<span class="number">-1</span>)</span><br><span class="line">linear_reg.fit(x_train, y_train)</span><br><span class="line">linear_pred = linear_reg.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'LinearRegression'</span>, y_test, linear_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_43_0.png" alt="output_43_0"></p>
<pre><code>              model        mse
0  LinearRegression  22.770784
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_43_2.png" alt="output_43_2"></p>
<br>
<h3 id="2-ridge"><a class="markdownIt-Anchor" href="#2-ridge"></a> (2)  Ridge</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ridge = Ridge(alpha=<span class="number">1</span>)</span><br><span class="line">ridge.fit(x_train, y_train)</span><br><span class="line">ridge_pred = ridge.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'Ridge(alpha=1)'</span>, y_test, ridge_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_46_0.png" alt="output_46_0"></p>
<pre><code>              model        mse
0  LinearRegression  22.770784
1    Ridge(alpha=1)  22.690411
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_46_2.png" alt="output_46_2"></p>
<br>
<h3 id="3-lasso"><a class="markdownIt-Anchor" href="#3-lasso"></a> (3)  LASSO</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lasso = Lasso(alpha=<span class="number">0.01</span>)</span><br><span class="line">lasso.fit(x_train, y_train)</span><br><span class="line">lasso_pred = lasso.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'Lasso(alpha=0.01)'</span>, y_test, lasso_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_49_0.png" alt="output_49_0"></p>
<pre><code>               model        mse
0   LinearRegression  22.770784
1     Ridge(alpha=1)  22.690411
2  Lasso(alpha=0.01)  22.635614
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_49_2.png" alt="output_49_2"></p>
<br>
<h3 id="4-elasticnet"><a class="markdownIt-Anchor" href="#4-elasticnet"></a> (4) ElasticNet</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">elasticnet = ElasticNet(alpha=<span class="number">0.5</span>, l1_ratio=<span class="number">0.2</span>)</span><br><span class="line">elasticnet.fit(x_train, y_train)</span><br><span class="line">elas_pred = elasticnet.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'ElasticNet(l1_ratio=0.2)'</span>, y_test, elas_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_52_0.png" alt="output_52_0"></p>
<pre><code>                      model        mse
0  ElasticNet(l1_ratio=0.2)  24.481069
1          LinearRegression  22.770784
2            Ridge(alpha=1)  22.690411
3         Lasso(alpha=0.01)  22.635614
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_52_2.png" alt="output_52_2"></p>
<br>
<h3 id="5-with-standard-scaling"><a class="markdownIt-Anchor" href="#5-with-standard-scaling"></a> (5) With Standard Scaling</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">standard_elasticnet = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    ElasticNet(alpha=<span class="number">0.5</span>, l1_ratio=<span class="number">0.2</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">elas_scaled_pred = standard_elasticnet.fit(x_train, y_train).predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'Standard ElasticNet'</span>, y_test, elas_scaled_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_55_0.png" alt="output_55_0"></p>
<pre><code>                      model        mse
0       Standard ElasticNet  26.010756
1  ElasticNet(l1_ratio=0.2)  24.481069
2          LinearRegression  22.770784
3            Ridge(alpha=1)  22.690411
4         Lasso(alpha=0.01)  22.635614
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_55_2.png" alt="output_55_2"></p>
<br>
<h3 id="6-polynomial-features"><a class="markdownIt-Anchor" href="#6-polynomial-features"></a> (6) Polynomial Features</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2-Degree Polynomial Features + Standard Scaling</span></span><br><span class="line">poly_elasticnet = make_pipeline(</span><br><span class="line">    PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>),</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    ElasticNet(alpha=<span class="number">0.5</span>, l1_ratio=<span class="number">0.2</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">poly_pred = poly_elasticnet.fit(x_train, y_train).predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'Poly ElasticNet'</span>, y_test, poly_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_58_0-1596543446470.png" alt="output_58_0"></p>
<pre><code>                      model        mse
0       Standard ElasticNet  26.010756
1  ElasticNet(l1_ratio=0.2)  24.481069
2          LinearRegression  22.770784
3            Ridge(alpha=1)  22.690411
4         Lasso(alpha=0.01)  22.635614
5           Poly ElasticNet  20.805986
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_58_2-1596543452273.png" alt="output_58_2"></p>
<br>
<br>
<h2 id="4-앙상블-ensemble-알고리즘"><a class="markdownIt-Anchor" href="#4-앙상블-ensemble-알고리즘"></a> <strong>4. 앙상블 (Ensemble)  알고리즘</strong></h2>
<p><a href="https://scikit-learn.org/stable/modules/classes.html?highlight=ensemble#module-sklearn.ensemble" target="_blank" rel="noopener">[sklearn.ensemble] Document</a></p>
<p><strong>앙상블 기법의 종류</strong></p>
<ul>
<li>
<p>보팅 (Voting): 투표를 통해 결과 도출</p>
</li>
<li>
<p>배깅 (Bagging): 샘플 중복 생성을 통해 결과 도출</p>
</li>
<li>
<p>부스팅 (Boosting): 이전 오차를 보완하면서 가중치 부여</p>
</li>
<li>
<p>스태킹 (Stacking): 여러 모델을 기반으로 예측된 결과를 통해 meta 모델이 다시 한번 예측</p>
<br>
</li>
</ul>
<h3 id="4-1-보팅-voting"><a class="markdownIt-Anchor" href="#4-1-보팅-voting"></a> 4-1. 보팅 (Voting)</h3>
<h4 id="회귀-regression"><a class="markdownIt-Anchor" href="#회귀-regression"></a> &gt;&gt; 회귀 (Regression)</h4>
<p>Voting은 단어 뜻 그대로 <strong>투표를 통해 최종 결과를 결정하는 방식</strong>이다. Voting과 Bagging은 모두 투표방식이지만, 다음과 같은 큰 차이점이 있다:</p>
<ul>
<li>Voting은 다른 알고리즘 model을 조합해서 사용함</li>
<li>Bagging은 같은 알고리즘 내에서 다른 sample 조합을 사용함</li>
</ul>
 <br>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingRegressor</span><br></pre></td></tr></tbody></table></figure>
<p>반드시, <strong>Tuple 형태로 모델</strong>을 정의해야 한다.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 보팅에 참여한 single models 지정</span></span><br><span class="line">single_models = [</span><br><span class="line">    (<span class="string">'linear_reg'</span>, linear_reg),</span><br><span class="line">    (<span class="string">'ridge'</span>, ridge),</span><br><span class="line">    (<span class="string">'lasso'</span>, lasso),</span><br><span class="line">    (<span class="string">'elasticnet'</span>, elasticnet),</span><br><span class="line">    (<span class="string">'standard_elasticnet'</span>, standard_elasticnet),</span><br><span class="line">    (<span class="string">'poly_elasticnet'</span>, poly_elasticnet)</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># voting regressor 만들기</span></span><br><span class="line">voting_regressor = VotingRegressor(single_models, n_jobs=<span class="number">-1</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">voting_regressor.fit(x_train, y_train)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>VotingRegressor(estimators=[('linear_reg',
                             LinearRegression(copy_X=True, fit_intercept=True,
                                              n_jobs=-1, normalize=False)),
                            ('ridge',
                             Ridge(alpha=1, copy_X=True, fit_intercept=True,
                                   max_iter=None, normalize=False,
                                   random_state=None, solver='auto',
                                   tol=0.001)),
                            ('lasso',
                             Lasso(alpha=0.01, copy_X=True, fit_intercept=True,
                                   max_iter=1000, normalize=False,
                                   positive=False, pr...
                                                                 interaction_only=False,
                                                                 order='C')),
                                             ('standardscaler',
                                              StandardScaler(copy=True,
                                                             with_mean=True,
                                                             with_std=True)),
                                             ('elasticnet',
                                              ElasticNet(alpha=0.5, copy_X=True,
                                                         fit_intercept=True,
                                                         l1_ratio=0.2,
                                                         max_iter=1000,
                                                         normalize=False,
                                                         positive=False,
                                                         precompute=False,
                                                         random_state=None,
                                                         selection='cyclic',
                                                         tol=0.0001,
                                                         warm_start=False))],
                                      verbose=False))],
                n_jobs=-1, weights=None)
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">voting_pred = voting_regressor.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'Voting Ensemble'</span>, y_test, voting_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_74_0-1596543659301.png" alt="output_74_0"></p>
<pre><code>                      model        mse
0       Standard ElasticNet  26.010756
1  ElasticNet(l1_ratio=0.2)  24.481069
2          LinearRegression  22.770784
3            Ridge(alpha=1)  22.690411
4         Lasso(alpha=0.01)  22.635614
5           Voting Ensemble  22.092158
6           Poly ElasticNet  20.805986
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_74_2-1596543665427.png" alt="output_74_2"></p>
<br>
<h4 id="분류-classification"><a class="markdownIt-Anchor" href="#분류-classification"></a> &gt;&gt; 분류 (Classification)</h4>
<p><a href="https://teddylee777.github.io/machine-learning/ensemble%EA%B8%B0%EB%B2%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4%EC%99%80-%EC%A2%85%EB%A5%98-1" target="_blank" rel="noopener">참고 자료 (Blog)</a></p>
<p>분류기 모델을 만들때, Voting 앙상블은 1가지의 <strong>중요한 parameter</strong>가 있다:</p>
<ul>
<li><code>voting</code> = {‘hard’, ‘soft’}</li>
</ul>
<br>
<p>class를 0, 1로 분류 예측을 하는 이진 분류를 예로 들어 보자.</p>
<p><strong>(1) hard 로 설정한 경우</strong></p>
<p>Hard Voting 방식에서는 결과 값에 대한 다수 class를 사용한다.</p>
<blockquote>
<p>분류를 예측한 값이 1, 0, 0, 1, 1 이었다고 가정한다면 1이 3표, 0이 2표를 받았기 때문에 Hard Voting 방식에서는 1이 최종 값으로 예측을 하게 된다.</p>
</blockquote>
 <br> 
<p><strong>(2) soft 로 설정한 경우</strong></p>
<p>soft voting 방식은 각각의 확률의 평균 값을 계산한다음에 가장 확률이 높은 값으로 확정짓게 된다.</p>
<blockquote>
<p>가령 class 0이 나올 확률이 (0.4, 0.9, 0.9, 0.4, 0.4)이었고, class 1이 나올 확률이 (0.6, 0.1, 0.1, 0.6, 0.6) 이었다면,</p>
<ul>
<li>class 0이 나올 최종 확률은 (0.4+0.9+0.9+0.4+0.4) / 5 = 0.44,</li>
<li>class 1이 나올 최종 확률은 (0.6+0.1+0.1+0.6+0.6) / 5 = 0.4</li>
</ul>
<p>가 되기 때문에 앞선 Hard Voting의 결과와는 다른 결과 값이 최종으로 선출되게 된다.</p>
</blockquote>
<br>  
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression, RidgeClassifier</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">models = [</span><br><span class="line">    (<span class="string">'Logit'</span>, LogisticRegression()),</span><br><span class="line">    (<span class="string">'ridge'</span>, RidgeClassifier())</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<p>voting 옵션 지정</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vc = VotingClassifier(models, voting=<span class="string">'soft'</span>)</span><br></pre></td></tr></tbody></table></figure>
<br>  
<h3 id="4-2-배깅-bagging"><a class="markdownIt-Anchor" href="#4-2-배깅-bagging"></a> 4-2. 배깅 (Bagging)</h3>
<p><a href="https://teddylee777.github.io/machine-learning/ensemble%EA%B8%B0%EB%B2%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4%EC%99%80-%EC%A2%85%EB%A5%98-2" target="_blank" rel="noopener">참고 자료 (Blog)</a></p>
<p>Bagging은 <strong>Bootstrap Aggregating의 줄임말</strong>이다.</p>
<p>Bootstrap은 여러 개의 dataset을 중첩을 허용하게 하여 샘플링하여 분할하는 방식.</p>
<p>데이터 셋의 구성이 [1, 2, 3, 4, 5]로 되어 있다면,</p>
<ol>
<li>group 1 = [1, 2, 3]</li>
<li>group 2 = [1, 3, 4]</li>
<li>group 3 = [2, 3, 5]</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(<span class="string">'https://teddylee777.github.io/images/2019-12-17/image-20191217015537872.png'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_94_0-1596543677837.png" alt="output_94_0"></p>
<br>  
<p><strong>Voting VS Bagging</strong></p>
<ul>
<li><strong>Voting</strong>은 여러 알고리즘의 조합에 대한 앙상블</li>
<li><strong>Bagging</strong>은 하나의 단일 알고리즘에 대하여 여러 개의 샘플 조합으로 앙상블</li>
</ul>
<p><strong>대표적인 Bagging 앙상블</strong></p>
<ol>
<li>
<p>Random Forest</p>
</li>
<li>
<p>Bagging</p>
</li>
</ol>
  <br> 
<h4 id="random-forest"><a class="markdownIt-Anchor" href="#random-forest"></a> &gt;&gt; Random Forest</h4>
<ul>
<li>Decision Tree 기반 Bagging 앙상블</li>
<li>굉장히 인기있는 앙상블 모델</li>
<li>사용성이 쉽고, 성능도 우수함</li>
</ul>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" target="_blank" rel="noopener">[sklearn.ensemble.<strong>RandomForestRegressor</strong>] Document</a><br>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" target="_blank" rel="noopener">[sklearn.ensemble.<strong>RandomForestClassifier</strong>] Document</a></p>
  <br>
<ul>
<li><strong>회귀 (Regression)</strong></li>
</ul>
<p><strong>Hyper-parameter의 default value로 모델 학습</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rfr = RandomForestRegressor(random_state=<span class="number">1</span>)</span><br><span class="line">rfr.fit(x_train, y_train)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',
                      max_depth=None, max_features='auto', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      n_estimators=100, n_jobs=None, oob_score=False,
                      random_state=1, verbose=0, warm_start=False)
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rfr_pred = rfr.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'RandomForest Ensemble'</span>, y_test, rfr_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_107_0-1596543686370.png" alt="output_107_0"></p>
<pre><code>                      model        mse
0       Standard ElasticNet  26.010756
1  ElasticNet(l1_ratio=0.2)  24.481069
2          LinearRegression  22.770784
3            Ridge(alpha=1)  22.690411
4         Lasso(alpha=0.01)  22.635614
5           Voting Ensemble  22.092158
6           Poly ElasticNet  20.805986
7     RandomForest Ensemble  13.781191
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_107_2-1596543692042.png" alt="output_107_2"></p>
<br>
<p><strong>주요 Hyper-parameter</strong></p>
<ul>
<li><strong>random_state:</strong> random seed 고정 값</li>
<li><strong>n_jobs:</strong> CPU 사용 갯수</li>
<li><strong>max_depth:</strong> 깊어질 수 있는 최대 깊이. 과대적합 방지용</li>
<li><strong>n_estimators:</strong> 암상블하는 트리의 갯수</li>
<li><strong>max_features:</strong> best split을 판단할 때 최대로 사용할 feature의 갯수 {‘auto’, ‘sqrt’, ‘log2’}. 과대적합 방지용</li>
<li><strong>min_samples_split:</strong> 트리가 분할할 때 최소 샘플의 갯수. default=2. 과대적합 방지용</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(<span class="string">'https://teddylee777.github.io/images/2020-01-09/decistion-tree.png'</span>, width=<span class="number">600</span>)</span><br></pre></td></tr></tbody></table></figure>
<img src="/images/S-Python-sklearn4/output_110_0-1596543700142.png" alt="output_110_0" style="zoom: 50%;">
 <br> 
<p><strong>With Hyper-parameter Tuning</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rfr_t = RandomForestRegressor(random_state=<span class="number">1</span>, n_estimators=<span class="number">500</span>, max_depth=<span class="number">7</span>, max_features=<span class="string">'sqrt'</span>)</span><br><span class="line">rfr_t.fit(x_train, y_train)</span><br><span class="line">rfr_t_pred = rfr_t.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'RandomForest Ensemble w/ Tuning'</span>, y_test, rfr_t_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_113_0-1596543713401.png" alt="output_113_0"></p>
<pre><code>                             model        mse
0              Standard ElasticNet  26.010756
1         ElasticNet(l1_ratio=0.2)  24.481069
2                 LinearRegression  22.770784
3                   Ridge(alpha=1)  22.690411
4                Lasso(alpha=0.01)  22.635614
5                  Voting Ensemble  22.092158
6                  Poly ElasticNet  20.805986
7            RandomForest Ensemble  13.781191
8  RandomForest Ensemble w/ Tuning  11.481491
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_113_2-1596543719396.png" alt="output_113_2"></p>
<br>
<h3 id="4-3-부스팅-boosting"><a class="markdownIt-Anchor" href="#4-3-부스팅-boosting"></a> 4-3. 부스팅 (Boosting)</h3>
<p><a href="https://teddylee777.github.io/machine-learning/ensemble%EA%B8%B0%EB%B2%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4%EC%99%80-%EC%A2%85%EB%A5%98-3" target="_blank" rel="noopener">참고 자료 (Blog)</a></p>
<p>악한 학습기를 순차적으로 학습을 하되, 이전 학습에 대하여 잘멋 예측된 데이터에 <strong>가중치를 부여해 오차를 보완</strong>해 나가는 방식이다.</p>
<p><strong>장점</strong></p>
<ul>
<li>성능이 매우 우수하다 (LightGBM, XGBoost)</li>
</ul>
<p><strong>단점</strong></p>
<ul>
<li>부스팅 알고리즘의 특성상 계속 약점(오분류/잔차)을 보완하려고 하기 때문에 <strong>잘못된 레이블링이나 아웃라이어에 필요 이상으로 민감</strong>할 수 있다</li>
<li>다른 앙상블 대비 <strong>학습 시간이 오래걸린다는 단점</strong>이 존재</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(<span class="string">'https://keras.io/img/graph-kaggle-1.jpeg'</span>, width=<span class="number">800</span>)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_120_0-1596543725618.jpg" alt="output_120_0"></p>
  <br>
<p><strong>대표적인 Boosting 앙상블</strong></p>
<ol>
<li>
<p>AdaBoost</p>
</li>
<li>
<p>GradientBoost</p>
</li>
<li>
<p>LightGBM (LGBM)</p>
</li>
<li>
<p>XGBoost</p>
</li>
</ol>
  <br> 
<h4 id="4-3-1-gradient-boost"><a class="markdownIt-Anchor" href="#4-3-1-gradient-boost"></a> 4-3-1. Gradient Boost</h4>
<ul>
<li><strong>장점:</strong> 성능이 우수함</li>
<li><strong>단점:</strong> 학습 시간이 너무 오래 걸린다</li>
</ul>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html?highlight=gradient%20boost#sklearn.ensemble.GradientBoostingRegressor" target="_blank" rel="noopener">[sklearn.ensemble.<strong>GradientBoostingRegressor</strong>] Document</a></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor, GradientBoostingClassifier</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default value로 학습</span></span><br><span class="line">gbr = GradientBoostingRegressor(random_state=<span class="number">1</span>)</span><br><span class="line">gbr.fit(x_train, y_train)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='ls', max_depth=3,
                          max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_impurity_split=None,
                          min_samples_leaf=1, min_samples_split=2,
                          min_weight_fraction_leaf=0.0, n_estimators=100,
                          n_iter_no_change=None, presort='deprecated',
                          random_state=1, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gbr_pred = gbr.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'GradientBoost Ensemble'</span>, y_test, gbr_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_129_0-1596543733348.png" alt="output_129_0"></p>
<pre><code>                             model        mse
0              Standard ElasticNet  26.010756
1         ElasticNet(l1_ratio=0.2)  24.481069
2                 LinearRegression  22.770784
3                   Ridge(alpha=1)  22.690411
4                Lasso(alpha=0.01)  22.635614
5                  Voting Ensemble  22.092158
6                  Poly ElasticNet  20.805986
7            RandomForest Ensemble  13.781191
8           GradientBoost Ensemble  13.451877
9  RandomForest Ensemble w/ Tuning  11.481491
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_129_2-1596543738508.png" alt="output_129_2"></p>
<br>
<p><strong>주요 Hyper-parameter</strong></p>
<ul>
<li><strong>random_state:</strong> random seed 고정 값</li>
<li><strong>n_jobs:</strong> CPU 사용 갯수</li>
<li><strong>learning rate:</strong> 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. default=0.1 (n_estimators와 같이 튜닝해야 함)</li>
<li><strong>n_estimators:</strong> 부스팅 스테이지 수. default=100<br>
(Random Forest 트리의 갯수 설정과 비슷)</li>
<li><strong>subsample:</strong> 샘플 사용 비율 (max_features와 비슷). 과대적합 방지용</li>
<li><strong>min_samples_split:</strong> 노드 분할시 최소 샘플의 갯수. default=2. 과대적합 방지용</li>
</ul>
<blockquote>
<p>There’s a trade-off between <font color="blue"><em>learning_rate</em></font> and <font color="blue"><em>n_estimators</em></font>.<br>
둘의 곱을 유지하는 것이 좋다</p>
</blockquote>
<br>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># with hyper-parameter tuning</span></span><br><span class="line"><span class="comment"># learning_rate=0.01 (without tuning n_estimators together)</span></span><br><span class="line">gbr_t = GradientBoostingRegressor(random_state=<span class="number">1</span>, learning_rate=<span class="number">0.01</span>)</span><br><span class="line">gbr_t.fit(x_train, y_train)</span><br><span class="line">gbr_t_pred = gbr_t.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'GradientBoost Ensemble w/ tuning (lr=0.01)'</span>, y_test, gbr_t_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_133_0-1596543744823.png" alt="output_133_0"></p>
<pre><code>                                         model        mse
0                          Standard ElasticNet  26.010756
1   GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                     ElasticNet(l1_ratio=0.2)  24.481069
3                             LinearRegression  22.770784
4                               Ridge(alpha=1)  22.690411
5                            Lasso(alpha=0.01)  22.635614
6                              Voting Ensemble  22.092158
7                              Poly ElasticNet  20.805986
8                        RandomForest Ensemble  13.781191
9                       GradientBoost Ensemble  13.451877
10             RandomForest Ensemble w/ Tuning  11.481491
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_133_2-1596543750011.png" alt="output_133_2"></p>
<br>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tuning: learning_rate=0.01, n_estimators=1000</span></span><br><span class="line">gbr_t2 = GradientBoostingRegressor(random_state=<span class="number">1</span>, learning_rate=<span class="number">0.01</span>, n_estimators=<span class="number">1000</span>)</span><br><span class="line">gbr_t2.fit(x_train, y_train)</span><br><span class="line">gbr_t2_pred = gbr_t2.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'GradientBoost Ensemble w/ tuning (lr=0.01, est=1000)'</span>, y_test, gbr_t2_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_135_0-1596543755501.png" alt="output_135_0"></p>
<pre><code>                                                model        mse
0                                 Standard ElasticNet  26.010756
1          GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                            ElasticNet(l1_ratio=0.2)  24.481069
3                                    LinearRegression  22.770784
4                                      Ridge(alpha=1)  22.690411
5                                   Lasso(alpha=0.01)  22.635614
6                                     Voting Ensemble  22.092158
7                                     Poly ElasticNet  20.805986
8                               RandomForest Ensemble  13.781191
9                              GradientBoost Ensemble  13.451877
10  GradientBoost Ensemble w/ tuning (lr=0.01, est...  13.002472
11                    RandomForest Ensemble w/ Tuning  11.481491
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_135_2-1596543760559.png" alt="output_135_2"></p>
<br>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tuning: learning_rate=0.01, n_estimators=1000, subsample=0.8</span></span><br><span class="line">gbr_t3 = GradientBoostingRegressor(random_state=<span class="number">42</span>, learning_rate=<span class="number">0.01</span>, n_estimators=<span class="number">1000</span>, subsample=<span class="number">0.7</span>)</span><br><span class="line">gbr_t3.fit(x_train, y_train)</span><br><span class="line">gbr_t3_pred = gbr_t3.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'GradientBoost Ensemble w/ tuning (lr=0.01, est=1000, subsample=0.7)'</span>, y_test, gbr_t3_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_137_0-1596543766188.png" alt="output_137_0"></p>
<pre><code>                                                model        mse
0                                 Standard ElasticNet  26.010756
1          GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                            ElasticNet(l1_ratio=0.2)  24.481069
3                                    LinearRegression  22.770784
4                                      Ridge(alpha=1)  22.690411
5                                   Lasso(alpha=0.01)  22.635614
6                                     Voting Ensemble  22.092158
7                                     Poly ElasticNet  20.805986
8                               RandomForest Ensemble  13.781191
9                              GradientBoost Ensemble  13.451877
10  GradientBoost Ensemble w/ tuning (lr=0.01, est...  13.002472
11  GradientBoost Ensemble w/ tuning (lr=0.01, est...  12.607717
12                    RandomForest Ensemble w/ Tuning  11.481491
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_137_2-1596543771241.png" alt="output_137_2"></p>
<br>
<h4 id="4-3-2-xgboost"><a class="markdownIt-Anchor" href="#4-3-2-xgboost"></a> 4-3-2. XGBoost</h4>
<p>e<strong>X</strong>treme <strong>G</strong>radient <strong>B</strong>oosting</p>
<p><a href="https://xgboost.readthedocs.io/en/latest/" target="_blank" rel="noopener">[XGBoost] Document</a></p>
<p><strong>주요 특징</strong></p>
<ul>
<li>
<p>scikit-learn 패키지 아님</p>
</li>
<li>
<p>성능이 우수함</p>
</li>
<li>
<p>GBM보다는 빠르고 성능도 향상됨</p>
</li>
<li>
<p>여전히 학습 속도가 느림</p>
<br>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install xgboost</span><br></pre></td></tr></tbody></table></figure>
<pre><code>Requirement already satisfied: xgboost in d:\anaconda\lib\site-packages (1.1.1)
Requirement already satisfied: scipy in d:\anaconda\lib\site-packages (from xgboost) (1.4.1)
Requirement already satisfied: numpy in d:\anaconda\lib\site-packages (from xgboost) (1.18.1)
Note: you may need to restart the kernel to use updated packages.
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor, XGBClassifier</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default value로 학습</span></span><br><span class="line">xgb = XGBRegressor(random_state=<span class="number">1</span>)</span><br><span class="line">xgb.fit(x_train, y_train)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.300000012, max_delta_step=0, max_depth=6,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=100, n_jobs=0, num_parallel_tree=1,
             objective='reg:squarederror', random_state=1, reg_alpha=0,
             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',
             validate_parameters=1, verbosity=None)
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xgb_pred = xgb.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'XGBoost'</span>, y_test, xgb_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_146_0-1596543778583.png" alt="output_146_0"></p>
<pre><code>                                                model        mse
0                                 Standard ElasticNet  26.010756
1          GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                            ElasticNet(l1_ratio=0.2)  24.481069
3                                    LinearRegression  22.770784
4                                      Ridge(alpha=1)  22.690411
5                                   Lasso(alpha=0.01)  22.635614
6                                     Voting Ensemble  22.092158
7                                     Poly ElasticNet  20.805986
8                                             XGBoost  13.841454
9                               RandomForest Ensemble  13.781191
10                             GradientBoost Ensemble  13.451877
11  GradientBoost Ensemble w/ tuning (lr=0.01, est...  13.002472
12  GradientBoost Ensemble w/ tuning (lr=0.01, est...  12.607717
13                    RandomForest Ensemble w/ Tuning  11.481491
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_146_2-1596543783973.png" alt="output_146_2"></p>
<br>
<p><strong>주요 Hyper-parameter</strong></p>
<ul>
<li>
<p><strong>random_state:</strong> random seed 고정 값</p>
</li>
<li>
<p><strong>n_jobs:</strong> CPU 사용 갯수</p>
</li>
<li>
<p><strong>learning_rate:</strong> 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1</p>
</li>
<li>
<p><strong>n_estimators:</strong> 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100</p>
</li>
<li>
<p><strong>max_depth:</strong> 트리의 깊이. 과대적합 방지용. default=3.</p>
</li>
<li>
<p><strong>subsample:</strong> 샘플 사용 비율. 과대적합 방지용. default=1.0</p>
</li>
<li>
<p><strong>max_features:</strong> 최대로 사용할 feature의 비율. 과대적합 방지용. default=1.0</p>
<br>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># with hyeper-parameter tuning</span></span><br><span class="line">xgb_t = XGBRegressor(random_state=<span class="number">1</span>, learning_rate=<span class="number">0.01</span>, n_estimators=<span class="number">1000</span>, subsample=<span class="number">0.7</span>, max_features=<span class="number">0.8</span>, max_depth=<span class="number">7</span>)</span><br><span class="line">xgb_t.fit(x_train, y_train)</span><br><span class="line">xgb_t_pred = xgb_t.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'XGBoost w/ Tuning'</span>, y_test, xgb_t_pred)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[16:55:00] WARNING: C:\Users\Administrator\workspace\xgboost-win64_release_1.1.0\src\learner.cc:480: 
Parameters: { max_features } might not be used.

  This may not be accurate due to some parameters are only used in language bindings but
  passed down to XGBoost core.  Or some parameters are not used but slip through this
  verification. Please open an issue if you find above cases.
</code></pre>
<p>​<br>
​</p>
<p><img src="/images/S-Python-sklearn4/output_150_1-1596543791859.png" alt="output_150_1"></p>
<pre><code>                                                model        mse
0                                 Standard ElasticNet  26.010756
1          GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                            ElasticNet(l1_ratio=0.2)  24.481069
3                                    LinearRegression  22.770784
4                                      Ridge(alpha=1)  22.690411
5                                   Lasso(alpha=0.01)  22.635614
6                                     Voting Ensemble  22.092158
7                                     Poly ElasticNet  20.805986
8                                             XGBoost  13.841454
9                               RandomForest Ensemble  13.781191
10                             GradientBoost Ensemble  13.451877
11  GradientBoost Ensemble w/ tuning (lr=0.01, est...  13.002472
12  GradientBoost Ensemble w/ tuning (lr=0.01, est...  12.607717
13                                  XGBoost w/ Tuning  11.987602
14                    RandomForest Ensemble w/ Tuning  11.481491
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_150_3-1596543797426.png" alt="output_150_3"></p>
<br>
<h4 id="4-3-3-lightgbm"><a class="markdownIt-Anchor" href="#4-3-3-lightgbm"></a> 4-3-3. LightGBM</h4>
<p><a href="https://lightgbm.readthedocs.io/en/latest/" target="_blank" rel="noopener">[LightGBM] Document</a></p>
<p><strong>주요 특징</strong></p>
<ul>
<li>
<p>scikit-learn 패키지가 아님</p>
</li>
<li>
<p>성능이 우수함</p>
</li>
<li>
<p>속도도 매우 빠름</p>
<br>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install lightgbm</span><br></pre></td></tr></tbody></table></figure>
<pre><code>Requirement already satisfied: lightgbm in d:\anaconda\lib\site-packages (2.3.1)
Requirement already satisfied: scipy in d:\anaconda\lib\site-packages (from lightgbm) (1.4.1)
Requirement already satisfied: numpy in d:\anaconda\lib\site-packages (from lightgbm) (1.18.1)
Requirement already satisfied: scikit-learn in d:\anaconda\lib\site-packages (from lightgbm) (0.22.1)
Requirement already satisfied: joblib&gt;=0.11 in d:\anaconda\lib\site-packages (from scikit-learn-&gt;lightgbm) (0.14.1)
Note: you may need to restart the kernel to use updated packages.
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMRegressor, LGBMClassifier</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default value 로 학습</span></span><br><span class="line">lgbm = LGBMRegressor(random_state=<span class="number">1</span>)</span><br><span class="line">lgbm.fit(x_train, y_train)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=1, reg_alpha=0.0, reg_lambda=0.0, silent=True,
              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lgbm_pred = lgbm.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'LightGBM'</span>, y_test, lgbm_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_158_0-1596543804030.png" alt="output_158_0"></p>
<pre><code>                                                model        mse
0                                 Standard ElasticNet  26.010756
1          GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                            ElasticNet(l1_ratio=0.2)  24.481069
3                                    LinearRegression  22.770784
4                                      Ridge(alpha=1)  22.690411
5                                   Lasso(alpha=0.01)  22.635614
6                                     Voting Ensemble  22.092158
7                                     Poly ElasticNet  20.805986
8                                             XGBoost  13.841454
9                               RandomForest Ensemble  13.781191
10                             GradientBoost Ensemble  13.451877
11  GradientBoost Ensemble w/ tuning (lr=0.01, est...  13.002472
12                                           LightGBM  12.882170
13  GradientBoost Ensemble w/ tuning (lr=0.01, est...  12.607717
14                                  XGBoost w/ Tuning  11.987602
15                    RandomForest Ensemble w/ Tuning  11.481491
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_158_2-1596543812633.png" alt="output_158_2"></p>
<br>
<p><strong>주요 Hyperparameter</strong></p>
<ul>
<li>
<p><strong>random_state:</strong> random seed 고정 값</p>
</li>
<li>
<p><strong>n_jobs:</strong> CPU 사용 갯수</p>
</li>
<li>
<p><strong>learning_rate:</strong> 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1</p>
</li>
<li>
<p><strong>n_estimators:</strong> 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100</p>
</li>
<li>
<p><strong>max_depth:</strong> 트리의 깊이. 과대적합 방지용. default=3.</p>
</li>
<li>
<p><strong>colsample_bytree:</strong> 샘플 사용 비율 (max_features와 비슷한 개념). 과대적합 방지용. default=1.0</p>
<br>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># with hyper-parameter tuning</span></span><br><span class="line">lgbm_t = LGBMRegressor(random_state=<span class="number">1</span>, learning_rate=<span class="number">0.01</span>, n_estimators=<span class="number">2000</span>, colsample_bytree=<span class="number">0.9</span>, subsample=<span class="number">0.7</span>, max_depth=<span class="number">5</span>)</span><br><span class="line">lgbm_t.fit(x_train, y_train)</span><br><span class="line">lgbm_t_pred = lgbm_t.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'LightGBM w/ Tuning'</span>, y_test, lgbm_t_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_162_0-1596543872792.png" alt="output_162_0"></p>
<pre><code>                                                model        mse
0                                 Standard ElasticNet  26.010756
1          GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                            ElasticNet(l1_ratio=0.2)  24.481069
3                                    LinearRegression  22.770784
4                                      Ridge(alpha=1)  22.690411
5                                   Lasso(alpha=0.01)  22.635614
6                                     Voting Ensemble  22.092158
7                                     Poly ElasticNet  20.805986
8                                             XGBoost  13.841454
9                               RandomForest Ensemble  13.781191
10                             GradientBoost Ensemble  13.451877
11  GradientBoost Ensemble w/ tuning (lr=0.01, est...  13.002472
12                                           LightGBM  12.882170
13  GradientBoost Ensemble w/ tuning (lr=0.01, est...  12.607717
14                                 LightGBM w/ Tuning  12.200040
15                                  XGBoost w/ Tuning  11.987602
16                    RandomForest Ensemble w/ Tuning  11.481491
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_162_2-1596543881929.png" alt="output_162_2"></p>
<br>
<h3 id="4-4-스태킹-stacking"><a class="markdownIt-Anchor" href="#4-4-스태킹-stacking"></a> 4-4. 스태킹 (Stacking)</h3>
<p>개별 모델이 예측한 데이터를 기반으로 <strong>final_estimators</strong> 종합하여 예측을 수행</p>
<ul>
<li>
<p>성능을 극으로 끌오올릴 때 활용하기도 함</p>
</li>
<li>
<p>과대적합을 유발할 수 있다. (특히, 데이터셋이 적은 경우)</p>
<br>
</li>
</ul>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html" target="_blank" rel="noopener">[sklearn.ensemble.<strong>StackingRegressor</strong>] Document</a></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> StackingRegressor</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stack_models = [</span><br><span class="line">    (<span class="string">'elasticnet'</span>, poly_elasticnet),</span><br><span class="line">    (<span class="string">'randomforest'</span>, rfr_t),</span><br><span class="line">    (<span class="string">'lgbm'</span>, lgbm_t)</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">stack_reg = StackingRegressor(stack_models, final_estimator=xgb, n_jobs=<span class="number">-1</span>)</span><br><span class="line">stack_reg.fit(x_train, y_train)</span><br><span class="line">stack_pred = stack_reg.predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'Stacking Ensemble'</span>, y_test, stack_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_171_0-1596543891772.png" alt="output_171_0"></p>
<pre><code>                                                model        mse
0                                 Standard ElasticNet  26.010756
1          GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                            ElasticNet(l1_ratio=0.2)  24.481069
3                                    LinearRegression  22.770784
4                                      Ridge(alpha=1)  22.690411
5                                   Lasso(alpha=0.01)  22.635614
6                                     Voting Ensemble  22.092158
7                                     Poly ElasticNet  20.805986
8                                   Stacking Ensemble  16.906090
9                                             XGBoost  13.841454
10                              RandomForest Ensemble  13.781191
11                             GradientBoost Ensemble  13.451877
12  GradientBoost Ensemble w/ tuning (lr=0.01, est...  13.002472
13                                           LightGBM  12.882170
14  GradientBoost Ensemble w/ tuning (lr=0.01, est...  12.607717
15                                 LightGBM w/ Tuning  12.200040
16                                  XGBoost w/ Tuning  11.987602
17                    RandomForest Ensemble w/ Tuning  11.481491
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_171_2-1596543909420.png" alt="output_171_2"></p>
<br>
<h3 id="4-5-weighted-blending"><a class="markdownIt-Anchor" href="#4-5-weighted-blending"></a> 4-5. Weighted Blending</h3>
<p>각 모델의 <strong>예측값</strong>에 대하여 weight를 곱해혀 최종 output 산출</p>
<ul>
<li>
<p>모델에 대한 가중치를 조절하여, 최종 output을 산출함</p>
</li>
<li>
<p><strong>가중치의 합은 1.0</strong>이 되도록 설정</p>
<br>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">final_outputs = {</span><br><span class="line">    <span class="string">'randomforest'</span>: rfr_t_pred,</span><br><span class="line">    <span class="string">'xgboost'</span>: xgb_t_pred,</span><br><span class="line">    <span class="string">'lgbm'</span>: lgbm_t_pred</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">final_prediction=\</span><br><span class="line">final_outputs[<span class="string">'randomforest'</span>] * <span class="number">0.5</span>\</span><br><span class="line">+final_outputs[<span class="string">'xgboost'</span>] * <span class="number">0.3</span>\</span><br><span class="line">+final_outputs[<span class="string">'lgbm'</span>] * <span class="number">0.2</span>\</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mse_eval(<span class="string">'Weighted Blending'</span>, y_test, final_prediction)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_178_0-1596543918172.png" alt="output_178_0"></p>
<pre><code>                                                model        mse
0                                 Standard ElasticNet  26.010756
1          GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                            ElasticNet(l1_ratio=0.2)  24.481069
3                                    LinearRegression  22.770784
4                                      Ridge(alpha=1)  22.690411
5                                   Lasso(alpha=0.01)  22.635614
6                                     Voting Ensemble  22.092158
7                                     Poly ElasticNet  20.805986
8                                   Stacking Ensemble  16.906090
9                                             XGBoost  13.841454
10                              RandomForest Ensemble  13.781191
11                             GradientBoost Ensemble  13.451877
12  GradientBoost Ensemble w/ tuning (lr=0.01, est...  13.002472
13                                           LightGBM  12.882170
14  GradientBoost Ensemble w/ tuning (lr=0.01, est...  12.607717
15                                 LightGBM w/ Tuning  12.200040
16                                  XGBoost w/ Tuning  11.987602
17                    RandomForest Ensemble w/ Tuning  11.481491
18                                  Weighted Blending  10.585610
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_178_2-1596543958921.png" alt="output_178_2"></p>
<br>
<h3 id="4-6-앙상블-모델-정리"><a class="markdownIt-Anchor" href="#4-6-앙상블-모델-정리"></a> 4-6. 앙상블 모델 정리</h3>
<ol>
<li>
<p>앙상블은 대체적으로 단일 모델 대비 성능이 좋다</p>
</li>
<li>
<p>앙상블을 앙상블하는 기법인 Stacking과 Weighted Blending도 참고해 볼만 하다</p>
</li>
<li>
<p>앙상블 모델은 적절한 <strong>Hyper-parameter Tuning</strong>이 중요하다</p>
</li>
<li>
<p>앙상블 모델은 대체적으로 학습시간이 더 오래 걸린다</p>
</li>
<li>
<p>따라서, 모델 튜닝을 하는 데에 시간이 오래 소유된다</p>
<br>
</li>
</ol>
  <br>
<h2 id="5-cross-validation"><a class="markdownIt-Anchor" href="#5-cross-validation"></a> <strong>5. Cross Validation</strong></h2>
<h3 id="5-1-cross-validation-소개"><a class="markdownIt-Anchor" href="#5-1-cross-validation-소개"></a> 5-1. Cross Validation 소개</h3>
<p><a href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation" target="_blank" rel="noopener">Cross Validation 알아보기</a></p>
<p><a href="https://3months.tistory.com/321" target="_blank" rel="noopener">참고 자료: 딥러닝 모델의 K-겹 교차검증 (K-fold Cross Validation)</a></p>
<p>전에 진행했던 실습에서도 보였듯이, Hyper-parameter의 값은 모델의 성능을 좌우한다. 그러므로 예측 모델의 성능을 높이기 위해, Hyper-parameter Tuning이 매우 중요하다.</p>
<ul>
<li>
<p>이를 실현하기 위해 저희는 Training data을 다시 Training set과 Validation set으로 나눈다. Trainging set에서 Hyper-parameter값을 바뀌가면서 모델 학습하고, Validation set에서 모델의 성능을 평가하여, 모델 성능을 제일 높일 수 있는 Hyper-parameter값을 선택한다</p>
</li>
<li>
<p>하지만, 데이터의 일부만 Validation set으로 사용해 모델 성능을 평가하게 되면, 훈련된 모델이 Test set에 대한 성능 평가의 신뢰성이 떨어질 수 있다. 이를 방지하기 위해 **K-fold Cross Validation (K-겹 교차검증)**을 많이 활용한다</p>
<ul>
<li>K겹 교차 검증은 모든 데이터가 최소 한 번은 validation set으로 쓰이도록 한다<br>
(아래의 그림을 보면, 데이터를 5개로 쪼개 매번 validation set을 바꿔나가는 것을 볼 수 있다)</li>
<li>K번 검증을 통해 구한 K 개의 평가지표 값을 평균 내어 모델 성능을 평가한다</li>
</ul>
</li>
</ul>
<p><img src="/images/S-Python-sklearn4/grid_search_cross_validation.png" alt="CV"></p>
<br>
<p>[예시]</p>
<ul>
<li>
<p>Estimation 1일 때,<br>
Training set: [2, 3, 4, 5] / Validation set: [1]</p>
</li>
<li>
<p>Estimation 2일 때,<br>
Training set: [1, 3, 4, 5] / Validation set: [2]</p>
<br>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_splits = <span class="number">5</span></span><br><span class="line">kfold = KFold(n_splits=n_splits, random_state=<span class="number">1</span>, shuffle = <span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.head()</span><br></pre></td></tr></tbody></table></figure>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table>
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = np.array(df.drop(<span class="string">'MEDV'</span>, <span class="number">1</span>))</span><br><span class="line">Y = np.array(df[<span class="string">'MEDV'</span>])</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lgbm_fold = LGBMRegressor(random_state=<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">1</span></span><br><span class="line">total_error = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> kfold.split(X):</span><br><span class="line">    x_train_fold, x_test_fold = X[train_index], X[test_index]</span><br><span class="line">    y_train_fold, y_test_fold = Y[train_index], Y[test_index]</span><br><span class="line">    lgbm_fold_pred = lgbm_fold.fit(x_train_fold, y_train_fold).predict(x_test_fold)</span><br><span class="line">    error = mean_squared_error(y_test_fold, lgbm_fold_pred)</span><br><span class="line">    print(<span class="string">'Fold = {}, prediction score = {:.2f}'</span>.format(i, error))</span><br><span class="line">    total_error += error</span><br><span class="line">    i+=<span class="number">1</span></span><br><span class="line">print(<span class="string">'---'</span>*<span class="number">10</span>)</span><br><span class="line">print(<span class="string">'Average Error: %s'</span> % (total_error / n_splits))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>Fold = 1, prediction score = 9.76
Fold = 2, prediction score = 20.58
Fold = 3, prediction score = 6.95
Fold = 4, prediction score = 12.18
Fold = 5, prediction score = 10.87
------------------------------
Average Error: 12.06743160435072
</code></pre>
<br>
<h3 id="5-2-hyper-parameter-튜닝"><a class="markdownIt-Anchor" href="#5-2-hyper-parameter-튜닝"></a> 5-2. Hyper-parameter 튜닝</h3>
<p><strong>Hyper-parameter 튜닝</strong> 시 경우의 수가 너무 많으므로 우리는 <strong>자동화</strong>할 틸요가 있다</p>
<p>sklearn 패키지에서 자주 사용되는 hyper-parameter 튜닝을 돕는 클래스는 다음 2가지가 있다:</p>
<ol>
<li><strong>RandomizedSerchCV</strong></li>
<li><strong>GridSerchCV</strong></li>
</ol>
<p><strong>적용하는 방법</strong></p>
<ol>
<li>
<p>사용할 Search 방법을 선택한다</p>
</li>
<li>
<p>hyper-parameter 도메인(값의 범위)을 설정한다 (<code>max_depth</code>, <code>n_estimators</code>… 등등)</p>
</li>
<li>
<p>학습을 시킨 후, 기다린다</p>
</li>
<li>
<p>도출된 결과 값을 모델에 적용하고 성능을 비교한다</p>
<br>
</li>
</ol>
<h4 id="1-randomizedsearchcv"><a class="markdownIt-Anchor" href="#1-randomizedsearchcv"></a> (1) RandomizedSearchCV</h4>
<ul>
<li>
<p>모든 매개 변수 값이 시도되는 것이 아니라 지정된 분포에서 고정 된 수의 매개 변수 설정이 샘플링된다.</p>
</li>
<li>
<p>시도 된 매개 변수 설정의 수는 <code>n_iter</code>에 의해 제공됨.</p>
<br>
</li>
</ul>
<p><strong>주요 Hyper-parameter (LGBM)</strong></p>
<ul>
<li><strong>random_state:</strong> random seed 고정 값</li>
<li><strong>n_jobs:</strong> CPU 사용 갯수</li>
<li><strong>learning_rate:</strong> 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1</li>
<li><strong>n_estimators:</strong> 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100</li>
<li><strong>max_depth:</strong> 트리의 깊이. 과대적합 방지용. default=3.</li>
<li><strong>colsample_bytree:</strong> 샘플 사용 비율 (max_features와 비슷한 개념). 과대적합 방지용. default=1.0</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">params = {</span><br><span class="line">    <span class="string">'learning_rate'</span>: [<span class="number">0.005</span>, <span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.05</span>],</span><br><span class="line">    <span class="string">'n_estimators'</span>: [<span class="number">500</span>, <span class="number">1000</span>, <span class="number">2000</span>, <span class="number">3000</span>],</span><br><span class="line">    <span class="string">'max_depth'</span>: [<span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>],</span><br><span class="line">    <span class="string">'colsample_bytree'</span>: [<span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">1.0</span>],</span><br><span class="line">    <span class="string">'subsample'</span>: [<span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">1.0</span>],</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><figcaption><span>조절하여, 총 몇 회의 시도를 진행할 것인자 정의한다  </span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(회수가 늘어나면, 더 좋은 parameter를 찾을 확률은 올라가지만, 그만큼 시간이 오래걸린다.)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">rcv_lgbm = RandomizedSearchCV(LGBMRegressor(), params, random_state=1, cv=5, n_iter=100, scoring='neg_mean_squared_error')</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rcv_lgbm.fit(x_train, y_train)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>RandomizedSearchCV(cv=5, error_score=nan,
                   estimator=LGBMRegressor(boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           importance_type='split',
                                           learning_rate=0.1, max_depth=-1,
                                           min_child_samples=20,
                                           min_child_weight=0.001,
                                           min_split_gain=0.0, n_estimators=100,
                                           n_jobs=-1, num_leaves=31,
                                           objective=None, random_state=None,
                                           reg_alpha=0.0, reg_lambda=0.0,
                                           silen...
                                           subsample_freq=0),
                   iid='deprecated', n_iter=100, n_jobs=None,
                   param_distributions={'colsample_bytree': [0.8, 0.9, 1.0],
                                        'learning_rate': [0.005, 0.01, 0.03,
                                                          0.05],
                                        'max_depth': [3, 5, 7],
                                        'n_estimators': [500, 1000, 2000, 3000],
                                        'subsample': [0.7, 0.8, 0.9, 1.0]},
                   pre_dispatch='2*n_jobs', random_state=1, refit=True,
                   return_train_score=False, scoring='neg_mean_squared_error',
                   verbose=0)
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rcv_lgbm.best_score_</span><br></pre></td></tr></tbody></table></figure>
<pre><code>-11.132039701508374
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rcv_lgbm.best_params_</span><br></pre></td></tr></tbody></table></figure>
<pre><code>{'subsample': 0.8,
 'n_estimators': 1000,
 'max_depth': 3,
 'learning_rate': 0.05,
 'colsample_bytree': 0.9}
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lgbm_best = LGBMRegressor(learning_rate=<span class="number">0.05</span>, n_estimators=<span class="number">1000</span>, subsample=<span class="number">0.8</span>, max_depth=<span class="number">3</span>, colsample_bytree=<span class="number">0.9</span>)</span><br><span class="line">lgbm_best_pred = lgbm_best.fit(x_train, y_train).predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'RandomSearch LGBM'</span>, y_test, lgbm_best_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_216_0-1596543976320.png" alt="output_216_0"></p>
<pre><code>                                                model        mse
0                                 Standard ElasticNet  26.010756
1          GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                            ElasticNet(l1_ratio=0.2)  24.481069
3                                    LinearRegression  22.770784
4                                      Ridge(alpha=1)  22.690411
5                                   Lasso(alpha=0.01)  22.635614
6                                     Voting Ensemble  22.092158
7                                     Poly ElasticNet  20.805986
8                                   Stacking Ensemble  16.906090
9                                             XGBoost  13.841454
10                              RandomForest Ensemble  13.781191
11                             GradientBoost Ensemble  13.451877
12  GradientBoost Ensemble w/ tuning (lr=0.01, est...  13.002472
13                                           LightGBM  12.882170
14                                  RandomSearch LGBM  12.661917
15  GradientBoost Ensemble w/ tuning (lr=0.01, est...  12.607717
16                                 LightGBM w/ Tuning  12.200040
17                                  XGBoost w/ Tuning  11.987602
18                    RandomForest Ensemble w/ Tuning  11.481491
19                                  Weighted Blending  10.585610
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_216_2-1596543983232.png" alt="output_216_2"></p>
<br>
<h4 id="2-gridserchcv"><a class="markdownIt-Anchor" href="#2-gridserchcv"></a> (2) GridSerchCV</h4>
<ul>
<li>모든 매개 변수 값에 대하여 <strong>완전 탐색</strong>을 시도한다</li>
<li>따라서, 최적화할 parameter가 많다면, <strong>시간이 매우 오래</strong>걸린다</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">params = {</span><br><span class="line">    <span class="string">'learning_rate'</span>: [<span class="number">0.04</span>, <span class="number">0.05</span>, <span class="number">0.06</span>],</span><br><span class="line">    <span class="string">'n_estimators'</span>: [<span class="number">800</span>, <span class="number">1000</span>, <span class="number">1200</span>],</span><br><span class="line">    <span class="string">'max_depth'</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">    <span class="string">'colsample_bytree'</span>: [<span class="number">0.8</span>, <span class="number">0.85</span>, <span class="number">0.9</span>],</span><br><span class="line">    <span class="string">'subsample'</span>: [<span class="number">0.8</span>, <span class="number">0.85</span>, <span class="number">0.9</span>],</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid_search = GridSearchCV(LGBMRegressor(), params, cv=<span class="number">5</span>, n_jobs=<span class="number">-1</span>, scoring=<span class="string">'neg_mean_squared_error'</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid_search.fit(x_train, y_train)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>GridSearchCV(cv=5, error_score=nan,
             estimator=LGBMRegressor(boosting_type='gbdt', class_weight=None,
                                     colsample_bytree=1.0,
                                     importance_type='split', learning_rate=0.1,
                                     max_depth=-1, min_child_samples=20,
                                     min_child_weight=0.001, min_split_gain=0.0,
                                     n_estimators=100, n_jobs=-1, num_leaves=31,
                                     objective=None, random_state=None,
                                     reg_alpha=0.0, reg_lambda=0.0, silent=True,
                                     subsample=1.0, subsample_for_bin=200000,
                                     subsample_freq=0),
             iid='deprecated', n_jobs=-1,
             param_grid={'colsample_bytree': [0.8, 0.85, 0.9],
                         'learning_rate': [0.04, 0.05, 0.06],
                         'max_depth': [3, 4, 5],
                         'n_estimators': [800, 1000, 1200],
                         'subsample': [0.8, 0.85, 0.9]},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring='neg_mean_squared_error', verbose=0)
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid_search.best_score_</span><br></pre></td></tr></tbody></table></figure>
<pre><code>-11.10039780445118
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid_search.best_params_</span><br></pre></td></tr></tbody></table></figure>
<pre><code>{'colsample_bytree': 0.9,
 'learning_rate': 0.05,
 'max_depth': 3,
 'n_estimators': 800,
 'subsample': 0.8}
</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lgbm_best = LGBMRegressor(learning_rate=<span class="number">0.05</span>, n_estimators=<span class="number">800</span>, subsample=<span class="number">0.8</span>, max_depth=<span class="number">3</span>, colsample_bytree=<span class="number">0.9</span>)</span><br><span class="line">lgbm_best_pred = lgbm_best.fit(x_train, y_train).predict(x_test)</span><br><span class="line">mse_eval(<span class="string">'GridSearch LGBM'</span>, y_test, lgbm_best_pred)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/images/S-Python-sklearn4/output_226_0-1596543990796.png" alt="output_226_0"></p>
<pre><code>                                                model        mse
0                                 Standard ElasticNet  26.010756
1          GradientBoost Ensemble w/ tuning (lr=0.01)  24.599441
2                            ElasticNet(l1_ratio=0.2)  24.481069
3                                    LinearRegression  22.770784
4                                      Ridge(alpha=1)  22.690411
5                                   Lasso(alpha=0.01)  22.635614
6                                     Voting Ensemble  22.092158
7                                     Poly ElasticNet  20.805986
8                                   Stacking Ensemble  16.906090
9                                             XGBoost  13.841454
10                              RandomForest Ensemble  13.781191
11                             GradientBoost Ensemble  13.451877
12  GradientBoost Ensemble w/ tuning (lr=0.01, est...  13.002472
13                                           LightGBM  12.882170
14                                    GridSearch LGBM  12.794172
15                                  RandomSearch LGBM  12.661917
16  GradientBoost Ensemble w/ tuning (lr=0.01, est...  12.607717
17                                 LightGBM w/ Tuning  12.200040
18                                  XGBoost w/ Tuning  11.987602
19                    RandomForest Ensemble w/ Tuning  11.481491
20                                  Weighted Blending  10.585610
</code></pre>
<p><img src="/images/S-Python-sklearn4/output_226_2-1596543997347.png" alt="output_226_2"></p>
<br>
<br><script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Hyemin Kim</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://hyemin-kim.github.io/2020/08/04/S-Python-sklearn4/">https://hyemin-kim.github.io/2020/08/04/S-Python-sklearn4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/sklearn/">sklearn</a><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning</a><a class="post-meta__tags" href="/tags/%EC%95%99%EC%83%81%EB%B8%94/">앙상블</a></div><div class="post_share"><div class="social-share" data-image="https://ohiing.com/wp-content/uploads/2020/02/scikit-learn-2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/08/06/S-Python-sklearn5/"><img class="prev-cover" src="https://ohiing.com/wp-content/uploads/2020/02/scikit-learn-2.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Python &gt;&gt; sklearn - (5) 비지도 학습 (Unsupervised Learning)</div></div></a></div><div class="next-post pull-right"><a href="/2020/07/29/S-Python-sklearn3/"><img class="next-cover" src="https://ohiing.com/wp-content/uploads/2020/02/scikit-learn-2.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Python &gt;&gt; sklearn - (3) 회귀 (Regression)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2020/08/13/E-Python-Classification-1/" title="【실습】 Python >> Classification -- 포켓몬 분류 분석"><img class="cover" src="https://s1.ax1x.com/2020/08/25/dcPqB9.png"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-13</div><div class="title">【실습】 Python >> Classification -- 포켓몬 분류 분석</div></div></a></div><div><a href="/2020/08/11/E-Python-LinearRegression-1/" title="【실습】 Python >> EDA & Linear Regression -- 부동산 가격 예측"><img class="cover" src="https://s1.ax1x.com/2020/08/25/dciYCV.png"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-11</div><div class="title">【실습】 Python >> EDA & Linear Regression -- 부동산 가격 예측</div></div></a></div><div><a href="/2020/07/17/S-Python-sklearn0/" title="Python >> sklearn -(0) sklearn 개요"><img class="cover" src="https://ohiing.com/wp-content/uploads/2020/02/scikit-learn-2.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-17</div><div class="title">Python >> sklearn -(0) sklearn 개요</div></div></a></div><div><a href="/2020/07/26/S-Python-sklearn2/" title="Python >> sklearn - (2) 분류 (Classification)"><img class="cover" src="https://ohiing.com/wp-content/uploads/2020/02/scikit-learn-2.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-26</div><div class="title">Python >> sklearn - (2) 분류 (Classification)</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></article></main><footer id="footer" style="background-image: url(https://ohiing.com/wp-content/uploads/2020/02/scikit-learn-2.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Hyemin Kim</div><div class="framework-info"><span>Framework </span><a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk({
      clientID: 'ed00dae828f43e807ca1',
      clientSecret: '6127c1ef27fe0e15655c18f5ce3817472c83d2cd',
      repo: 'hyemin-Kim.github.io',
      owner: 'hyemin-Kim',
      admin: [''],
      id: 'd7b7ad87ea8d1a629c9cb70ef7ad68e8',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: false,
      updateCountCallback: commentCount
    })
    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    $.getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js', initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>