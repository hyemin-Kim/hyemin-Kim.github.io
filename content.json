{"meta":{"title":"Hyemin Kim","subtitle":"김혜민 / 金慧敏","description":"","author":"Hyemin Kim","url":"https://hyemin-kim.github.io","root":"/"},"pages":[{"title":"","date":"2020-05-20T10:31:42.690Z","updated":"2020-05-04T13:34:00.910Z","comments":false,"path":"categories/index - default.html","permalink":"https://hyemin-kim.github.io/categories/index%20-%20default.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"Categories","date":"2020-04-30T15:00:00.000Z","updated":"2020-10-27T11:59:33.979Z","comments":false,"path":"categories/index.html","permalink":"https://hyemin-kim.github.io/categories/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"","date":"2020-05-04T18:01:24.474Z","updated":"2020-05-04T18:01:24.474Z","comments":false,"path":"about/index.html","permalink":"https://hyemin-kim.github.io/about/index.html","excerpt":"","text":"Hello document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"Links","date":"2020-05-20T08:53:04.000Z","updated":"2020-05-21T11:40:00.304Z","comments":false,"path":"link/index.html","permalink":"https://hyemin-kim.github.io/link/index.html","excerpt":"","text":"Some Useful Links Github Hexo Themes Hexo Usage Hexo Plugins document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"","date":"2020-05-20T08:48:36.782Z","updated":"2020-05-04T13:34:13.986Z","comments":false,"path":"tags/index - default.html","permalink":"https://hyemin-kim.github.io/tags/index%20-%20default.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"tagcloud","date":"2020-05-08T05:30:56.000Z","updated":"2020-05-08T05:34:14.280Z","comments":false,"path":"tagcloud/index.html","permalink":"https://hyemin-kim.github.io/tagcloud/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"Tag Cloud","date":"2020-04-30T15:00:00.000Z","updated":"2020-05-21T14:01:11.233Z","comments":false,"path":"tags/index.html","permalink":"https://hyemin-kim.github.io/tags/index.html","excerpt":"","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"}],"posts":[{"title":"SQL >> 테이블 관리 (3)","slug":"S-SQL-Table-3","date":"2020-12-21T09:13:02.000Z","updated":"2020-12-21T09:56:52.571Z","comments":true,"path":"2020/12/21/S-SQL-Table-3/","link":"","permalink":"https://hyemin-kim.github.io/2020/12/21/S-SQL-Table-3/","excerpt":"","text":"테이블 관리 (3) 1. 테이블 제거 1-1. 개념 1-2. 테이블 제거 문법 1-3. 테이블 제거 실습 &gt;&gt; 실습 준비 &gt;&gt; 테이블 제거 실습 2. 임시 테이블 2-1. 개념 2-2. 임시 테이블 생성 문법 2-3. 임시 테이블 실습 3. TRUNCATE 3-1. 개념 3-2. TRUNCATE 문법 3-3. TRUNCATE 실습 1. 테이블 제거 1-1. 개념 존재하는 테이블을 제거할 수 있다. 하지반 테이블 제거 시는 항상 주의해야하고 FK (Foreign Key) 관계도 유의해야 한다. 1-2. 테이블 제거 문법 1DROP TABLE TABLE_NAME 1-3. 테이블 제거 실습 &gt;&gt; 실습 준비 12345678910111213141516CREATE TABLE DIRECTOR( DIRECTOR_ID INT NOT NULL PRIMARY KEY, FIRSTNAME VARCHAR (50), LASTNAME VARCHAR (50));CREATE TABLE FILMS( FILM_ID SERIAL PRIMARY KEY, TITLE VARCHAR (255) NOT NULL, PREMIERE_DAY DATE, DIRECTOR_ID INT NOT NULL, FOREIGN KEY (DIRECTOR_ID) REFERENCES DIRECTOR (DIRECTOR_ID)); 1234567INSERT INTO DIRECTORVALUES (1, '준호', '봉');INSERT INTO FILMSVALUES (1, '기생충', '2019-05-30', 1);COMMIT; 1SELECT * FROM DIRECTOR; 1SELECT * FROM FILMS; &gt;&gt; 테이블 제거 실습 [MISSION 1] 부모 테이블 (DIRECTOR 테이블)을 제거하기 부모 테이블은 바로 제거할 수 없다. 부모 테이블의 컬럼이 자식 테이블에서 첨조되고 있기 때문에 참조 누락성 제약조건으로 인해 제거할 수 없다. 굳이 제거하고 싶으면 CASCADE 옵션을 사용해야한다. 1DROP TABLE DIRECTOR; 지금 DIRECTOR 테이블의 DIRECTOR_ID 컬럼이 FILMS 테이블에서 FK로 참조되고 있기 때문에 FK 제약 조건으로 인해 테이블 제거할 수 없다. 이 경우에 DROP TABLE ... CASCADE 명령어를 이용하여 해당 테이블과 관계된 모든 개체를 함계 삭제한다. 1DROP TABLE DIRECTOR CASCADE; -- CASCADE 옵션으로 제거 성공 12-- 제거 성공 확인SELECT * FROM DIRECTOR; 부모 테이블이 제거된 경우 자식 테이블의 행은 존재하지반 FK 제약 조건은 삭제됨. [MISSION 2] 자식 테이블 (FILMS 테이블)을 제거하기 자식 테이블은 바로 제거할 수 있다. 두 테이블을 다시 생성한 후 자식 테이블 (FILMS 테이블)을 먼저 제거 한다. 1DROP TABLE FILMS; -- 자식 테이블 제거 성공 12-- 제거 성공 확인SELECT * FROM FILMS; 2. 임시 테이블 2-1. 개념 임시 테이블은 DB 접속 세션의 활동 기간 동안 존재하는 테이블이다. 세션이 종료되면 임시 테이블은 자동으로 소멸된다. 2-2. 임시 테이블 생성 문법 12CREATE TEMP TABLE TABLE_NAME (COLUMN); 2-3. 임시 테이블 실습 [실습 1] 임시 테이블 생성 후 세션 재접속 세션을 종료 후 재접속을 하면 임시 테이블이 소멸된다. 임시 테이블 생성 12CREATE TEMP TABLE TB_CUST_TEMP_TEST (CUST_ID INT); 1SELECT * FROM TB_CUST_TEMP_TEST; 테이블에 값을 입력하기 12INSERT INTO TB_CUST_TEMP_TESTVALUES (1); 세션 종료 후 재접속 임시 테이블 불러오기 1SELECT * FROM TB_CUST_TEMP_TEST; 세션을 종료 후 재접속을 하면 임시 테이블이 소멸된 것을 확인할 수 있다. [실습 2] 기존에 존재하는 테이블과 같은 이름으로 임시 테이블 생성 후 제거 같은 이름의 일반 테이블과 임시 테이블이 동시에 존재할 경우: SELECT 문으로 테이블 조회할 때 임시 테이블을 볼러온다 DROP 문으로 테이블 제거할 때도 임시 테이블을 먼저 제거한다. 일반 테이블 생성 12345CREATE TABLE TB_CUST_TEMP_TEST( CUST_ID SERIAL PRIMARY KEY, CUST_NAME VARCHAR NOT NULL); 같은 이름의 임시 테이블 생성 12CREATE TEMP TABLE TB_CUST_TEMP_TEST( CUST_ID INT ); 테이블 조회 1SELECT * FROM TB_CUST_TEMP_TEST; 임시 테이블을 불러오는 것을 확인할 수 있다 테이블 제거 1DROP TABLE TB_CUST_TEMP_TEST; 1SELECT * FROM TB_CUST_TEMP_TEST; 임시 테이블이 제거되고 일반 테이블이 그대로 남아있는 것을 확인할 수 있다. 3. TRUNCATE 3-1. 개념 대용량의 테이블을 빠르게 지우는 방법으로 TRUNCATE가 있다. 테이블의 세그먼트 자체를 바로 지우기 때문에 빠르게 데이터가 지워진다. &gt;&gt; DELETE VS TRUNCATE DELETE TRUNCATE 데이터가 지워지지만 테이블 용량이 줄어 들지 않는다. 테이블을 삭제하지 않고 데이터만 삭제하지만, 테이블 용량이 줄어 든다. 원하는 데이터만 지울 수 있다. 한꺼번에 다 지워야 한다. 삭제 후 잘못 삭제한 것을 되돌릴 수 있다. 삭제 후 되돌릴 수 없다. 속도가 느리다. 속도가 빠르다. 3-2. TRUNCATE 문법 12-- 1개의 테이블 데이터를 빠라게 삭제TRUNCATE TABLE BIG_TABLE; 12-- N개의 테이블 데이터를 빠르게 삭제TRUNCATE TABLE BIG_TABLE_1, BIG_TABLE_2; 3-3. TRUNCATE 실습 먼저 ACCOUNT 테이블과 동일한 새로운 테이블 BIG_TABLE를 만든다. 12CREATE TABLE BIG_TABLE AS SELECT * FORM ACCOUNT; 이제 TRUNCATE 문을 이용하여 BIG_TABLE의 데이터를 모두 삭제한다. 123TRUNCATE TABLE BIG_TABLE;SELECT * FROM BIG_TABLE; * ROLLBACK을 통해 데이터를 다시 복원할 수 없다. 12ROLLBACK;SELECT * FORM BIG_TABLE; document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 9. Table","slug":"【STUDY-SQL】/SQL-9-Table","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-9-Table/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"}]},{"title":"SQL >> 테이블 관리 (2)","slug":"S-SQL-Table-2","date":"2020-12-21T09:03:29.000Z","updated":"2020-12-21T09:19:56.056Z","comments":true,"path":"2020/12/21/S-SQL-Table-2/","link":"","permalink":"https://hyemin-kim.github.io/2020/12/21/S-SQL-Table-2/","excerpt":"","text":"테이블 관리 (2) 1. 컬럼 추가 1-1. 컬럼 추가 문법 1-2. 컬럼 추가 실습 2. 컬럼 제거 2-1. 컬럼 제거 문법 2-2. 컬럼 제거 실습 &gt;&gt; 실습 준비 &gt;&gt; 컬럼 제거 실습 3. 컬럼 데이터 타입 변경 3-1. 컬럼 데이터 타입 변경 문법 3-2. 컬럼 데이터 타입 변경 실습 &gt;&gt; 실습 준비 &gt;&gt; 컬럼 데이터 타입 변경 실습 4. 컬럼 이름 변경 4-1. 컬럼 이름 변경 문법 4-2. 컬럼 이름 변경 실습 &gt;&gt; 실습 준비 &gt;&gt; 컬럼 이름 변경 실습 1. 컬럼 추가 1-1. 컬럼 추가 문법 123ALTER TABLE TABLE_NAME ADD COLUMN COL_NAME_1 SETTING ADD COLUMN COL_NAME_2 SETTING 1-2. 컬럼 추가 실습 ORACLE (오라클): create, drop, alter 명령어 (DDL)를 치는 순간 자동으로 commit이 됩니다. 즉, commit 할 필요가 없습니다. 하지만 delete, update, merge, insert 등 명령어는 commit 필요 PostgreSQL: create, drop, alter, delete, update, merge, insert 명령어 모두 commit 을 필요합니다. (1) 아래와 같은 테이블을 생성한다 1234CREATE TABLE TB_CUST( CUST_ID SERIAL PRIMARY KEY, CUST_NAME VARCHAR(50) NOT NULL); (2) 폰변호를 저장할 컬럼을 추가한다 12ALTER TABLE TB_CUST ADD COLUMN PHONE_NUMBER VARCHAR(13); (3) 팩스번호 및 이메일 주소를 저장할 컬럼을 추가한다. (한 번에 2개 추가) 123ALTER TABLE TB_CUST ADD COLUMN FAX_NUMBER VARCHAR(13), ADD COLUMN EMAIL_ADDR VARCHAR(50); &gt;&gt; TB_CUST의 생성 과정(DDL) 확인 (4) NOT NULL 제약 컬럼 추가 &gt;&gt; 문제 테이블에 데이터가 있을 때는 NOT NULL 제약 컬럼을 바로 추가할 수 없다. 추가하는 순간 해당 컬럼에 값이 없어서 NOT NULL 제약을 위반하게 되기 때문이다. [[ 실습 ]] 먼저 테이블에 데이터를 입력한다 12345INSERT INTO TB_CUSTVALUES(1, '홍길동', '010-1234-5678', '02-123-1234', 'dbmsexpert@naver.com');COMMIT; 데이터를 입력 후 아래와 같이 NOT NULL 컬럼을 추가하면 기존에 레코드가 있기 때문에 ERROR가 발생한다. 12ALTER TABLE TB_CUST ADD COLUMN ADDRESS VARCHAR NOT NULL; &gt;&gt; 해결 이런 경우 해결책은 우선 NULL 조건으로 컬럼을 추가한 다음, 컬럼 값을 부여하는 UPDATE 작업을 진행하고 다시 NOT NULL 제약을 추가하는 것이다. [[ 실습 ]] 먼저 NULL 조건으로 컬럼을 추가한다. 12ALTER TABLE TB_CUST ADD COLUMN ADDRESS VARCHAR NULL; 그 다음 ADDRESS 컬럼을 UPDATE 한다. (컬럼 값 부여) 12345UPDATE TB_CUST SET ADDRESS = '서울시 영등포구' WHERE CUST_ID = 1; COMMIT; 마지막으로 ALTER COLUMN으로 NOT NULL로 제약 조건을 준다. 12ALTER TABLE TB_CUSTALTER COLUMN ADDRESS SET NOT NULL; 2. 컬럼 제거 2-1. 컬럼 제거 문법 1234ALTER TABLE TABLE_NAME DROP COLUMN COL_NAME_1, DROP COLUMN COL_NAME_2, ... ; 123-- CASCADE 옵션: 해당 컬럼과 관련 있는 모든 개체들이 함께 삭제(DROP)된다.ALTER TABLE TABLE_NAME DROP COLUMN COL_NAME CASCADE; 2-2. 컬럼 제거 실습 &gt;&gt; 실습 준비 1234CREATE TABLE PUBLISHERS ( PUBLISHER_ID SERIAL PRIMARY KEY, NAME VARCHAR NOT NULL); 1234CREATE TABLE CATEGORIES ( CATEGORY_ID SERIAL PRIMARY KEY, NAME VARCHAR NOT NULL); 1234567891011CREATE TABLE BOOKS ( BOOK_ID SERIAL PRIMARY KEY, TITLE VARCHAR NOT NULL, ISBN VARCHAR NOT NULL, PUBLISHED_DATE DATE NOT NULL, DESCRIPTION VARCHAR, CATEGORY_ID INT NOT NULL, PUBLISHER_ID INT NOT NULL, FOREIGN KEY (CATEGORY_ID) REFERENCES CATEGORIES (CATEGORY_ID), FOREIGN KEY (PUBLISHER_ID) REFERENCES PUBLISHERS (PUBLISHER_ID)); 3개의 TABLE을 하나의 뷰로 생성한다. 1234567891011121314CREATE VIEW BOOK_INFO AS SELECT B.BOOK_ID, B.TITLE, B.ISBN, B.PUBLISHED_DATE, P.NAMEFROM BOOKS B, PUBLISHERS PWHERE P.PUBLISHER_ID = B.PUBLISHER_IDORDER BY B.TITLE; &gt;&gt; 컬럼 제거 실습 관계: 한 개의 카테고리가 여러 개의 책을 가진다. 한 개의 책은 반드시 카테고리를 가진다. 한 개의 출판사는 여러 개의 책을 출판한다. 한 개의 책은 반드시 출판사를 가진다. [실습 1] BOOKS 테이블에서 CATEGORY_ID 컬럼을 제거한다 1SELECT * FROM BOOKS; 123ALTER TABLE BOOKS DROP COLUMN CATEGORY_ID;SELECT * FROM COOKS; BOOKS 테이블은 자식 테이블이므로 CATEGORY_ID 컬럼은 제거가 가능하다. 컬럼이 제거되면서 CATEGORY_ID의 FK (Foreign Key) 도 함께 삭제된다. [실습 2] BOOKS 테이블에서 PUBLISHER_ID 컬럼을 제거한다 1ALTER TABLE BOOKS DROP COLUMN PUBLISHER_ID; PUBLISHER_ID 컬럼을 제거하고자 하는 경우 위와 같은 에러가 발생한다. 해당 컬럼은 BOOK_INFO 뷰에서 참조하고 있기 때문이다. 이런 경우에는 CASCADE 옵션을 줘서 삭제한다. 123ALTER TABLE BOOKS DROP COLUMN PUBLISHER_ID CASCADE;SELECT * FROM BOOKS; 1SELECT * FROM BOOK_INFO; 컬럼 삭제에는 성공했지만 BOOK_INFO 뷰도 같이 DROP 되었다. [실습 3] 동시에 N개의 컬럼을 제거한다 123ALTER TABLE BOOKS DROP COLUMN ISBN, DROP COLUMN DESCRIPTION; 3. 컬럼 데이터 타입 변경 3-1. 컬럼 데이터 타입 변경 문법 1234ALTER TABLE TABLE_NAME ALTER COLUMN COL_NAME_1 TYPE NEW_TYPE, ALTER COLUMN COL_NAME_2 TYPE NEW_TYPE, ... ; 3-2. 컬럼 데이터 타입 변경 실습 &gt;&gt; 실습 준비 12345678CREATE TABLE ASSETS ( ID SERIAL PRIMARY KEY, NAME TEXT NOT NULL, ASSET_NO VARCHAR(10) NOT NULL, DESCRIPTION TEXT, LOCATION TEXT, DATE_ACQUIRED DATE NOT NULL); 1234567891011INSERT INTO ASSETS ( NAME, ASSET_NO, LOCATION, DATE_ACQUIRED)VALUES('Server', '10001', 'Server room', '2017-01-01'),('UPS', '10002', 'Server room', '2017-01-02');COMMIT; 1SELECT * FROM ASSETS; &gt;&gt; 컬럼 데이터 타입 변경 실습 [MISSION 1] NAME, DESCRIPTION, LOCATION 컬럼의 데이터 타입을 TEXT에서 VARCHAR로 바꾸기 12-- 1개 컬럼의 데이터 타입 변경ALTER TABLE ASSETS ALTER COLUMN NAME TYPE VARCHAR(50); 1234-- 한번에 N개 컬럼의 데이터 타입 변경ALTER TABLE ASSETS ALTER COLUMN DESCRIPTION TYPE VARCHAR(100), ALTER COLUMN LOCATION TYPE VARCHAR(500); [MISSION 2] ASSET_NO 컬럼의 데이터 타입을 VARCHAR 에서 INT로 바꾸기 그냥 TYPE INT 로 바꾸면 ERROR 가 발생한다 1ALTER TABLE ASSETS ALTER COLUMN ASSET_NO TYPE INT; [주의] USING col_name::integer 구문을 추가해야 함 12ALTER TABLE ASSETS ALTER COLUMN ASSET_NO TYPE INT USING ASSET_NO::INTEGER; 4. 컬럼 이름 변경 4-1. 컬럼 이름 변경 문법 12ALTER TABLE TABLE_NAME RENAME COLUMN COL_NAME_OLD TO COL_NAME_NEW; 4-2. 컬럼 이름 변경 실습 &gt;&gt; 실습 준비 123DROP TABLE CUSTOMER_GROUPS;DROP TABLE CUSTOMERS;DROP VIEW CUSTOMER_DATA; 12345678910111213CREATE TABLE CUSTOMER_GROUPS ( ID SERIAL PRIMARY KEY, NAME VARCHAR NOT NULL);CREATE TABLE CUSTOMERS ( ID SERIAL PRIMARY KEY, NAME VARCHAR NOT NULL, PHONE VARCHAR NOT NULL, EMAIL VARCHAR, GROUP_ID INT, FOREIGN KEY (GROUP_ID) REFERENCES CUSTOMER_GROUPS (ID)); 1SELECT * FROM CUSTOMER_GROUPS; 1SELECT * FROM CUSTOMERS; 123456789CREATE VIEW CUSTOMER_DATAAS SELECT C.ID, C.NAME, G.NAME CUSTOMER_GROUPFROM CUSTOMERS C, CUSTOMER_GROUPS GWHERE C.GROUP_ID = G.ID; 1SELECT * FROM CUSTOMER_DATA; &gt;&gt; 컬럼 이름 변경 실습 [MISSION 1] CUSTOMERS 테이블 EMIAL 컬럼의 이름을 바꾸기: EMAIL --&gt; CONTACT_EMAIL 12ALTER TABLE CUSTOMERS RENAME COLUMN EMIAL TO CONTACT_EMAIL; 1SELECT * FROM CUSTOMERS; [MISSION 2] CUSTOMER_GROUPS 테이블의 NAME 컬럼의 이름을 바꾸기: NAME --&gt; GROUP_NAME 12ALTER TABLE CUSTOMER_GROUPS RENAME COLUMN NAME TO GROUP_NAME; 1SELECT * FROM CUSTOMER_GROUPS; 뷰 CUSTOMER_DATA 에서 참조중인 CUSTOMER_GROUP 테이블의 NAME 컬럼의 이름도 바뀌었는지 살펴본다 1SELECT * FROM CUSTOMER_DATA; 컬럼명이 바뀐 것이 뷰에 자동으로 적용된 것을 확인할 수 있다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 9. Table","slug":"【STUDY-SQL】/SQL-9-Table","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-9-Table/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"}]},{"title":"SQL >> 테이블 관리 (1)","slug":"S-SQL-Table-1","date":"2020-12-21T08:53:24.000Z","updated":"2020-12-21T09:56:10.687Z","comments":true,"path":"2020/12/21/S-SQL-Table-1/","link":"","permalink":"https://hyemin-kim.github.io/2020/12/21/S-SQL-Table-1/","excerpt":"","text":"테이블 관리 (1) 1. 데이터 타입 &gt;&gt; Boolean, Character, Numeric &gt;&gt; Time, Arrays, JSON 2. 테이블 생성 3. CTAS (CREATE TABLE AS SELECT) 3-1. 개념 3-2. CTAS 문법 3-3. CTAS 실습 4. 테이블 구조 변경 5. 테이블 이름 변경 5-1. 테이블 이름 변경 문법 5-2. 테이블 이름 변경 실습 5-3. 걸론 1. 데이터 타입 테이블은 컬럼으로 이루어져 있고 컬럼은 다양한 데이터 타입을 지원한다. 이는 RDBMS가 제 역할을 하는데 있어서 매우 중요하다. 다양한 데이터 타입 지원: Boolean Character Numeric Time Arrays JSON &gt;&gt; Boolean, Character, Numeric 데이터 타입 세부 타입 설명 Boolean Boolean 참과 거짓의 값을 저장한다. Character CHAR 고정형 길이의 문자열을 저장한다. 만약 CHAR(10)인데 'ABCDE’만 입력할 경우 실제로는 'ABCDE '로 뒤에 공백을 붙여 저장한다. VARCHAR 가변형 길이의 분자열을 저장한다. 만약 VARCHAR(10) 인데 'ABCDE’만 입력할 경우 실제로 'ABCDE’만 저장한다. (뒤에 공백을 붙이지 않는다) TEXT 대용량의 문자데이터를 저장한다. Numeric INT 정수형데이터를 저장한다. 크기는 4byte이다. (범위는 -2,147,483,648 to 2,147,483,647) SMALLINT 정수형 데이터를 저장한다. 크기는 2byte이다. (범위는 -32,768 to 32,767) float 부동 소수점의 데이터를 저장한다. 크기는 8byte이다. numeric NUMERIC(15, 2)와 같이 전체 크기와 소수점의 자리를 지정할 수 있다. [실습] 123456789101112CREATE TABLEDATA_TYPE_TEST_1( A_BOOLEAN BOOLEAN, B_CHAR CHAR(10), C_VARCHAR VARCHAR(10), D_TEXT TEXT, E_INT INT, F_SMALLINT SMALLINT, G_FLOAT FLOAT, H_NUMERIC NUMERIC(15, 2)); 123456789101112131415INSERT INTO DATA_TYPE_TEST_1VALUES( TRUE, -- A_BOOLEAN 'ABCDE', -- B_CHAR 'ABCDE', -- C_VARCHAR 'TEXT', -- D_TEXT 1000, -- E_INT 10, -- F_SMALLINT 10.12345, -- G_FLOAT 10.25 -- H_NUMERIC);COMMIT; 1SELECT * FROM DATA_TYPE_TEST_1; &gt;&gt; Time, Arrays, JSON 데이터 타입 세부 타입 설명 Time Date 일자 데이터를 저장한다. TIME 시간 데이터를 저장한다. TIMESTAMP 일자와 시간 데이터를 모두 저장한다. Arrays array 배열 형식의 데이터를 저장한다. 한개의 컬럼에 여러개의 데이터를 동시에 저장할 수 있으며 저장한 데이터의 순서로 조회할 수 있다. JSON JSON JSON형식의 데이터를 저장한다. JSON형식의 데이터를 입력해서 JSON형식대로 각 LEVEL의 데이터를 저장할 수 있다. [실습] 12345678CREATE TABLE DATA_TYPE_TEST_2( A_DATE DATE, B_TIME TIME, C_TIMESTAMP TIMESTAMP, D_ARRAY TEXT[], E_JSON JSON); 1234567891011INSERT INTO DATA_TYPE_TEST_2VALUES( CURRENT_DATE, -- A_DATE LOCALTIME, -- B_TIME CURRENT_TIMESTAMP, -- C_TIMESTAMP ARRAY ['010-1234-1234', '010-4321-4321'], -- D_ARRAY '{\"customer\": \"John Doe\", \"items\": {\"product\": \"Beer\", \"qty\": 6}}' -- E_JSON);COMMIT; 1SELECT * FROM DATA_TYPE_TEST_2; 2. 테이블 생성 테이블은 데이터를 담는 그릇으로써 반드시 생성해야만 데이터를 저장할 수 있다. &gt;&gt; 테이블 생성 시 컬럼의 제약 조건 제약조건명 설명 NOT NULL 해당 제약 조건이 있는 컬럼은 NULL이 저장될 수 없다. UNIQUE 해당 제약 조건이 있는 컬럼의 값은 테이블 내에서 유일해야 한다. PRIMARY KEY 해당 제약 조건이 있는 컬럼의 값은 테이블 내에서 유일해야 하고 반드시 NOT NULL이어야 한다. CHECK 해당 제약 조건이 있는 컬럼은 지정하는 조건에 맞는 값이 들어가야 한다. REFERENCES 해당 제약 조건이 있는 컬럼의 값은 참조하는 테이블의 특정 컬럼에 값이 존재해야 한다. &gt;&gt; 테이블 생성 실습 123456789CREATE TABLE ACCOUNT( USER_ID SERIAL PRIMARY KEY, USERNAME VARCHAR (50) UNIQUE NOT NULL, PASSWORD VARCHAR (50) NOT NULL, EMAIL VARCHAR (355) UNIQUE NOT NULL, CREATED_ON TIMESTAMP NOT NULL, LAST_LOGIN TIMESTAMP); 12345CREATE TABLE ROLE( ROLE_ID SERIAL PRIMARY KEY, ROLE_NAME VARCHAR (255) UNIQUE NOT NULL); 12345678910111213CREATE TABLE ACCOUNT_ROLE( USER_ID INTEGER NOT NULL, ROLE_ID INTEGER NOT NULL, GRANT_DATE TIMESTAMP WITHOUT TIME ZONE, PRIMARY KEY (USER_ID, ROLE_ID), -- 기본키는 USER_ID + ROLE_ID로 한다 CONSTRAINT ACCOUNT_ROLE_ROLE_ID_FKEY FOREIGN KEY (ROLE_ID) -- ROLE_ID 컬럼은 ROLE 테이블의 ROLE_ID 컬럼을 참조한다 REFERENCES ROLE (ROLE_ID) MATCH SIMPLE -- ROLE_ID 컬럼은 ROLE 테이블의 ROLE_ID컬럼에 대한 삭제 혹은 변경 시 아무것도 하지 않는다 ON UPDATE NO ACTION ON DELETE NO ACTION, CONSTRAINT ACCOUNT_ROLE_USER_ID_FKEY FOREIGN KEY (USER_ID) -- USER_ID 컬럼은 ACCOUNT 테이블의 USER_ID 컬럼을 참조한다 REFERENCES ACCOUNT (USER_ID) MATCH SIMPLE -- USER_ID 컬럼은 ACCOUNT 테이블의 USER_ID 컬럼에 대한 삭제 혹은 변경 시 아무것도 하지 않는다 ON UPDATE NO ACTION ON DELETE NO ACTION); ACCOUNT 테이블과 ROLE 테이블은 다대다의 매칭관계이다. 두 테이블의 내용이 매칭될 수 있도록 ACCOUNT_ROLE 테이블을 생성하여 1대다의 관계를 만들어 준다. 12345INSERT INTO ACCOUNTVALUES(1, '홍길동', '1234', 'honggd@naver.com', CURRENT_TIMESTAMP, NULL);COMMIT; 1SELECT * FROM ACCOUNT; 1234INSERT INTO ROLEVALUES (1, 'DBA');COMMIT; 1SELECT * FROM ROLE; 1234INSERT INTO ACCOUNT_ROLE VALUES (1, 1, CURRENT_TIMESTAMP);COMMIT; 1SELECT * FROM ACCOUNT_ROLE; &gt;&gt; 에러 (1) 참조 누락성 제약 조건 위반 참조키(Foreign Key)에 해당 데이터가 존재하지 않는 경우 12INSERT INTO ACCOUNT_ROLEVALUES (2, 1, CURRENT_TIMESTAMP); 12INSERT INTO ACCOUNT_ROLEVALUES (1, 2, CURRENT_TIMESTAMP); (2) 고유 제약 조건 위반 중복값이 없어야 하는 PRIMARY KEY에 중복이 발생한 경우 12INSERT INTO ACCOUNT_ROLEVALUES (1, 1, CURRENT_TIMESTAMP); (3) 참조 시 갱신/삭제 불가 데이터가 참조되어 있을 때 해당 데이터를 갱신/삭제 불가한다. 123UPDATE ACCOUNTSET USER_ID = 2WHERE USER_ID = 1; 12DELETE FROM ACCOUNTWHERE USER_ID = 1; 3. CTAS (CREATE TABLE AS SELECT) 3-1. 개념 CTAS는 CREATE TABLE AS SELECT의 약어로써 SELECT문을 기반으로 CREATE TABLE을 할 수 있는 CREATE문이다. 3-2. CTAS 문법 123CREATE TABLE NEW_TABLE -- 새로운 테이블의 이름을 설정한다ASSELECT문 -- SELECT문을 작성한다 123CREATE TABLE NEW_TABLE (NEW_COLUMN_1, NEW_COLUMN_2) -- 새로운 테이블명의 이름과 컬럼명을 설정한다ASSELECT문 -- SELECT문을 작성한다 123CREATE TABLE IF NOT EXISTS NEW_TABLE -- 기존에 테이블이 존재하지 않는 경우만 생성한다ASSELECT문 -- SELECT문을 작성한다 3-3. CTAS 실습 [MISSION] 액션영화의 정보만으로 신규 테이블을 생성 1SELECT * FROM CATEGORY WHERE CATEGORY_ID = 1; 1234567891011SELECT A.FILM_ID, A.TITLE, A.RELEASE_YEAR, A.LENGTH, A.RATINGFROM FILM A, FILM_CATEGORY BWHERE A.FILM_ID = B.FILM_IDAND B.CATEGORY_ID = 1; &gt;&gt; 액션 영화 테이블 생성 123456789101112CREATE TABLE ACTION_FILM ASSELECT A.FILM_ID, A.TITLE, A.RELEASE_YEAR, A.LENGTH, A.RATINGFROM FILM A, FILM_CATEGORY BWHERE A.FILM_ID = B.FILM_IDAND B.CATEGORY_ID = 1; 1SELECT * FROM ACTION_FILM; 4. 테이블 구조 변경 한번 만들어진 테이블이라고 하더라도 데이블 구조를 변경할 수 있다. 이 기능으로 인해 업무변화에 유연하게 대처할 수 있다. &gt;&gt; 테이블 구조 변경 실습 123456CREATE TABLE LINKS ( LINK_ID SERIAL PRIMARY KEY, TITLE VARCHAR (512) NOT NULL, URL VARCHAR (1024) NOT NULL UNIQUE); 123-- 1) ACTIVE 컬럼을 추가ALTER TABLE LINKS ADD COLUMN ACTIVE BOOLEAN;SELECT * FROM LINKS; 123-- 2) ACTIVE 컬럼을 제거ALTER TABLE LINKS DROP COLUMN ACTIVE;SELECT * FROM LINKS; 123-- 3) TITLE 컬럼을 LINK_TITLE 컬럼으로 변경ALTER TABLE LINKS RENAME COLUMN TITLE TO LINK_TITLE;SELECT * FROM LINKS; 123-- 4) TARGET 컬럼을 추가ALTER TABLE LINKS ADD COLUMN TARGET VARCHAR(10);SELECT * FROM LINKS; 1234-- 5) TARGET 컬럼의 DEFAULT값을 \"_blank\"로 설정ALTER TABLE LINKS ALTER COLUMN TARGETSET DEFAULT '_blank';SELECT * FROM LINKS; 데이터 추가 해보기 123INSERT INTO LINKS (LINK_TITLE, URL)VALUES('PostgreSQL Tutorial', 'http://www.postgresqltutorial.com/'); 1SELECT * FROM LINKS; 12-- 6) 체크 제약 조건 추가ALTER TABLE LINKS ADD CHECK (TARGET IN ('_self', '_blank', '_parent', '_top')); TARGET 컬럼의 체크 제약 조건에 없는 ‘whatever’ 값으로 INSERT 시도 12INSERT INTO LINKS (LINK_TITLE, URL, TARGET)VALUES('PostgreSQL', 'http://www.postgresql.org/', 'whatever'); TARGET 컬럼의 체크 제약 조건에 없는 ‘whatever’ 값으로 INSERT 시도 12INSERT INTO LINKS (LINK_TITLE, URL, TARGET)VALUES('PostgreSQL', 'http://www.postgresql.org/', '_self'); 5. 테이블 이름 변경 한번 만들어진 테이블이라고 하더라도 테이블 이름을 변경할 수 있다. 이 기능으로 인해 업무변화에 유연하게 대처할 수 있다. 5-1. 테이블 이름 변경 문법 12ALTER TABLE OLD_TABLE_NAME RENAME TO NEW_TABLE_NAME 5-2. 테이블 이름 변경 실습 &gt;&gt; 테이블 이름 변경 실습 (1) [MISSION] VENDORS 테이블을 SUPPLIERS 테이블로 변경 12345678-- VENDORS 테이블 생성CREATE TABLE VENDORS( ID SERIAL PRIMARY KEY, NAME VARCHAR NOT NULL);SELECT * FROM VENDORS; 12-- VENDORS 테이블의 이름을 SUPPLIERS 로 변경ALTER TABLE VENDORS RENAME TO SUPPLIERS; 1SELECT * FROM SUPPLIERS; 1SELECT * FROM VENDORS; &gt;&gt; 테이블 이름 변경 실습 (2) 123456-- SUPPLIER_GROUPS 테이블 생성CREATE TABLE SUPPLIER_GROUPS( ID SERIAL PRIMARY KEY, NAME VARCHAR NOT NULL) 12345-- SUPPLIERS 테이블에 컬럼 추가 후 FK 생성ALTER TABLE SUPPLIERS ADD COLUMN GROUP_ID INT NOT NULL; -- SUPPLIERS 테이블에 GROUP_ID 컬럼 추가ALTER TABLE SUPPLIERS ADD FOREIGN KEY (GROUP_ID) REFERENCES SUPPLIER_GROUPS (ID); -- SUPPLIER_GROUPS 테이블의 ID 컬럼을 참조하여 SUPPLIERS 테이블의 GROUP_ID 컬러의 값을 지정 123456789-- 아래와 같은 뷰를 생성 (뷰는 실체하는 데이터가 아닌 보기용)CREATE VIEW SUPPLIER_DATA ASSELECT S.ID, S.NAME, G.NAME \"GROUP\"FROM SUPPLIERS S, SUPPLIER_GROUPS GWHERE G.ID = S.GROUP_ID; 1) 먼저 SUPPLIERS 테이블의 생성 과정 (DDL)를 살펴 본다 [SUPPLIERS 테이블 --&gt; 우클릭 --&gt; SQL 생성 --&gt; DDL] 지금 SUPPLIERS 테이블의 GROUP_ID 컬럼은 SUPPLIER_GROUPS 테이블의 ID 컬럼을 참조하고 있다. 그렇다면 여기서 부모 테이블인 SUPPLIER_GROUPS 테이블의 테이블명을 바꾸면 자식 테이블인 SUPPLIERS 테이블은 어떻게 될까? 2) SUPPLIER_GROUPS 테이블의 이름을 “GROUPS” 로 바꾼다 1ALTER TABLE SUPPLIER_GROUPS RENAME TO GROUPS; 다시 SUPPLIERS 테이블의 DDL을 확인: 이제 SUPPLIERS 테이블의 GROUP_ID 컬럼은 GROUPS 테이블의 ID 컬럼을 참조하고 있다. 즉 테이블명의 변경이 자동으로 반영된다. 그렇다면 우리가 만들었던 SUPPLIER_DATA 뷰는 어떻게 되었을까? 테이블명이 바뀌었음에도 불구하고 자동으로 GROUPS 테이블을 참조하고 있다. 즉 테이블명의 변경이 자동으로 반영된다. 5-3. 걸론 ALTER TALBE 문을 활용하여 테이블의 이름을 변경 가능. 테이블 이름 변경하면, 기존의 참조무걸성 제약조건이나 뷰 등이 자동으로 반영된다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 9. Table","slug":"【STUDY-SQL】/SQL-9-Table","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-9-Table/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"}]},{"title":"SQL >> 데이터 조작 (2)","slug":"S-SQL-Manipulation-2","date":"2020-12-07T04:46:50.000Z","updated":"2020-12-07T06:43:36.517Z","comments":true,"path":"2020/12/07/S-SQL-Manipulation-2/","link":"","permalink":"https://hyemin-kim.github.io/2020/12/07/S-SQL-Manipulation-2/","excerpt":"","text":"데이터 조작 (2) 1. DELETE 문 1-1. 개념 1-2. DELETE 문법 (1) DELETE (2) DELETE JOIN 1-3. DELETE 문 실습 (1) 특정 조건의 행을 삭제 (2) DELETE JOIN의 사용 (3) 전체 행 삭제 2. UPSERT 문 2-1. 개념 2-2. UPSERT 문법 2-3. UPSERT 문 실습 &gt;&gt; 실습 준비 &gt;&gt; UPSERT 문 실습 – DO NOTHING &gt;&gt; UPSERT 문 실습 – UPDATE 3. EXPORT 작업 3-1. 개념 3-2. EXPORT 작업 실습 &gt;&gt; 실습 – 엑셀(.CSV) 형식으로 출력 &gt;&gt; 실습 – 텍스트(.TXT) 파일로 출력 &gt;&gt; 실습 – 컬럼명 없이 출력 4. IMPORT 작업 4-1. 개념 4-2. IMPORT 작업 실습 &gt;&gt; 실습 준비 &gt;&gt; 실습 – 엑셀파일을 적재 &gt;&gt; 실습 – 텍스트 파일을 적재 &gt;&gt; 실습 – 컬럼명이 없는 엑셀 파일 적재 1. DELETE 문 1-1. 개념 DELETE문은 테이블에서 특정 데이터를 삭제하거나 테이블 내에 존재하는 모든 데이터를 삭제할 수 있다. 1-2. DELETE 문법 (1) DELETE 12345DELETE FROM TARGET_TABLE A WHERE 조건식; -- 삭제할 행에 대한 조건 COMMIT; (2) DELETE JOIN DELETE 시 다른 테이블의 내용을 참조하고 싶을 때 DELETE JOIN 문을 사용한다. 123456DELETE FROM TARGET_TABLE A USING REF_TABLE B -- 참조 테이블 지정 WHERE 조건식; -- JOIN &amp; DELETE 조건 COMMIT; 1-3. DELETE 문 실습 1SELECT * FROM LINK; 1SELECT * FROM LINK_TMP; (1) 특정 조건의 행을 삭제 [MISSION] LINK테이블에서 ID가 5인 행을 삭제 1SELECT * FROM LINK; 1234567DELETE FROM LINK WHERE ID = 5; COMMIT;SELECT * FROM LINK; (2) DELETE JOIN의 사용 [MISSION] LINK_TMP 테이블에서 ID가 LINK 테이블의 ID 와 매칭된 행을 삭제 1SELECT * FROM LINK; 1SELECT * FROM LINK_TMP; 1234DELETE FROM LINK_TMP A USING LINK B WHERE A.ID = B.ID; 1SELECT * FROM LINK_TMP; (3) 전체 행 삭제 &gt;&gt; LINK 테이블 1SELECT * FROM LINK; 1234DELETE FROM LINK;COMMIT;SELECT * FROM LINK; &gt;&gt; LINK_TMP 테이블 1SELECT * FROM LINK_TMP; 1234DELETE FROM LINK_TMP;COMMIT;SELECT * FROM LINK_TMP; 2. UPSERT 문 2-1. 개념 UPSERT문은 INSERT를 시도할 때 조건(상황)에 따라 UPDATE를 할 수 있는 구문이다. 복잡한 업무 처리에 자주 사용된다. 2-2. UPSERT 문법 123INSERT INTO TABLE_NAME (COLUMN_1) VALUES (VALUE_1) -- INSERT 시도ON CONFLICT TARGET ACTION; -- 충돌 시 다른 액션 2-3. UPSERT 문 실습 &gt;&gt; 실습 준비 1234567CREATE TABLE CUSTOMERS( CUSTOMER_ID SERIAL PRIMARY KEY, NAME VARCHAR UNIQUE, EMAIL VARCHAR NOT NULL, ACTIVE BOOL NOT NULL DEFAULT TRUE); NAME 컬럼이 UNIQUE 제약 조건 컬럼임을 주의 한다. 즉 NAME 컬럼은 중복된 값이 존재할 수 없다 1234567INSERT INTO CUSTOMERS (NAME, EMAIL)VALUES ('IBM', 'contact@ibm.com'), ('Microsoft', 'contact@microsoft.com'), ('Intel', 'contact@intel.com'); COMMIT; 1SELECT * FROM CUSTOMERS; &gt;&gt; UPSERT 문 실습 – DO NOTHING DO NOTHING: INSERT 액션이 충돌 시 (기존에 존재할 경우) 아무것도 안함 [MISSION] Microsoft(기존에 존재하는 NAME)에 EMAIL 주소 추가 'Microsoft’라는 NAME이 이미 존재하므로 NAME의 UNIQUE 조건과 충돌 이런 경우에 아무 액션도 취하지 않음 (즉, 변화 없음) 1234567INSERT INTO CUSTOMERS (NAME, EMAIL) VALUES ('Microsoft', 'hotline@microsoft.com')ON CONFLICT (NAME) -- 충돌 시(기존에 존재할 경우)DO NOTHING; -- 아무 것도 안함COMMIT; 해당 DO NOTHING 명령어 없으면 SQL ERROR 발생 1234INSERT INTO CUSTOMERS (NAME, EMAIL) VALUES ('Microsoft', 'hotline@microsoft.com')ON CONFLICT (NAME) &gt;&gt; UPSERT 문 실습 – UPDATE UPDATE: INSERT 액션이 충돌 시 (기존에 존재할 경우) UPDATE 함 [MISSION] Microsoft(기존에 존재하는 NAME)에 EMAIL 주소 추가 'Microsoft’라는 NAME이 이미 존재하므로 NAME의 UNIQUE 조건과 충돌 이런 경우에 데이터를 UPDATE함 123456789INSERT INTO CUSTOMERS (NAME, EMAIL) VALUES ('Microsoft', 'hotline@microsoft.com')ON CONFLICT (NAME) -- 충돌 검증 컬럼DO UPDATE SET EMAIL = EXCLUDED.EMAIL || '; ' || CUSTOMERS.EMAIL; -- EXCLUDED.EMAIL은 위에서 INSERT 시도한 EMAIL값을 가리킴 COMMIT; 3. EXPORT 작업 3-1. 개념 EXPORT는 테이블의 데이터를 다른 형태의 데이터로 추출하는 작업이다. 대표적으로 CSV 형식으로 가장 많이 추출한다. 3-2. EXPORT 작업 실습 1SELECT * FROM DATEGORY; &gt;&gt; 실습 – 엑셀(.CSV) 형식으로 출력 1234COPY CATEGORY (CATEGORY_ID, NAME, LAST_UPDATE) -- 추출할 테이블과 컬럼을 지정TO 'E:\\Study_SQL\\DB_CATEGORY.csv' -- 추출한 데이터를 저장할 파일을 지정DELIMITER ',' -- 구분자를 지정CSV HEADER; -- 파일 형식을 지정 저장 디랙토리(폴더)는 반드시 미리 존재해야 한다 (여기서는 ‘E:\\Study_SQL’) &gt;&gt; 실습 – 텍스트(.TXT) 파일로 출력 1234COPY CATEGORY(CATEGORY_ID, NAME, LAST_UPDATE)TO 'E:\\Study_SQL\\DB_CATEGORY.txt'DELIMITER '|'CSV HEADER; &gt;&gt; 실습 – 컬럼명 없이 출력 1234COPY CATEGORY(CATEGORY_ID, NAME, LAST_UPDATE)TO 'E:\\Study_SQL\\DB_CATEGORY_2.csv'DELIMITER ','CSV; 4. IMPORT 작업 4-1. 개념 IMPORT는 다른 형식의 데이터를 테이블에 넣는 작업을 말한다. 데이터 구축 시 자주 사용 된다. 4-2. IMPORT 작업 실습 &gt;&gt; 실습 준비 1234567CREATE TABLE CATEGORY_IMPORT( CATEGORY_ID SERIAL NOT NULL, \"NAME\" VARCHAR(25) NOT NULL, LAST_UPDATE TIMESTAMP NOT NULL DEFAULT NOW(), CONSTRAINT CATEGORY_IMPORT_PKEY PRIMARY KEY (CATEGORY_ID)); 1SELECT * FROM CATEGORY_IMPORT; &gt;&gt; 실습 – 엑셀파일을 적재 1234COPY CATEGORY_IMPORT(CATEGORY_ID, \"NAME\", LAST_UPDATE) -- 적재할 테이블 및 컬럼을 지정FROM 'E:\\Study_SQL\\DB_CATEGORY.csv' -- 적재할 파일을 지정DELIMITER ',' -- 적재할 파일의 구분자를 알려준다CSV HEADER; -- 파일 형식을 지정한다 1SELECT * FROM CATEGORY_IMPORT; &gt;&gt; 실습 – 텍스트 파일을 적재 123-- 실습 전 먼저 데이터를 삭제해야 함DELETE FROM CATEGORY_IMPORT;COMMIT; 1234COPY CATEGORY_IMPORT(CATEGORY_ID, \"NAME\", LAST_UPDATE)FROM 'E:\\Study_SQL\\DB_CATEGORY.txt'DELIMITER '|'CSV HEADER; 1SELECT * FROM CATEGORY_IMPORT; &gt;&gt; 실습 – 컬럼명이 없는 엑셀 파일 적재 123-- 실습 전 먼저 데이터를 삭제해야 함DELETE FROM CATEGORY_IMPORT;COMMIT; 123456COPY CATEGORY_IMPORT(CATEGORY_ID, \"NAME\", LAST_UPDATE)FROM 'E:\\Study_SQL\\DB_CATEGORY_2.csv'DELIMITER ','CSV;SELECT * FROM CATEGORY_IMPORT; DB_CATEGOROY_2.csv 파일은 컬럼명(header) 이 존재하지 않으므로 반드시 HEADER를 제거해야한다. HEADER를 제거하지 않을 경우 가장 첫번째 데이터를 헤더로 인식하여 한건이 누락된다 12DELETE FROM CATEGORY_IMPORT;COMMIT; 123456COPY CATEGORY_IMPORT(CATEGORY_ID, \"NAME\", LAST_UPDATE)FROM 'E:\\Study_SQL\\DB_CATEGORY_2.csv'DELIMITER ','CSV HEADER;SELECT * FROM CATEGORY_IMPORT; document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 8. Manipulation","slug":"【STUDY-SQL】/SQL-8-Manipulation","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-8-Manipulation/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Manipulation","slug":"Manipulation","permalink":"https://hyemin-kim.github.io/tags/Manipulation/"}]},{"title":"SQL >> 데이터 조작 (1)","slug":"S-SQL-Manipulation-1","date":"2020-12-07T04:45:49.000Z","updated":"2020-12-07T06:12:13.285Z","comments":true,"path":"2020/12/07/S-SQL-Manipulation-1/","link":"","permalink":"https://hyemin-kim.github.io/2020/12/07/S-SQL-Manipulation-1/","excerpt":"","text":"데이터 조작 (1) 1. INSERT 문 1-1. 개념 1-2. INSERT 문법 1-3. INSERT 문 실습 &gt;&gt; 실습 준비 &gt;&gt; INSERT 문 실습 2. UPDATE 문 2-1. 개념 2-2. UPDATE 문법 2-3. UPDATE 실습 &gt;&gt; 실습 준비 &gt;&gt; UPDATE 문 실습 3. UPDATE JOIN 문 3-1. 개념 3-2. UPDATE JOIN 문법 3-3. UPDATE JOIN 실습 &gt;&gt; 실습 준비 &gt;&gt; UPDATE JOIN 문 실습 1. INSERT 문 1-1. 개념 INSERT는 테이블이 만들어지면 데이블 안에 데이터를 추가하는 명령어이다. 1-2. INSERT 문법 &gt;&gt; 1) 테이블의 컬럼 순서대로 입력 1234567891011INSERT INTO TABLE_NAME -- INSERT할 테이블 지정VALUES( VALUE1, -- 각 컬럼 값을 입력한다 VALUE2, VALUE3, ...);COMMIT; &gt;&gt; 2) 테이블 컬럼 지정 (더 많이 쓰임. 컬럼 명을 명시해주기 때문에 유지보수에 용이함) 12345678910111213INSERT INTO TABLE_NAME ( COLUMN_1, COLUMN_2 )VALUES( VALUE1, VALUE2);COMMIT; 1-3. INSERT 문 실습 &gt;&gt; 실습 준비 12345678-- LINK라는 테이블을 생성함CREATE TABLE LINK ( ID SERIAL PRIMARY KEY, URL VARCHAR (255) NOT NULL, NAME VARCHAR (255) NOT NULL, DESCRIPTION VARCHAR (255), REL VARCHAR (50)); &gt;&gt; INSERT 문 실습 (1) 1개 ROW 입력 12345INSERT INTO LINK (URL, NAME)VALUES ('http://naver.com', 'Naver');COMMIT; 1SELECT * FROM LINK; 1234567-- 내용 안에 작은 따움표를 추가하고 싶으면 작은 따움표 두개(''##'')를 더 추가해주면 됨. INSERT INTO LINK (URL, NAME)VALUES ('''http://naver.com''', 'Naver'); COMMIT; 1SELECT * FROM FILM; (2) 동시에 N개 ROW 입력 12345678INSERT INTO LINK (URL, NAME)VALUES ('http://www.google.com', 'Google'), ('http://www.bing.com', 'Bing'), ('http://www.baidu.com', 'BaiDu');COMMIT; (3) 테이블 프레임에 테이블을 입력 &gt; LINK 테이블의 스키마(껍데기)만 가져와서 LINK_TMP 테이블을 생성한다 123CREATE TABLE LINK_TMP ASSELECT * FROM LINKWHERE 0 = 1; -- LINK_TMP 테이블의 구조는 LINK와 같고 데이터는 0건이 된다. 12345678INSERT INTO LINK_TMPSELECT * FROM LINK; COMMIT;SELECT * FROM LINK_TMP; 2. UPDATE 문 2-1. 개념 UPDATE 문은 테이블에 존재하는 데이터를 수정하는 작업이다. 업무를 처리하는데 필수적인 것이며 동시성에 유의해야 한다. UPDATE는 대상 행에 대해서 락(LOCK)을 잡는다. 락(LOCK)이란 다른 사용자는 해당 행에 대해서 작업을 하지 못한다는 것이다. (대기하게 됨) 즉 UPDATE를 한 후 재빨리 COMMIT을 하지 않는다면 RDBMS의 동시성이 낮아진다. 2-2. UPDATE 문법 1234567UPDATE TABLE_NAME -- UPDATE할 테이블 지정SET COLUMN_1 = VALUE1, -- 수정할 컬럼 및 값 입력 COLUMN_2 = VALUE2WHERE 조건; -- 대상 조건 2-3. UPDATE 실습 &gt;&gt; 실습 준비 1234ALTER TABLE LINK ADD COLUMN LAST_UPDATE DATE; -- LINK테이블에 LAST_UPDATE컬럼을 추가ALTER TABLE LINK ALTER COLUMN LAST_UPDATE SET DEFAULT CURRENT_DATE; -- LAST_UPDATE 컬럼의 기본값을 현재시간으로 함SELECT * FROM LINK; &gt;&gt; UPDATE 문 실습 (1) 지정 범위 수정 (WHERE절) [MISSION] LAST_UPDATE 컬럼의 값을 지정한 DEFAULT값으로 UPDATE하기 12345UPDATE LINK SET LAST_UPDATE = DEFAULT WHERE LAST_UPDATE IS NULL; COMMIT; 1SELECT * FROM LINK; (2) 전체 테이블 수정 [MISSION] REL컬럼의 값을 'NO DATA’로 수정하기 이 기능은 조심해서 사용 필요. 1234UPDATE LINK SET REL = 'NO DATA'; COMMIT; (3) 전체 테이블 수정 – 특정 컬럼을 이용 [MISSION] DESCRIPTION 컬럼을 NAME 컬럼의 값으로 채우기 1234UPDATE LINK SET DESCRIPTION = NAME; COMMIT; 3. UPDATE JOIN 문 3-1. 개념 UPDATA 시 다른 테이블의 내용을 참조하고 싶을 때 UPDATE JOIN 문을 사용한다. 복잡한 업무를 처리하는데 매우 유용한 방법이다. 3-2. UPDATE JOIN 문법 12345UPDATE TARGET_TABLE A -- UPDATE할 테이블 지정 SET A.COLUMN_1 = 표현식 -- 특정 컬럼 UPDATE FROM REF_TABLE B -- 참조 테이블 지정 WHERE A.COLUMN_1 = B.COLUMN_1; -- 조인 조건 3-3. UPDATE JOIN 실습 &gt;&gt; 실습 준비 123456CREATE TABLE PRODUCT_SEGMENT( ID SERIAL PRIMARY KEY, SEGMENT VARCHAR NOT NULL, DISCOUNT NUMERIC (4, 2)); 1234567INSERT INTO PRODUCT_SEGMENT(SEGMENT, DISCOUNT)VALUES ('Grand Luxury', 0.05), ('Luxury', 0.06), ('Mass', 0.1);COMMIT; 12345678910CREATE TABLE PRODUCT( ID SERIAL PRIMARY KEY, NAME VARCHAR NOT NULL, PRICE NUMERIC(10, 2), -- 정가 NET_PRICE NUMERIC(10, 2), -- 할인가 (실 판매가) SEGMENT_ID INT NOT NULL, FOREIGN KEY(SEGMENT_ID) REFERENCES PRODUCT_SEGMENT(ID)); 1234567891011121314151617181920212223INSERT INTO PRODUCT (NAME, PRICE, SEGMENT_ID)VALUES ('K5', 804.89, 1), ('K7', 228.55, 3), ('K9', 366.45, 2), ('SONATA', 145.33, 3), ('SPARK', 551.77, 2), ('AVANTE', 261.58, 3), ('LOZTE', 519.62, 2), ('SANTAFE', 843.31, 1), ('TUSON', 254.18, 3), ('TRAX', 427.78, 2), ('ORANDO', 936.29, 1), ('RAY', 910.34, 1), ('MORNING', 208.33, 3), ('VERNA', 985.45, 1), ('K8', 841.26, 1), ('TICO', 896.38, 1), ('MATIZ', 575.74, 2), ('SPORTAGE', 530.64, 2), ('ACCENT', 892.43, 1), ('TOSCA', 161.71, 3);COMMIT; 1SELECT * FROM PRODUCT_SEGMENT; 1SELECT * FROM PRODUCT; &gt;&gt; UPDATE JOIN 문 실습 [MISSION] PRODUCT_SEGMENT 테이블에 있는 할인율(DISCOUNT) 정보를 이용해서 PRODUCT 테이블의 NET_PRICE (할인가 / 실 판매가)를 계산하여 채우기 1234UPDATE PRODUCT A SET NET_PRICE = A.PRICE * (1 - B.DISCOUNT) FROM PRODUCT_SEGMENT B WHERE A.SEGMENT_ID = B.ID; 1SELECT * FROM PRODUCT; document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 8. Manipulation","slug":"【STUDY-SQL】/SQL-8-Manipulation","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-8-Manipulation/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Manipulation","slug":"Manipulation","permalink":"https://hyemin-kim.github.io/tags/Manipulation/"}]},{"title":"【실습】 SQL >> 집합 연산자와 서브쿼리","slug":"E-SQL-Aggregate-and-SubQuery","date":"2020-12-01T02:19:23.000Z","updated":"2020-12-01T03:48:24.007Z","comments":true,"path":"2020/12/01/E-SQL-Aggregate-and-SubQuery/","link":"","permalink":"https://hyemin-kim.github.io/2020/12/01/E-SQL-Aggregate-and-SubQuery/","excerpt":"","text":"【실습】 집합 연산자와 서브쿼리 [1] 아래 SQL문은 FILM 테이블을 2번 스캔하고 RENTAL_RATE가 평균 이상인 FILM의 ID, 제목과 RENTAL_RATE를 출력했다. FILM 테이블을 한번만 SCAN하여 동일한 결과 집합을 구하는 SQL을 작성하라. &gt;&gt; 두 번 스캔 12345678910111213SELECT FILM_ID, TITLE, RENTAL_RATEFROM FILMWHERE RENTAL_RATE &gt;( SELECT AVG(RENTAL_RATE) FROM FILM); &gt;&gt; 한 번만 스캔 1) 우선 분석함수 AVG를 사용해서 평균을 구한다. 1234567SELECT FILM_ID, TITLE, RENTAL_RATE, AVG(RENTAL_RATE) OVER() AS AVG_RENTAL_RATEFROM FILM 2) 1번에서 구한 집합을 인라인뷰로 감싸서 평균보다 큰 값을 구한다. 12345678910111213SELECT FILM_ID, TITLE, RENTAL_RATEFROM( SELECT FILM_ID, TITLE, RENTAL_RATE, AVG(RENTAL_RATE) OVER() AS AVG_RENTAL_RATE FROM FILM) AWHERE A.RENTAL_RATE &gt; A.AVG_RENTAL_RATE; 똑같은 결과가 나오는 것을 확인할 수 있다 [2] 아래 SQL문은 EXCEPT 연산을 사용하여 재고가 없는 영화를 구하고 있다. 해당 SQL문은 EXCEPT연산을 사용하지 말고 같은 결과를 도출하라. &gt;&gt; EXCEPT 연산 사용 12345678910111213SELECT FILM_ID, TITLEFROM FILMEXCEPTSELECT DISTINCT INVENTORY.FILM_ID, TITLEFROM INVENTORYINNER JOIN FILMON FILM.FILM_ID = INVENTORY.FILM_IDORDER BY TITLE; &gt;&gt; NOT EXISTS 연산 사용 123456789101112SELECT FILM_ID, TITLEFROM FILM FWHERE NOT EXISTS ( SELECT * FROM INVENTORY I WHERE F.FILM_ID = I.FILM_ID); 똑같은 결과가 나오는 것을 확인할 수 있다 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【EXERCISE】","slug":"【EXERCISE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/"},{"name":"SQL","slug":"【EXERCISE】/SQL","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"SubQuery","slug":"SubQuery","permalink":"https://hyemin-kim.github.io/tags/SubQuery/"}]},{"title":"SQL >> 서브쿼리 (SubQuery)","slug":"S-SQL-SubQuery","date":"2020-11-27T02:16:42.000Z","updated":"2020-11-27T04:07:01.212Z","comments":true,"path":"2020/11/27/S-SQL-SubQuery/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/27/S-SQL-SubQuery/","excerpt":"","text":"서브쿼리 1. 서브쿼리란? 1-1. 개념 1-2. 서브쿼리 이해 [MISSION] &gt;&gt; Method 1: 메인쿼리 2개 사용 &gt;&gt; Method 2: 서브쿼리 사용 (1) 중첩 서브쿼리의 활용 (2) 인라인 뷰의 활용 (3) 스칼라 서브쿼리의 활용 2. ANY 연산자 2-1. 개념 2-2. ANY 연산자 실습 &gt;&gt; 준비 &gt;&gt; MISSION 1 &gt;&gt; MISSION 2 3. ALL 연산자 3-1. 개념 3-2. ALL 연산자 실습 &gt;&gt; MISSION 1 &gt;&gt; MISSION 2 4. EXISTS 연산자 4-1. 개념 4-2. EXISTS 연산자 실습 &gt;&gt; MISSION 5. NOT EXISTS 연산자 5-1. 개념 5-2. NOT EXISTS 연산자 실습 &gt;&gt; MISSION 1. 서브쿼리란? 1-1. 개념 서브쿼리는 SQL문 내에서 메인 쿼리가 아닌 하위에 존재하는 쿼리를 말한다. 서브쿼리를 활용함으로써 다양한 결과를 도출할 수 있다. 1-2. 서브쿼리 이해 [MISSION] FILM 테이블에서 RENTAL_RATE가 평균 보다 큰 집합 구하기 &gt;&gt; Method 1: 메인쿼리 2개 사용 1234SELECT AVG(RENTAL_RATE)FROM FILM; 12345678SELECT FILM_ID, TITLE, RENTAL_RATEFROM FILMWHERE RENTAL_RATE &gt; 2.98; Q: 위 2개의 SQL문을 결합하여 하나의 SQL문으로 결과를 도출할 수 없을까? A: 서브쿼리를 사용하면 된다! (중첩 서브쿼리, 인라인 뷰, 스칼라 서브쿼리가 존재한다.) &gt;&gt; Method 2: 서브쿼리 사용 (1) 중첩 서브쿼리의 활용 중첩 서브쿼리 (Nested Subquery): 메인쿼리의 WHERE절에 나타나는 서브쿼리 1234567891011121314SELECT FILM_ID, TITLE, RENTAL_RATEFROM FILMWHERE RENTAL_RATE &gt; (SELECT AVG(RENTAL_RATE)FROM FILM); (2) 인라인 뷰의 활용 인라인 뷰 (Inline View): 메인쿼리의 FROM 절에 나타나는 서브쿼리 (서브쿼리 SELECT 절의 결과를 메인쿼리의 FROM 절에서 하나의 테이블처럼 사용) 1234567891011121314SELECT A.FILM_ID, A.TITLE, A.RENTAL_RATEFROM FILM A, ( SELECT AVG(RENTAL_RATE) AS AVG_RENTAL_RATE FROM FILM ) BWHERE A.RENTAL_RATE &gt; B.AVG_RENTAL_RATE; (3) 스칼라 서브쿼리의 활용 스칼라 서브쿼리 (Scala Subquery): SELECT의 리스트 안에 존재하는 서브쿼리 12345678910111213141516171819SELECT A.FILM_ID, A.TITLE, A.RENTAL_RATEFROM( SELECT A.FILM_ID, A.TITLE, A.RENTAL_RATE, ( SELECT AVG(L.RENTAL_RATE) FROM FILM L ) AS AVG_RENTAL_RATE FROM FILM A) AWHERE A.RENTAL_RATE &gt; A.AVG_RENTAL_RATE; 2. ANY 연산자 2-1. 개념 ANY 연산자는 주로 메인쿼리 WHERE절의 비교 조건식에서 서브쿼리와 함께 사용된다. 서브쿼리에 의해 반환된 값 집합과 비교할 때 한번이라도 조건에 만족한다면 TURE를 반환한다. (즉, 서브쿼리 결과 집합중의 어떤 한 값 보다만 어떻다라면 TRUE를 반환) 2-2. ANY 연산자 실습 &gt;&gt; 준비 먼저 영화 분류별 상영시간이 가장 긴 영화의 카테고리 ID 및 상영시간을 출력 123456789101112SELECT B.CATEGORY_ID, MAX(LENGTH)FROM FILM A, FILM_CATEGORY BWHERE A.FILM_ID = B.FILM_IDGROUP BY B.CATEGORY_IDORDER BY B.CATEGORY_ID; &gt;&gt; MISSION 1 영화의 상영시간이 위 집합 중 어느 하나의(ANY) 값 보다만 크거나 같으면 추출 [즉, 위에서 추출된 값 들의 최소값 보다만 크거나 같으면 추출] \" &gt;= ANY \" 활용 123456789101112131415SELECT TITLE, LENGTHFROM FILMWHERE LENGTH &gt;= ANY( SELECT MAX(LENGTH) -- SELECT한 변수는 하나여야 함 FROM FILM A, FILM_CATEGORY B WHERE A.FILM_ID = B.FILM_ID GROUP BY B.CATEGORY_ID);-- 결과적으로 상영시간이 MIN(max) = 178 min 보다 긴거나 같은 영화를 모두 추출 서브쿼리가 반환되는 결과가 하나의 값이 아닌 여러 값의 집합이기 때문에 ANY 연산자를 안 쓰면 ERROR가 난다. (명확한 기준이 없기 때문) 12345678910111213SELECT TITLE, LENGTHFROM FILMWHERE LENGTH &gt;=( SELECT MAX(LENGTH) FROM FILM A, FILM_CATEGORY B WHERE A.FILM_ID = B.FILM_ID GROUP BY B.CATEGORY_ID); &gt;&gt; MISSION 2 영화의 상영시간이 위에서 추출된 값들과 동일한 영화만 추출 [즉, 위 집합 중 어느 하나의(ANY) 값과 같으면 추출] \" = ANY \" 활용 12345678910111213SELECT TITLE, LENGTHFROM FILMWHERE LENGTH = ANY( SELECT MAX(LENGTH) FROM FILM A, FILM_CATEGORY B WHERE A.FILM_ID = B.FILM_ID GROUP BY B.CATEGORY_ID); \"=ANY\"는 \"IN\"과 동일 의미: 추출된 결과 집합 리스트 안의 값들과 매칭되는 값들을 찾는다 12345678910111213SELECT TITLE, LENGTHFROM FILMWHERE LENGTH IN( SELECT MAX(LENGTH) FROM FILM A, FILM_CATEGORY B WHERE A.FILM_ID = B.FILM_ID GROUP BY B.CATEGORY_ID); 3. ALL 연산자 3-1. 개념 ALL 연산자는 주로 메인쿼리 WHERE절의 비교 조건식에서 서브쿼리와 함께 사용된다. 서브쿼리에 의해 반환된 값 집합과 비교할 때 모두 조건에 만족해야만 TURE를 반환한다. (즉, 서브쿼리 결과 집합중의 모든 값 보다 어떻다해야 TRUE를 반환) 3-2. ALL 연산자 실습 &gt;&gt; MISSION 1 [준비] : 먼저 영화 분류별 상영시간이 가장 긴 영화의 카테고리 ID 및 상영시간을 출력 123456789101112SELECT B.CATEGORY_ID, MAX(LENGTH)FROM FILM A, FILM_CATEGORY BWHERE A.FILM_ID = B.FILM_IDGROUP BY B.CATEGORY_IDORDER BY B.CATEGORY_ID; [MISSION] : 영화의 상영시간이 위 집합의 모든(ALL) 값 보다 크거나 같아야 추출 [즉, 위에서 추출된 값 들의 최대값 보다 크거나 같아야 추출] 12345678910111213141516SELECT TITLE, LENGTHFROM FILMWHERE LENGTH &gt;= ALL( SELECT MAX(LENGTH) FROM FILM A, FILM_CATEGORY B WHERE A.FILM_ID = B.FILM_ID GROUP BY B.CATEGORY_ID);-- 결과적으로 상영시간이 MAX(max) = 185 min 보다 긴거나 같은 영화를 추출 &gt;&gt; MISSION 2 [준비] : 먼저 평가기준(RATING)별 영화의 평균 상영시간을 출력 123456789SELECT RATING, ROUND(AVG(LENGTH), 2) AS AVG_LENGTH -- 소수점 2자리FROM FILMGROUP BY RATINGORDER BY AVG_LENGTH; [MISSION] : 위에서 출력된 평균 상영시간보다 긴 영화의 정보를 출력 12345678910111213141516SELECT FILM_ID, TITLE, LENGTHFROM FILMWHERE LENGTH &gt;= ALL( SELECT ROUND(AVG(LENHTH), 2) -- SELECT한 변수는 하나여야 함 FROM FILM GROUP BY RATING)ORDER BY LENGTH;-- 결과적으로 상영시간이 MAX(avg_length) = 120.44 min 보다 긴거나 같은 영화를 추출 4. EXISTS 연산자 4-1. 개념 EXISTS 연산자는 주로 메인쿼리 WHERE절에서 서브쿼리와 함께 사용된다. 동작원리는 다음과 같습니다: 먼저 메인쿼리의 TABLE에 접근하여 하나의 레코드를 가져온다. 이 레코드에 대해서 EXISTS 이하의 서브쿼리를 실행하고 서브쿼리에 의해 반환된 값 집합이 존재하는지를 판단한다. 서브쿼리에 의해 반환된 값 집합이 존재한다면 TRUE를 반환하고 메인쿼리의 SELECT문을 그대로 실행한다. 반환된 값 집합이 존재하지 않다면 FALSE를 반환하고 메인쿼리의 SELECT문을 실행하지 않고 바로 다음 레코드로 넘어간다. &gt;&gt; 장점: 서브쿼리에 의해 반환된 값 집합의 존재여부만을 판단하므로 연산 시 부하가 줄어든다 (성능상 유리함) EXISTS 연산자와 IN 연산자의 차이점은 Tigercow.Dor님의 IN / EXISTS / NOT IN / NOT EXISTS 비교 에서 자세히 설명되어 있음. 참고 바람. 4-2. EXISTS 연산자 실습 &gt;&gt; MISSION 지불내역(AMOUNT)이 11달러 초과한 고객의 이름을 출력하라 1234567891011121314SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMER CWHERE EXISTS( SELECT 1 -- 반환 값이 임의로 지정해도 좋다. 존재 여부만 판단하기 떄문에 무엇을 반환하든 상관없음. FROM PAYMENT P WHERE P.CUSTOMER_ID = C.CUSTOMER_ID AND AMOUNT &gt; 11)ORDER BY FIRST_NAME, LAST_NAME; 동작 순서: 먼저 메인쿼리의 CUSTOMER 테이블에서 N번째 레코드를 가져온다 그 다음 EXISTS 이하의 서브쿼리를 실행: PAYMENT 테이블에서 CUSTOMER_ID는 CUSTOMER 테이블의 CUSTOMER_ID과 동일하면서 (가져온 N번째 레코드의 CUSTOMER_ID랑만 비교) 지불내역(AMOUNT)이 11달러 초과한 값의 존재 여부를 판단(하여 TURE이면 1를 반환) STEP 2에서 TURE로 판단되면 메인쿼리의 SELECT문을 그대로 실행 (즉 N번째 고객의 이름을 추출); FALSE로 판단되면 메인쿼리의 SELECT 문을 실현하지 않고 그 다음 레코드 (N+1 번째 레코드)를 가져와 STEP 2 를 진행한다. 위 과정을 반복하여 마지막 레코드까지 완료되면 해당 SQL문의 동작이 종료된다. 5. NOT EXISTS 연산자 5-1. 개념 NOT EXISTS 연산자는 주로 메인쿼리 WHERE절에서 서브쿼리와 함께 사용된다. 위에서 EXISTS에 대해서 이해했다면 크게 어려운 점이 없다. 동작원리는 다음과 같습니다: 먼저 메인쿼리의 TABLE에 접근하여 하나의 레코드를 가져온다. 이 레코드에 대해서 EXISTS 이하의 서브쿼리를 실행하고 서브쿼리에 의해 반환된 값 집합이 존재하는지를 판단한다. STEP 3는 EXISTS 연산자와 정 반대이다: 서브쿼리에 의해 반환된 값 집합이 존재하지 않다면 TRUE 를 반환하고 메인쿼리의 SELECT문을 그대로 실행한다. 반환된 값 집합이 존재한다면 FALSE를 반환하고 메인쿼리의 SELECT문을 실행하지 않고 바로 다음 레코드로 넘어간다. &gt;&gt; 장점: 서브쿼리에 의해 반환된 값 집합의 존재여부만을 판단하므로 연산 시 부하가 줄어든다 (성능상 유리함) 5-2. NOT EXISTS 연산자 실습 &gt;&gt; MISSION 지불내역(AMOUNT)이 11달러 초과한 적이 없는 고객의 이름을 출력하라 123456789101112131415SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMER CWHERE NOT EXISTS( SELECT 1 FROM PAYMENT P WHERE P.CUSTOMER_ID = C.CUSTOMER_ID AND P.AMOUNT &gt; 11)ORDER BY FIRST_NAME, LAST_NAME; ​ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 7. SubQuery","slug":"【STUDY-SQL】/SQL-7-SubQuery","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-7-SubQuery/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"}]},{"title":"SQL >> 집합 연산자","slug":"S-SQL-Operation","date":"2020-11-20T05:13:20.000Z","updated":"2020-11-20T06:52:35.801Z","comments":true,"path":"2020/11/20/S-SQL-Operation/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/20/S-SQL-Operation/","excerpt":"","text":"집합 연산자 1. UNION 연산 1-1. 개념 1-2. UNION 연산 문법 1-3. UNION 연산 실습 (1) 실습 준비 (2) UNION 연산 실습 2. UNION ALL 연산 2-1. 개념 2-2. UNION ALL 문법 2-3. UNION ALL 실습 3. INTERSECT 연산 3-1. 개념 3-2. INTERSECT 연산 문법 3-3. INTERSECT 연산 실습 (1) 실습 준비 (2) INTERSECT 연산 실습 4. EXCEPT 연산 4-1. 개념 4-2. EXCEPT 연산 문법 4-3. EXCEPT 연산 실습 1. UNION 연산 1-1. 개념 두 개 이상의 SELECT 문들의 결과 집합을 단일 결과 집합으로 결합하며 결합 시 중복된 데이터는 제거 된다. 1-2. UNION 연산 문법 1234567891011SELECT COLUMN_1_1, COLUMN_1_2 FROM TABLE_NAME_1UNIONSELECT COLUMN_2_1, COLUMN_2_2 FROM TABLE_NAME_2; 두 개의 SELECT 문 간 컬럼의 개수는 동일해야 하고 해당 순서의 열에는 서로 호환되는 데이터 유형이어야 한다. 두 개의 SELECT 문에서 중복되는 데이터 값이 있다면 중복을 제거 된다. ORDER BY 로 정렬하고자 할 경우 맨 마지막 SELECT문에 ORDER BY 절을 사용한다. 1-3. UNION 연산 실습 (1) 실습 준비 12345678910111213CREATE TABLE SALES2007_1( NAME VARCHAR(50), AMOUNT NUMERIC(15, 2));INSERT INTO SALES2007_1VALUES ('Mike', 150000.25), ('Jon', 132000.75), ('Mary', 100000); COMMIT; 12345678910111213CREATE TABLE SALES2007_2( NAME VARCHAR(50), AMOUNT NUMERIC(15, 2));INSERT INTO SALES2007_2VALUES ('Mike', 120000.25), ('Jon', 142000.75), ('Mary', 100000); COMMIT; 1SELECT * FROM SALES2007_1; 1SELECT * FROM SALES2007_2; (2) UNION 연산 실습 &gt;&gt; 일반 UNION 12345678910-- 전체 UNIONSELECT *FROM SALES2007_1UNIONSELECT *FROM SALES2007_2; (‘Mary’, ‘100000’) 중복 제거됨 12345678910-- NAME만 UNIONSELECT NAMEFROM SALES2007_1UNIONSELECT NAMEFROM SALES2007_2; ‘Mike’, ‘Jon’, ‘Mary’ 중복 제거됨 12345678910-- AMOUNT만 UNIONSELECT AMOUNTFROM SALES2007_1UNIONSELECT AMOUNTFROM SALES2007_2; ‘100000’ 중복 제거됨 &gt;&gt; UNION + ORDER BY 1234567891011SELECT *FROM SALES2007_1UNIONSELECT *FROM SALES2007_2ORDER BY -- ORDER BY는 맨 마지막 SELECT 문에 기재해야 한다. AMOUNT DESC; 2. UNION ALL 연산 2-1. 개념 두 개 이상의 SELECT 문들의 결과 집합을 단일 결과 집합으로 결합하며 결합 시 중복된 데이터도 모두 출력한다. 2-2. UNION ALL 문법 1234567891011SELECT COLUMN_1_1, COLUMN_1_2 FROM TABLE_NAME_1UNION ALLSELECT COLUMN_2_1, COLUMN_2_2 FROM TABLE_NAME_2; 두 개의 SELECT 문 간 컬럼의 개수는 동일해야 하고 해당 순서의 열에는 서로 호환되는 데이터 유형이어야 한다. 두 개의 SELECT 문에서 중복되는 데이터 값이 있어도 모두 출력한다. ORDER BY 로 정렬하고자 할 경우 맨 마지막 SELECT문에 ORDER BY 절을 사용한다. 2-3. UNION ALL 실습 &gt;&gt; 일반 UNION ALL 12345678910-- 전체 UNION ALLSELECT *FROM SALES2007_1UNION ALLSELECT *FROM SALES2007_2; (‘Mary’, ‘100000’) 중복 데이터 출력함 12345678910-- NAME만 UNION ALLSELECT NAMEFROM SALES2007_1UNION ALLSELECT NAMEFROM SALES2007_2; ‘Mike’, ‘Jon’, ‘Mary’ 중복 데이터 출력함 12345678910-- AMOUNT만 UNION ALLSELECT AMOUNTFROM SALES2007_1UNION ALLSELECT AMOUNTFROM SALES2007_2; ‘100000’ 중복 데이터 출력함 &gt;&gt; UNION ALL + ORDER BY 1234567891011SELECT *FROM SALES2007_1UNION ALLSELECT *FROM SALES2007_2ORDER BY -- ORDER BY는 맨 마지막 SELECT 문에 기재해야 한다. AMOUNT DESC; 3. INTERSECT 연산 3-1. 개념 INTERSECT 연산자는 두 개 이상의 SELECT 문들의 결과 집합의 교집합을 출력하는 연산자다. 3-2. INTERSECT 연산 문법 1234567891011SELECT COLUMN_1_1, COLUMN_1_2 FROM TABLE_NAME_1INTERSECTSELECT COLUMN_2_1, COLUMN_2_2 FROM TABLE_NAME_2; 두 개의 SELECT 문 간 컬럼의 개수는 동일해야 하고 해당 순서의 열에는 서로 호환되는 데이터 유형이어야 한다. ORDER BY 로 정렬하고자 할 경우 맨 마지막 SELECT문에 ORDER BY 절을 사용한다. 3-3. INTERSECT 연산 실습 (1) 실습 준비 123456789101112131415161718192021CREATE TABLE EMPLOYEES1( EMPLOYEE_ID SERIAL PRIMARY KEY, EMPLOYEE_NAME VARCHAR(255) NOT NULL);CREATE TABLE KEYS( EMPLOYEE_ID INT PRIMARY KEY, EFFECTIVE_DATE DATE NOT NULL, FOREIGN KEY (EMPLOYEE_ID) REFERENCES EMPLOYEES1 (EMPLOYEE_ID));CREATE TABLE HIPOS( EMPLOYEE_ID INT PRIMARY KEY, EFFECTIVE_DATE DATE NOT NULL, FOREIGN KEY (EMPLOYEE_ID) REFERENCES EMPLOYEES1 (EMPLOYEE_ID)); 1234567891011121314151617181920212223242526INSERT INTO EMPLOYEES1 (EMPLOYEE_NAME)VALUES('Joyce Edwards'),('Diane Collins'),('Alice Stewart'),('Julie Sanchez'),('Heather Morris'),('Teresa Rogers'),('Doris Reed'),('Gloria Cook'),('Evelyn Morgan'),('Jean Bell');INSERT INTO KEYSVALUES(1, '2000-02-01'),(2, '2001-06-01'),(5, '2002-01-01'),(7, '2005-06-01');INSERT INTO HIPOSVALUES(9, '2000-01-01'),(2, '2002-06-01'),(5, '2006-06-01'),(10, '2005-06-01'); 1SELECT * FROM EMPLOYEES1; 1SELECT * FROM KEYS; 1SELECT * FROM HIPOS; (2) INTERSECT 연산 실습 &gt;&gt; 일반 INTERSECT 12345678910-- \"KEYS\" INTERSECT \"HIPOS\"SELECT EMPLOYEE_ID FROM KEYSINTERSECTSELECT EMPLOYEE_ID FROM HIPOS; INNER 조인 연산과 결과가 동일함** (실무에서 INTERSECT 연산 보다 INNER 조인 더 많이 쓰인다) 1234567891011121314151617-- INNER JOIN (1)SELECT A.EMPLOYEE_IDFROM KEYS AINNER JOIN HIPOS BON A.EMPLOYEE_ID = B.EMPLOYEE_ID;-- INNER JOIN (2)SELECT A.EMPLOYEE_IDFROM KEYS A, HIPOS BWHERE A.EMPLOYEE_ID = B.EMPLOYEE_ID; &gt;&gt; INTERSECT + ORDER BY 12345678910SELECT EMPLOYEE_ID FROM KEYSINTERSECTSELECT EMPLOYEE_ID FROM HIPOSORDER BY EMPLOYEE_ID DESC; 4. EXCEPT 연산 4-1. 개념 EXCEPT 연산자는 맨위에 SELECT 문의 결과 집합에서 그 아래에 있는 SELECT 문의 결과 집합을 제외한 결과를 리턴한다. (실무에서 많이 쓰임) 4-2. EXCEPT 연산 문법 12345678910SELECT COLUMN_1_1, COLUMN_1_2 FROM TABLE_NAME_1SELECT COLUMN_2_1, COLUMN_2_2 FROM TABLE_NAME_2; 두 개의 SELECT 문 간 컬럼의 개수는 동일해야 하고 해당 순서의 열에는 서로 호환되는 데이터 유형이어야 한다. ORDER BY 로 정렬하고자 할 경우 맨 마지막 SELECT문에 ORDER BY 절을 사용한다. 4-3. EXCEPT 연산 실습 &gt;&gt; 실습 데이터 dvdrental 데이터셋의 “film” 테이블(영화dvd 정보) 과 “inventory” 테이블(dvd 제고 정보) 을 활용한다. 한 편의 영화가 여러 개의 제고가 있을 수 있다 &gt;&gt; MISSION: 제고가 존재하지 않는 영화의 ID와 제목을 추출한다 (1) 먼저 제고가 존재하는 영화의 ID와 제목을 추출 123456789SELECT DISTINCT A.FILM_ID, B.TITLEFROM INVENTORY AINNER JOIN FILM BON A.FILM_ID = B.FILM_IDORDER BY B.TITLE; (2) 이제 전체 영화에서 제고 있는 영화를 제거하면 제거 없는 영화의 정보를 추출할 수 있다 123456789101112131415SELECT FILM_ID, TITLEFROM FILMEXCEPTSELECT DISTINCT A.FILM_ID, B.TITLEFROM INVENTORY AINNER JOIN FILM BON A.FILM_ID = B.FILM_IDORDER BY TITLE; document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 6. Aggregate Operations","slug":"【STUDY-SQL】/SQL-6-Aggregate-Operations","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-6-Aggregate-Operations/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"}]},{"title":"【실습】 SQL >> 조인과 집계 데이터","slug":"E-SQL-join-and-aggregate","date":"2020-11-19T06:44:46.000Z","updated":"2020-11-19T06:54:13.873Z","comments":true,"path":"2020/11/19/E-SQL-join-and-aggregate/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/19/E-SQL-join-and-aggregate/","excerpt":"","text":"【실습】 조인과 집계 데이터 [1] RENTAL 테이블을 이용하여 연, 연월, 연월일, 전체 각각의 기준으로 RENTAL_ID 기준 렌탈이 일어난 횟수를 출력하라. (전체 데이터 기준으로 모든 행을 출력) &gt;&gt; 문제 풀이 1SELECT * FROM RENTAL; 1234567891011SELECT TO_CHAR(RENTAL_DATE, 'YYYY'), TO_CHAR(RENTAL_DATE, 'MM'), TO_CHAR(RENTAL_DATE, 'DD'), COUNT (RENTAL_ID)FROM RENTALGROUP BY ROLLUP (TO_CHAR(RENTAL_DATE, 'YYYY'), TO_CHAR(RENTAL_DATE, 'MM'), TO_CHAR(RENTAL_DATE, 'DD')); [2] RENTAL과 CUSTOMER 테이블을 이용하여 현재까지 가장 많이 RENTAL을 한 고객의 고객ID, 렌탈순위, 누적렌탈횟수, 이름을 출력하라. &gt;&gt; 문제 풀이 (1) 가장 먼저 RENTAL 순위를 구해야 한다. 고객 별로 렌탈 횟수 구함 ROW_NUMBER() 를 이용해 순위 번호 생성 (렌탈 횟수를 내림차순으로 정렬한 후 생성) 12345678910SELECT CUSTOMER_ID, ROW_NUMBER() OVER(ORDER BY COUNT(RENTAL_ID) DESC) AS RENTAL_RANK, COUNT(RENTAL_ID) AS RENTAL_COUNTFROM RANTALGROUP BY CUSTOMER_ID; -- CUSTOMER_ID 기준으로 GROUP BY 했기 때문에 ROW_NUMBER()에서 PARTITION BY가 생략되었다. (2) 이 상태에서 첫번째 순위인 데이처를 추출 (가장 많이 RENTAL 한 고객의 데이터) ORDER BY + LIMIT 이용 1234567891011SELECT CUSTOMER_ID, ROW_NUMBER() OVER(ORDER BY COUNT(RENTAL_ID) DESC) AS RENTAL_RANK, COUNT(RENTAL_ID) AS RENTAL_COUNTFROM RENTALGROUP BY CUSTOMER_IDORDER BY RENTAL_COUNT DESCLIMIT 1; (3) 마지막으로 CUSTOMER 테이블과 조인하여 해당 고객의 이름을 출력한다 직접 조인 CUSTOMER_ID 기준으로 GROUP BY 되어 있으므로 FIRST_NAME, LAST_NAME에 MAX함수를 사용해서 출력한다. 123456789101112131415SELECT A.CUSTOMER_ID, ROW_NUMBER() OVER(ORDER BY COUNT(A.RENTAL_ID) DESC) AS RENTAL_RANK, COUNT(A.RENTAL_ID) AS RENTAL_COUNT, MAX(B.FIRST_NAME) AS FIRST_NAME, MAX(B.LAST_NAME) AS LAST_NAMEFROM RENTAL A, CUSTOMER BWHERE A.CUSTOMER_ID = B.CUSTOMER_IDGROUP BY A.CUSTOMER_IDORDER BY RENTAL_COUNT DESCLIMIT 1; 서브커리 활용 1234567891011121314151617181920SELECT B.CUSTOMER_ID, B.RENTAL_RANK, B.RENTAL_COUNT, C.FIRST_NAME, C.LAST_NAMEFROM(SELECT A.CUSTOMER_ID, ROW_NUMBER() OVER(ORDER BY COUNT(A.RENTAL_ID) DESC) AS RENTAL_RANK, COUNT(A.RENTAL_ID) AS RENTAL_COUNTFROM RENTAL AGROUP BY CUSTOMER_IDORDER BY RENTAL_COUNT DESCLIMIT 1) B, CUSTOMER CWHERE B.CUSTOMER_ID = C.CUSTOMER_ID; document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【EXERCISE】","slug":"【EXERCISE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/"},{"name":"SQL","slug":"【EXERCISE】/SQL","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Join","slug":"Join","permalink":"https://hyemin-kim.github.io/tags/Join/"},{"name":"Aggregate","slug":"Aggregate","permalink":"https://hyemin-kim.github.io/tags/Aggregate/"}]},{"title":"SQL >> 분석 함수 (2)","slug":"S-SQL-Analytic-Function-2","date":"2020-11-17T23:57:17.000Z","updated":"2020-11-20T06:44:52.694Z","comments":true,"path":"2020/11/18/S-SQL-Analytic-Function-2/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/18/S-SQL-Analytic-Function-2/","excerpt":"","text":"분석 함수 (2) 1. FIRST_VALUE, LAST_VALUE 함수 1-1. 개념 1-2. FIRST_NAME 함수 실습 1-3. LAST_VALUE 함수 실습 2. LAG, LEAD 함수 2-1. 개념 2-2. LAG 함수 실습 – 이전 행의 값을 찾는다 2-3. LEAD 함수 실습 – 다음 행의 값을 찾는다 1. FIRST_VALUE, LAST_VALUE 함수 1-1. 개념 FIRST_VALUE, LAST_VALUE 함수는 특정 집합 내에서 결과 건수의 변화 없이 해당 집합안에서 특정 컬럼의 첫번째 값 혹은 마지막 값을 구하는 함수이다. 1-2. FIRST_NAME 함수 실습 1SELECT * FROM PRODUCT_GROUP; 1SELECT * FROM PRODUCT; &gt;&gt; MISSION: GROUP_NAME 기준 PRICE가 가장 작은 값을 출력한다. 1234567891011SELECT A.PRODUCT_NAME, B.GROUP_NAME, A.PRICE, FIRST_VALUE (A.PRICE) OVER (PARTITION BY B.GROUP_NAME ORDER BY A.PRICE) AS LOWEST_PRICE_PER_GROUPFROM PRODUCT AINNER JOIN PRODUCT_GROUP BON A.GROUP_ID = B.GROUP_ID; 1-3. LAST_VALUE 함수 실습 LAST_VALUE 함수 사용 시 추가적으로 LAST_VALUE를 선택하는 범위를 지정해줘야 함. &gt;&gt; MISSION: GROUP_NAME 기준 PRICE가 가장 큰 값을 출력한다. 1234567891011121314SELECT A.PRODUCT_NAME, B.GROUP_NAME, A.PRICE, LAST_VALUE (A.PRICE) OVER (PARTITION BY B.GROUP_NAME ORDER BY A.PRICE RANGE BETWEEN UNBOUNDED PRECEDING -- PARTITION의 첫번째 ROW부터 AND UNBOUNDED FOLLOWING) -- PARTITION의 마지막 ROW까지 AS HIGHEST_PRICE_PER_GROUPFROM PRODUCT AINNER JOIN PRODUCT_GROUP BON A.GROUP_ID = B.GROUP_ID; LAST_VALUE 함수에는 \"RANGE BETWEEN ENBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\"를 추가함 DEFAULT가 \"RANGE BETWEEN ENBOUNDED PRECEDING AND CURRENT ROW\"이기 때문이다 12345678910111213-- DEFAULT 경우:SELECT A.PRODUCT_NAME, B.GROUP_NAME, A.PRICE, LAST_VALUE (A.PRICE) OVER (PARTITION BY B.GROUP_NAME ORDER BY A.PRICE) AS HIGHEST_PRICE_PER_GROUPFROM PRODUCT AINNER JOIN PRODUCT_GROUP BON A.GROUP_ID = B.GROUP_ID; 범위 지정은 DEFAULT로 CURRENT ROW 까지여서 우리가 기대하는 바와 달리 PRICE 값 그대로 출력함. 2. LAG, LEAD 함수 2-1. 개념 LAG 와 LEAD 함수는 특정 집합 내에서 결과 건수의 변화 없이 해당 집합안에서 특정 컬럼의 이전 행의 값 혹은 다음 행의 값을 구하는 함수이다. 2-2. LAG 함수 실습 – 이전 행의 값을 찾는다 1SELECT * FROM PRODUCT_GROUP; 1SELECT * FROM PRODUCT; 1234567891011SELECT A.PRODUCT_NAME, B.GROUP_NAME, A.PRICE, LAG(A.PRICE, 1) OVER (PARTITION BY B.GROUP_NAME ORDER BY A.PRICE) AS PREV_PRICE, PRICE - LAG(A.PRICE, 1) OVER (PARTITION BY B.GROUP_NAME ORDER BY A.PRICE) AS CUR_PREV_DIFFFROM PRODUCT AINNER JOIN PRODUCT_GROUP BON A.GROUP_ID = B.GROUP_ID; 2-3. LEAD 함수 실습 – 다음 행의 값을 찾는다 1234567891011SELECT A.PRODUCT_NAME, B.GROUP_NAME, A.PRICE, LEAD(A.PRICE, 1) OVER (PARTITION BY B.GROUP_NAME ORDER BY A.PRICE) AS NEXT_PRICE, A.PRICE - LEAD(A.PRICE, 1) OVER (PARTITION BY B.GROUP_NAME ORDER BY A.PRICE) AS CUR_NEXT_DIFFFROM PRODUCT AINNER JOIN PRODUCT_GROUP BON A.GROUP_ID = B.GROUP_ID; document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 5. Analytic Function","slug":"【STUDY-SQL】/SQL-5-Analytic-Function","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-5-Analytic-Function/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Analytic Function","slug":"Analytic-Function","permalink":"https://hyemin-kim.github.io/tags/Analytic-Function/"}]},{"title":"SQL >> 분석 함수 (1) -- 평균 함수, 순위 함수","slug":"S-SQL-Analytic-Function-1","date":"2020-11-17T23:47:11.000Z","updated":"2020-11-20T06:44:41.694Z","comments":true,"path":"2020/11/18/S-SQL-Analytic-Function-1/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/18/S-SQL-Analytic-Function-1/","excerpt":"","text":"분석 함수 (1) – 평균 함수, 순위 함수 0. 분석 함수란? 0-1. 개념 0-2. 분석 함수 실습 준비 0-3. 분석 함수 문법 0-4. 분석 함수 결과 예시 1. AVG 함수 1-1. 개념 1-2. AVG 함수 실습 (1) 전체 평균 가격(PRICE) 구하기 (2) 그룹별 평균 가격(PRICE) 구하기 (3) 그룹별 누적 평균 가격(PRICE) 구하기 2. ROW_NUMBER, RANK, DENSE_RANK 함수 2-1. 개념 2-2. 순위 함수 실습 2-2-1. ROW_NUMBER 함수 실습 2-2-2. RANK 함수 실습 2-2-3. DENSE_RANK 함수 실습 0. 분석 함수란? 0-1. 개념 분석 함수는 특정 집합 내에서 결과 건수의 변화없이 해당 집합안에서 합계 및 카운트 등을 계산할 수 있는 함수이다. 0-2. 분석 함수 실습 준비 12345678910111213CREATE TABLE PRODUCT_GROUP ( GROUP_ID SERIAL PRIMARY KEY, GROUP_NAME VARCHAR (255) NOT NULL);CREATE TABLE PRODUCT ( PRODUCT_ID SERIAL PRIMARY KEY, PRODUCT_NAME VARCHAR (255) NOT NULL, PRICE DECIMAL (11, 2), -- DECIMAL (전체 자릿수, 소수점 자릿수) GROUP_ID INT NOT NULL, FOREIGN KEY (GROUP_ID) REFERENCES PRODUCT_GROUP (GROUP_ID)); 1234567891011121314151617181920212223INSERT INTO PRODUCT_GROUP (GROUP_NAME)VALUES ('Smartphone'), ('Laptop'), ('Tablet'); COMMIT;INSERT INTO PRODUCT (PRODUCT_NAME, GROUP_ID, PRICE)VALUES ('Microsoft Lumia', 1, 200), ('HTC One', 1, 400), ('Nexus', 1, 500), ('iPhone', 1, 900), ('HP Elite', 2, 1200), ('Lenovo Thinkpad', 2, 700), ('Sony VAIO', 2, 700), ('Dell Vostro', 2, 800), ('iPad', 3, 700), ('Kindle Fire', 3, 150), ('Samsung Galaxy Tab', 3, 200); COMMIT; 1SELECT * FROM PRODUCT_GROUP; 1SELECT * FROM PRODUCT; 0-3. 분석 함수 문법 12345SELECT C1, 분석함수(C2, C3, ...) OVER (PARTITION BY C4 ORDER BY C5)FROM TABLE_NAME; 분석함수(C2, C3,…) : 사용하고자 하는 분석함수와 적용할 대상 컬럼을 지정 PARTITION BY : 분석 함수를 적용 시 기준이 되는 컬럼을 지정 (즉, 그룹별로 값을 구할 때 그룹핑의 기준 컬럼) ORDER BY : 정렬 컬럼을 지정 0-4. 분석 함수 결과 예시 집계 함수 vs 분석 함수: 집계 함수는 집계의 결과만 출력한다 분석 함수는 집계의 결과 및 테이블의 내용을 함계 출력한다. –&gt; 이게 바로 분석 함수의 역할이다. 12345-- 집계 함수SELECT COUNT(*)FROM PRODUCT 123456-- 분석 함수SELECT COUNT(*) OVER(), -- 집계 결과 A.* -- 원래 집합FROM PRODUCT A 1. AVG 함수 1-1. 개념 AVG 함수는 특정 집합 내에서 결과 건수의 변화 없이 해당 집합안에서 특정 컬럼의 평균을 구하는 함수이다. 1-2. AVG 함수 실습 1SELECT * FROM PRODUCT_GROUP; 1SELECT * FROM PRODUCT; (1) 전체 평균 가격(PRICE) 구하기 &gt;&gt; 집계함수 사용 AVG: 집계의 결과만 출력 12345-- 집계 함수(AVG): 집계의 결과만 출력SELECTAVG(PRICE)FROMPRODUCT; &gt;&gt; 분석함수 사용 AVG ( ) OVER ( ) : 결과 집합을 그대로 출력하면서 집계 결과도 함계 출력 1234567-- 분석 함수SELECT PRODUCT_NAME, PRICE, AVG(PRICE) OVER()FROM PRODUCT; (2) 그룹별 평균 가격(PRICE) 구하기 &gt;&gt; 집계함수 사용 GROUP BY + AVG: 집계의 결과만 출력 123456789-- 집계 함수: GROUP BY + AVGSELECTB.GROUP_NAME,AVG(A.PRICE)FROM PRODUCT AINNER JOIN PRODUCT_GROUP BON (A.GROUP_ID = B.GROUP_ID)GROUP BYB.GROUP_NAME; &gt;&gt; 분석함수 사용 AVG (C1) OVER ( PARTITION BY C2 ) : 결과 집합을 그대로 출력하면서 집계 결과도 함계 출력 1234567891011-- 분석 함수SELECTA.PRODUCT_NAME,A.PRICE, B.GROUP_NAME,AVG(A.PRICE) OVER (PARTITION BY B.GROUP_NAME)FROMPRODUCT AINNER JOIN PRODUCT_GROUP BON A.GROUP_ID = B.GROUP_ID; (3) 그룹별 누적 평균 가격(PRICE) 구하기 &gt;&gt; 분석함수 사용 AVG (C1) OVER ( PARTITION BY C2 ORDER BY C3 ) 12345678910SELECT A.PRODUCT_NAME, A.PRICE, B.GROUP_NAME, AVG(A.PRICE) OVER (PARTITION BY B.GROUP_NAME ORDER BY A.PRICE)FROM PRODUCT AINNER JOIN PRODUCT_GROUP BON A.GROUP_ID = B.GROUP_ID; 2. ROW_NUMBER, RANK, DENSE_RANK 함수 2-1. 개념 ROW_NUMBER, RANK, DENSE_RANK 함수는 모두 특정 집합 내에서 결과 건수의 변화 없이 해당 집합안에서 특정 컬럼의 순위를 구하는 함수이다. ROW_NUMBER: 같은 순위가 있어도 무조건 순차적으로 순으로 순위를 매긴다. (1, 2, 3, 4, 5 …) RANK: 같은 순위가 있으면 동일 순위로 매기고 그 다음 순위를 건너뛰다. (1, 1, 3, 4, 5 …) DENSE_RANK: 같은 순위가 있으면 동일 순위로 매기고 그 다음 순위를 건너뛰지 않는다. (1, 1, 2, 3, 4 …) 2-2. 순위 함수 실습 1SELECT * FROM PRODUCT_GROUP; 1SELECT * FROM PRODUCT; 2-2-1. ROW_NUMBER 함수 실습 ROW_NUMBER: 같은 순위가 있어도 무조건 순차적으로 순으로 순위를 매긴다. (1, 2, 3, 4, 5…) 12345678910SELECT A.PRODUCT_NAME, B.GROUP_NAME, A.PRICE, ROW_NUMBER() OVER (PARTITION BY B.GROUP_NAME ORDER BY A.PRICE)FROM PRODUCT AINNER JOIN PRODUCT_GROUP BON A.GROUP_ID = B.GROUP_ID; Laptop 에서 가격순으로 정렬했을 때 \"Sony VAIO\"와 \"Lenovo Thinkpad\"의 가격이 동일해도 (즉, 가격 순위 같아도) 순차적으로 순번을 부여한다 2-2-2. RANK 함수 실습 RANK: 같은 순위가 있으면 동일 순위로 매기고 그 다음 순위는 건너뛰다. (1, 1, 3, 4, 5 …) 12345678910SELECT A.PRODUCT_NAME, B.GROUP_NAME, A.PRICE, RANK() OVER (PARTITION BY B.GROUP_NAME ORDER BY A.PRICE)FROM PRODUCT AINNER JOIN PRODUCT_GROUP BON A.GROUP_ID = B.GROUP_ID; 2-2-3. DENSE_RANK 함수 실습 DENSE_RANK: 같은 순위가 있으면 동일 순위로 매기고 그 다음 순위를 건너뛰지 않는다. (1, 1, 2, 3, 4 …) 1234567891011SELECT PRODUCT_NAME, GROUP_NAME, PRICE, DENSE_RANK() OVER (PARTITION BY GROUP_NAME ORDER BY PRICE)FROM PRODUCT AINNER JOIN PRODUCT_GROUP BON A.GROUP_ID = B.GROUP_ID; document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 5. Analytic Function","slug":"【STUDY-SQL】/SQL-5-Analytic-Function","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-5-Analytic-Function/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Analytic Function","slug":"Analytic-Function","permalink":"https://hyemin-kim.github.io/tags/Analytic-Function/"}]},{"title":"SQL >> 집계 함수 (2) -- 고급 집계 함수","slug":"S-SQL-Aggregate-2","date":"2020-11-16T23:46:08.000Z","updated":"2020-11-20T06:44:10.990Z","comments":true,"path":"2020/11/17/S-SQL-Aggregate-2/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/17/S-SQL-Aggregate-2/","excerpt":"","text":"집계 함수 (2) – 고급 집계 함수 1. GROUPING SET 절 1-0. 학습 준비 (데이터 생성) 1-1. GROUP BY 절 활용 (1) 2개 컬럼 GROUP BY 절 (2) 1개 컬럼 GROUP BY 절 (3) GROUP BY 안하기 (4) 추출된 정보 합치기 – UNION ALL의 활용 1-2. GROUPING SET 절 활용 1-2-1. 용도 1-2-2. GROUPING SET 절 문법 1-2-3. GROUPING SET 절 실습 2. ROLL UP 절 2-1. 용도 2-2. ROLLUP 절 문법 (1) 전체 ROLL UP (2) 부분 ROLL UP 2-3. ROLLUP 절 실습 (1) GROUP BY만 사용 (ROLL UP 안함) (2) GROUP BY + 전체 ROLL UP (3) GROUP BY + 부분 ROLL UP 3. CUBE 절 3-1. 용도 3-2. CUBE 절 문법 (1) 전체 컬럼 CUBE 지정 (2) 부분 컬럼 CUBE 지정 3-3. CUBE 절 실습 (1) 전체 컬럼 CUBE 지정 (2) 부분 컬럼 CUBE 지정 1. GROUPING SET 절 1-0. 학습 준비 (데이터 생성) 12345678910111213141516CREATE TABLE SALES( BRAND VARCHAR NOT NULL, SEGMENT VARCHAR NOT NULL, QUANTITY INT NOT NULL, PRIMARY KEY (BRAND, SEGMENT));INSERT INTO SALES (BRAND, SEGMENT, QUANTITY)VALUES ('ABC', 'Premium', 100), ('ABC', 'Basic', 200), ('XYZ', 'Premium', 100), ('XYZ', 'Basic', 300); COMMIT; 1SELECT * FROM SALES; 1-1. GROUP BY 절 활용 (1) 2개 컬럼 GROUP BY 절 [MISSION 1] BRAND별, SEGMENT별 총 판패량 구하기 123456789SELECT BRAND, SEGMENT, SUM(QUANTITY)FROM SALESGROUP BY BRAND, SEGMENT; (2) 1개 컬럼 GROUP BY 절 [MISSION 2] BRAND별 총 판매량 구하기 1234567SELECT BRAND, SUM(QUANTITY)FROM SALESGROUP BY BRAND; [MISSION 3] SEGMENT별 총 판매량 구하기 1234567SELECT SEGMENT, SUM(QUANTITY)FROM SALESGROUP BY SEGMENT; (3) GROUP BY 안하기 [MISSION 4] 판매량 전체 합계 구하기 1234SELECT SUM(QUANTITY)FROM SALES; (4) 추출된 정보 합치기 – UNION ALL의 활용 123456789101112131415161718192021222324252627SELECT -- BRAND별, SEGMENT별 총 판패량 BRAND, SEGMENT, SUM(QUANTITY)FROM SALESGROUP BY BRAND, SEGMENTUNION ALLSELECT -- BRAND별 총 판패량 BRAND, NULL, SUM(QUANTITY)FROM SALESGROUP BY BRANDUNION ALLSELECT -- SEGMENT별 총 판패량 NULL, SEGMENT, SUM(QUANTITY)FROM SALESGROUP BY SEGMENTUNION ALLSELECT -- 전체 총 판패량 NULL, NULL, SUM(QUANTITY)FROM SALES; [주의] 각각의 UNION query는 같은 수의 columns를 가져야 한다. 따라서 각 부분의 SELECT 절에서 컬럼수가 부족하면 NULL로 채워야 함. 이 방법의 단점: 동일한 테이블을 4번씩이나 읽고 있다. --&gt; 성능 저하 가능성이 존재 SQL 문이 너무 길어진다. --&gt; 복잡하다 --&gt; 유지보수가 용이하지 않다 &gt;&gt; 이런 불편함을 줄이기 위해서 GROUPING SET 절을 활용한다. 1-2. GROUPING SET 절 활용 1-2-1. 용도 GROUPING SET 절을 사용하여 여러 개의 UNION ALL을 이용한 SQL과 같은 결과를 도출할 수 있다. 1-2-2. GROUPING SET 절 문법 GROUPING SET 절을 이용하면 한번에 다양한 기준의 컬럼 조합으로 집계를 구할 수 있다. 1234567891011121314SELECT C1, C2, 집계함수(C3)FROM TABLE_NAMEGROUP BYGROUPING SETS( (C1, C2), (C1), (C2), ()); 1-2-3. GROUPING SET 절 실습 &gt;&gt; GROUPING SET 절의 활용 GROUPING SET 절을 이용하여 BRAND, SEGMENT 기준, BRAND 기준, SEGMENT 기준, 전체기준으로 QUANTITY 합계의 값을 구할 수 있다. 1SELECT * FROM SALES; 1234567891011121314SELECT BRAND, SEGMENT, SUM(QUANTITY)FROM SALESGROUP BYGROUPING SETS( (BRAND, SEGMENT), (BRAND), (SEGMENT), ()); &gt;&gt; GROUPING 함수의 활용 GROUPING 함수를 이용하여 해당 컬럼이 GROUPING 시 사용되었으면 0, 그렇지 않으면 1을 리턴한다. 1SELECT * FROM SALES; 123456789101112131415161718SELECT GROUPING(BRAND) AS GROUPING_BRAND, GROUPING(SEGMENT) AS GROUPING_SEGMENT, BRAND, SEGMENT, SUM(QUANTITY)FROM SALESGROUP BYGROUPING SETS( (BRAND, SEGMENT), (BRAND), (SEGMENT), ())ORDER BY BRAND, SEGMENT; 123456789101112131415161718192021SELECT CASE WHEN GROUPING(BRAND) = 0 AND GROUPING(SEGMENT) = 0 THEN '브랜드별 + 등급별' WHEN GROUPING(BRAND) = 0 AND GROUPING(SEGMENT) = 1 THEN '브랜드별' WHEN GROUPING(BRAND) = 1 AND GROUPING(SEGMENT) = 0 THEN '등급별' WHEN GROUPING(BRAND) = 1 AND GROUPING(SEGMENT) = 1 THEN '전체합계' ELSE '' END AS \"집계기준\", BRAND, SEGMENT, SUM(QUANTITY)FROM SALESGROUP BYGROUPING SETS( (BRAND, SEGMENT), (BRAND), (SEGMENT), ())ORDER BY BRAND, SEGMENT; 2. ROLL UP 절 2-1. 용도 지정된 GROUPING 컬럼의 소계를 생성하는데 사용된다. 간단한 문법으로 다양한 소계를 출력할 수 있다. 2-2. ROLLUP 절 문법 ROLLUP 절은 GROUP BY 절과 함계 사용된다. ROLLUP 할 컬럼은 무조건 SELECT 절에 포함되어 있어야 한다. ROLLUP 절 컬럼의 지정 순서가 의미 있다. (1) 전체 ROLL UP 컬럼의 지정 순서가 의미 있음 123456789-- 전체 ROLL UPSELECT C1, C2, C3, 집계함수(C4)FROM TABLE_NAMEGROUP BY ROLLUP(C1, C2, C3); -- 소계를 생성할 컬럼을 지정한다. -- 컬럼 지정 순서에 따라 결과값이 달라질 수 있다. (2) 부분 ROLL UP 특정 컬럼만 분리하여 ROLL UP 할 수 있다 이런 경우에 분리된 특정 컬럼(C1)으로 시작하는 GROUPING SET 만 해당 즉, 전체 ROLL UP과 달리, GROUPING 하지 않는 전체 합계를 구하지 않는다. 12345678-- 부분 ROLL UPSELECT C1, C2, C3, 집계함수(C4)FROM TABLE_NAMEGROUP BY C1 ROLLUP(C2, C3) -- 특정 컬럼을 제외한 부분적인 ROLLUP도 가능하다. 2-3. ROLLUP 절 실습 1SELECT * FROM SALES; (1) GROUP BY만 사용 (ROLL UP 안함) 1234567891011-- BRAND, SEGMENT 컬럼 기준으로 GROUP BY 한다.SELECT BRAND, SEGMENT, SUM(QUANTITY)FROM SALESGROUP BY BRAND, SEGMENTORDER BY BRAND, SEGMENT; (2) GROUP BY + 전체 ROLL UP 1234567891011-- BRAND, SEGMENT 컬럼 기준으로 ROLL UP 한다.SELECT BRAND, SEGMENT, SUM(QUANTITY)FROM SALESGROUP BY ROLLUP (BRAND, SEGMENT)ORDER BY BRAND, SEGMENT; 전체 컬럼 ROLLUP 결과: BRAND + SEGMENT 별 합계 --&gt; GROUP BY (BRAND, SEGMENT) 결과 BRAND 별 합계 (소계) --&gt; GROUP BY + ROLL UP 절의 첫번째 컬럼 전체 합계 (총계) (3) GROUP BY + 부분 ROLL UP 1234567891011-- SEGMENT 컬럼 기준으로 GROUP BY 한다 + BRAND 컬럼 기준으로 부분 ROLL UP 한다SELECT SEGMENT, BRAND, SUM(QUANTITY)FROM SALESGROUP BY SEGMENT, ROLLUP (BRAND)ORDER BY SEGMENT, BRAND 부분 컬럼 ROLLUP 결과: SEGMENT, BRAND 별 합계 --&gt; GROUP BY (SEGMENT, BRAND) 결과 SEGMENT 별 합계 (소계) --&gt; ROLLUP 절에서 제외된 특정 컬럼 **&gt;&gt; **전체 합계 (총계)를 구하지 않는다 3. CUBE 절 3-1. 용도 지정된 GROUPING 컬럼의 다차원 소계를 생성하는데 사용된다. 간단한 문법으로 다차원 소계를 출력할 수 있다. 3-2. CUBE 절 문법 CUBE절은 GROUP BY 절과 함계 사용된다. CUBE 할 컬럼은 무조건 SELECT 절에 포함되어 있어야 한다. CUBE절 컬럼의 지정 순서가 의미 없다 (1) 전체 컬럼 CUBE 지정 컬럼의 지정 순서가 의미 없음 지정한 그룹의 모든 경우의 수 에 대한 소계와 총계를 구한다 1234567SELECT C1, C2, C3, 집계함수(C4)FROM TABLE_NAMEGROUP BY CUBE (C1, C2, C3); CUBE 절 내 인자의 개수가 N개이면 2의 N승의 소계가 발생하게 된다. CUBE (C1, C2, C3)를 GROUPING SETS으로 표현하면 총 9개의 소계가 발생한다. (2) 부분 컬럼 CUBE 지정 특정 컬럼만 분리하여 CUBE 를 지정할 수 있다 이런 경우에 분리된 특정 컬럼(C1)으로 시작하는 GROUPING SET 만 해당 1234567SELECT C1, C2, C3, 집계함수(C4)FROM TABLE_NAMEGROUP BY C1, CUBE (C2, C3); 3-3. CUBE 절 실습 1SELECT * FROM SALES; (1) 전체 컬럼 CUBE 지정 12345678910-- BRAND, SEGMENT 컬럼 기준으로 CUBE 한다.SELECT BRAND, SEGMENT, SUM(QUANTITY)FROM SALESGROUP BY CUBE (BRAND, SEGMENT)ORDER BY BRAND, SEGMENT; 전체 컬럼 CUBE 결과: BRAND + SEGMENT 별 합계 --&gt; GROUP BY (BRAND, SEGMENT) 결과 BRAND 별 합계 (소계) SEGMENT 별 합계 (소계) 전체 합계 (총계) &gt;&gt; 인자가 2개 이므로 총 4개의 경우의 수가 합계로 출력된다 (2) 부분 컬럼 CUBE 지정 1234567891011-- BRAND 컬럼 기준으로 GROUP BY 한다 + SEGMENT 컬럼 기준으로 부분 CUBE 한다SELECT BRAND, SEGMENT, SUM(QUANTITY)FROM SALESGROUP BY BRAND, CUBE (SEGMENT)ORDER BY BRAND, SEGMENT 부분 컬럼 CUBE 결과: BRAND + SEGMENT 별 합계 --&gt; GROUP BY (BRAND, SEGMENT) 결과 BRAND 별 합계 (소계) --&gt; CUBE 절에서 제외된 특정 컬럼 &gt;&gt; SEGMENT 별 합계 (소계)를 구하지 않는다. **&gt;&gt; ** 전체 합계 (총계)를 구하지 않는다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 4. Aggregate Function","slug":"【STUDY-SQL】/SQL-4-Aggregate-Function","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-4-Aggregate-Function/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Aggregate","slug":"Aggregate","permalink":"https://hyemin-kim.github.io/tags/Aggregate/"}]},{"title":"SQL >> 집계 함수 (1) -- 기초 집계 함수","slug":"S-SQL-Aggregate-1","date":"2020-11-12T11:23:06.000Z","updated":"2020-11-20T06:44:02.835Z","comments":true,"path":"2020/11/12/S-SQL-Aggregate-1/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/12/S-SQL-Aggregate-1/","excerpt":"","text":"집계 함수 (1) – 기초 집계 함수 1. GROUP BY 절 1-1. 개념 1-2. GROUP BY 절 문법 1-3. GROUP BY 절 실습 1-3-0. 실습 데이터 1-3-1. 단순 GROUP BY 1-3-2. GROUPING + GROUP 별 요약 2. HAVING 절 2-1. 개념 2-2. HAVING 절 문법 2-3. HAVING 절 실습 2-3-1. GROUP BY “합계” + HAVING 2-3-2. GROUP BY “카운트” + HAVING 1. GROUP BY 절 1-1. 개념 GROUP BY 절은 SELECT 문에서 반환된 행을 그룹으로 나눈다. 각 그룹에 대한 합계, 평균, 카운트 등을 계산할 수 있다. 1-2. GROUP BY 절 문법 1234567SELECT COLUMN_1, -- GROUPING 기준 컬럼 기재 집계함수(COLUMN2) -- 집계함수 사용하여 그룹별 요약값 도출FROM TABLE_NAMEGROUP BY COLUMN_1; -- GROUP BY 절 기재, N개의 컬럼을 GROUP BY 하는 경우 ','구분 -- GROUP BY 절은 FROM 또는 WHERE절 바로 뒤에 나타나야 함 1-3. GROUP BY 절 실습 1-3-0. 실습 데이터 &gt;&gt; “dvdrental” 데이터 --&gt; “payment” 테이블 1-3-1. 단순 GROUP BY &gt;&gt; 특정 컬럼의 UNIQUE VALUE를 추출할 때 쓰이다 (SELECT DISTINCT과 유사) [MISSION] 중복 값이 제거된 CUSTOMER_ID를 추출 1234567-- GROUP BY 사용SELECT CUSTOMER_IDFROM PAYMENTGROUP BY CUSTOMER_ID; 1234-- [대체] SELECT DISTINCT 사용SELECT DISTINCT CUSTOMER_IDFROM PAYMENT; 1-3-2. GROUPING + GROUP 별 요약 1) 합계 구하기 [MISSION] 거래액이 (AMOUNT의 합계) 가장 많은 고객순으로 출력 1234567891011-- 거래액이 (AMOUNT의 합계) 가장 많은 고객순으로 출력SELECT CUSTOMER_ID, FIRST_NAME, LAST_NAME, SUM(AMOUNT) AS AMOUNT_SUMFROM PAYMENTGROUP BY CUSTOMER_IDORDER BY AMOUNT_SUM DESC; 2) 카운트 구하기 [MISSION 1] 직원별 처리한 결제 건수 출력 12345678-- 직원별 처리한 결제 건수 출력SELECT STAFF_ID, COUNT(PAYMENT_ID) AS N_PAYMENTFROM PAYMENTGROUP BY STAFF_ID; [MISSION 2] STAFF 테이블에 있는 직원 이름 (FIRST_NAME, LAST_NAME)도 함께 추출 12345678910111213141516-- STAFF 테이블에 있는 직원 이름 (FIRST_NAME, LAST_NAME)도 함께 추출SELECT A.STAFF_ID, A.FIRST_NAME, A.LAST_NAME, COUNT(B.PAYMENT_ID) AS N_PAYMENTFROM STAFF AINNER JOIN PAYMENT BON A.STAFF_ID = B.STAFF_IDGROUP BY -- [주의]: SELECT 문에서 집계함수를 제외한 모든 컬럼명을 GROUP BY에서 적어야 함 A.STAFF_ID, B.STAFF_ID, A.FIRST_NAME, A.LAST_NAME; 2. HAVING 절 2-1. 개념 HAVING 절은 GROUP BY 절과 함께 사용하여 GROUP BY의 결과를 특정 조건으로 필터링하는 기능을 한다. 2-2. HAVING 절 문법 12345678SELECT COLUMN_1, -- GROUPING 기준 컬럼 기재 집계함수(COLUMN_2) -- 집계함수 사용하여 그룹별 요약값 도출FROM TABLE_NAMEGROUP BY -- GROUP BY 절 기재, N개의 컬럼을 GROUP BY 하는 경우 ','구분 COLUMN_1 -- GROUP BY 절은 FROM 또는 WHERE절 바로 뒤에 나타나야 함HAVING 조건식; HAVING 절은 GROUP BY 절에 의해 생성된 그룹행의 조건을 설정한다 반면에 WHERE 절은 GROUP BY 절이 적용된기 전에 개별 행의 조건을 설정한다 2-3. HAVING 절 실습 2-3-1. GROUP BY “합계” + HAVING [GROUP BY 결과 출력] 12345678-- 거래액이 (AMOUNT의 합계) 가장 많은 고객순으로 출력SELECT CUSTOMER_ID, SUM(AMOUNT) AS AMOUNT_SUMFROM PAYMENTGROUP BY CUSTOMER_ID; [MISSION 1] GROUP BY의 결과 값 중에서 AMOUNT_SUM이 200을 초과하는 행 출력 12345678910-- AMOUNT_SUM &gt; 200SELECT CUSTOMER_ID, SUM(AMOUNT) AS AMOUNT_SUMFROM PAYMENTGROUP BY CUSTOMER_IDHAVING SUM(AMOUNT) &gt; 200; -- [주의]: 여기서 SUM(AMOUNT)의 ALIAS(별칭)을 쓰면 안됨. 주의: HAVING 절 뒤에 집계 데이터의 별칭(ALIAS)을 쓰면 안됨. (The HAVING clause is evaluated before the SELECT - so the server doesn’t yet know about that alias.) [MISSION 2] CUSTOMER 테이블에 있는 고객 이메일 주소 (EMAIL)도 함께 추출 1234567891011121314SELECT A.CUSTOMER_ID, B.EMAIL, SUM(A.AMOUNT) AS AMOUNT_SUMFROM PAYMENT A, CUSTOMER BWHERE A.CUSTOMER_ID = B.CUSTOMER_IDGROUP BY A.CUSTOMER_ID, B.EMAILHAVING SUM(A.AMOUNT) &gt; 200; 2-3-2. GROUP BY “카운트” + HAVING [GROUP BY 결과 출력] 12345678-- 매장(STORE)별 구매 고객 수 추출SELECT STORE_ID, COUNT(CUSTOMER_ID) AS N_CUSTOMERFROM CUSTOMERGROUP BY STORE_ID; [MISSION] 구매 고객 수가 300 이상인 매장만 출력 12345678910-- N_CUSTOMER &gt; 300SELECT STORE_ID, COUNT(CUSTOMER_ID) AS N_CUSTOMERFROM CUSTOMERGROUP BY STORE_IDHAVING COUNT(CUSTOMER_ID) &gt; 300; 1234567-- 해당 매장 정보 출력SELECT *FROM STOREWHERE STORE_ID = 1; document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 4. Aggregate Function","slug":"【STUDY-SQL】/SQL-4-Aggregate-Function","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-4-Aggregate-Function/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Aggregate","slug":"Aggregate","permalink":"https://hyemin-kim.github.io/tags/Aggregate/"}]},{"title":"SQL >> 조인 (2)","slug":"S-SQL-Join-2","date":"2020-11-12T06:21:33.000Z","updated":"2020-11-12T07:03:13.291Z","comments":true,"path":"2020/11/12/S-SQL-Join-2/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/12/S-SQL-Join-2/","excerpt":"","text":"조인 (2) 1. SELF 조인 1-1. 개념 1-2. SELF 조인 문법 1-3. SELF 조인 실습 1-3-1. 실습 준비 1-3-2. SELF 조인 실습 2. FULL OUTER 조인 2-1. 개념 2-2. FULL OUTER 조인 문법 2-3. FULL OUTER 조인 실습 2-3-1. BASKET 데이터를 활용한 간단한 실습 2-3-2. 추가 실습 3. CROSS 조인 3-1. 개념 3-2. CROSS 조인 문법 3-3. CROSS 조인 실습 3-3-0. 실습 준비 3-3-1. CROSS 조인 실습 4. NATURAL 조인 4-1. 개념 4-2. NATURAL 조인 문법 4-3. NATURAL 조인 실습 4-3-0. 실습 준비 4-3-1. NATURAL 조인 실습 1. SELF 조인 1-1. 개념 SELF 조인은 같은 테이블 끼리 특정 컬럼을 기준으로 매칭 되는 컬럼을 출력하는 조인이다. 즉, 같은 테이블의 데이터를 각각의 집합으로 분류한 후 조인한다. 1-2. SELF 조인 문법 12345678SELECT A.COL_1, A.COL_2, ..., B.COL_1, B.COL_3, ...FROM TABLE_NAME AS AINNER JOIN TABLE_NAME AS B -- THE SAME TABLE WITH THE FORMERON A.COL_T = B.COL_T 1-3. SELF 조인 실습 1-3-1. 실습 준비 12345678910CREATE TABLE EMPLOYEE( EMPLOYEE_ID INT PRIMARY KEY, FIRST_NAME VARCHAR(255) NOT NULL, LAST_NAME VARCHAR(255) NOT NULL, MANAGER_ID INT, FOREIGN KEY (MANAGER_ID) -- MANAGER_ID는 같은 테이블 (EMPLOYEE)의 EMPLOYEE_ID를 참조함 REFERENCES EMPLOYEE (EMPLOYEE_ID) ON DELETE CASCADE); 1234567891011121314151617INSERT INTO EMPLOYEE ( EMPLOTEE_ID, FIRST_NAME, LAST_NAME, MANAGER_ID)VALUES(1, 'Windy', 'Hays', NULL),(2, 'Ava', 'Christensen', 1),(3, 'Hassan', 'Conner', 1),(4, 'Anna', 'Reeves', 2),(5, 'Sau', 'Norman', 2),(6, 'Kelsie', 'Hays', 3),(7, 'Tory', 'Goff', 3),(8, 'Salley', 'Lester', 3);COMMIT; 1SELECT * FROM EMPLOYEE &gt;&gt; 조직도 1-3-2. SELF 조인 실습 &gt;&gt; SELF INNER 조인 실습 MISSION: 각 직원의 상위 관리자를 출력 최고관리자인 'Windy Hays’는 결과 집합에 포함시키지 않음. 1234567891011SELECT E.FIRST_NAME || ' ' || E.LAST_NAME AS EMPLOYEE, M.FIRST_NAME || ' ' || M.LAST_NAME AS MANAGERFROM EMPLOYEE E -- EMPLOYEE 중심INNER JOIN EMPLOYEE M -- MANAGER 중심ON E.MANAGER_ID = M.EMPLOYEE_ID -- 매칭 시 헷갈리지 않도록 주의ORDER BY MANAGER &gt;&gt; SELF LEFT OUTER 조인 실습 MISSION: 각 직원의 상위 관리자를 출력하면서 모든 직원을 출력 최고관리자인 'Windy Hays’가 결과 집합에 포함시킴 1234567891011SELECT E.FIRST_NAME || ' ' || E.LAST_NAME AS EMPLOYEE, M.FIRST_NAME || ' ' || M.LAST_NAME AS MANAGERFROM EMPLOYEE ELEFT OUTER JOIN EMPLOYEE MON E.MANAGER_ID = M.EMPLOYEE_IDORDER BY MANAGER &gt;&gt; 부정형 조건 실습 MISSION: FILM 테이블에서 영화의 상영시간이 동일한 서로 다른 영화의 리스트를 출력 film *film_id title discription release_year language_id rentall_duration rental_rate length replacement_cost rating last_update special_features fulltext 12345678910SELECT A.TITLE, B.TITLE, A.LENGTHFROM FILM AINNER JOIN FILM BON A.FILM_ID != B.FILM_ID AND A.LENGTH = B.LENGTH 2. FULL OUTER 조인 2-1. 개념 FULL OUTER 조인은 INNER, LEFT OUTER, RIGHT OUTER 조인 집합을 모두 출력하는 조인 방식이다. 즉, 두 테이블간 출력가능한 모든 데이터를 포함한 집합을 출력한다. 2-2. FULL OUTER 조인 문법 123456789SELECT A.COL_A1, A.COL_A2, ..., B.COL_B1, B.COL_B2, ...FROM TABLE_A AFULL OUTER JOIN TABLE_B BON A.COL_Z_A = B.COL_Z_B 2-3. FULL OUTER 조인 실습 2-3-1. BASKET 데이터를 활용한 간단한 실습 &gt;&gt; FULL OUTER JOIN (1) LEFT ONLY + LEFT&amp;RIGHT + RIGHT ONLY 1234567891011SELECT A.ID ID_A, A.FRUIT FRUIT_A, B.ID ID_B, B.FRUIT FRUIT_BFROM BASKET_A AFULL OUTER JOIN BASKET_B BON A.FRUIT = B.FRUIT (2) ONLY OUTER (LEFT ONLY + RIGHT ONLY) 12345678910111213SELECT A.ID ID_A, A.FRUIT FRUIT_A, B.ID ID_B, B.FRUIT FRUIT_BFROM BASKET_A AFULL OUTER JOIN BASKET_B BON A.FRUIT = B.FRUIT WHERE A.ID IS NULL -- LEFT OUTER OR B.ID IS NULL -- RIGHT OUTER 2-3-2. 추가 실습 &gt;&gt; 실습 준비 1234567891011121314CREATE TABLEIF NOT EXISTS DEPARTMENTS -- 종재하지 않으면 생성( DEPARTMENT_ID SERIAL PRIMARY KEY, DEPARTMENT_NAME VARCHAR (255) NOT NULL);CREATE TABLEIF NOT EXISTS EMPLOYEES( EMPLOYEE_ID SERIAL PRIMARY KEY, EMPLOYEE_NAME VARCHAR (255), DEPARTMENT_ID INTEGER); 1234567891011121314151617181920212223INSERT INTO DEPARTMENTS(DEPARTMENT_NAME)VALUES('Sales'),('Marketing'),('HR'),('IT'),('Production');COMMIT;INSERT INTO EMPLOYEES( EMPLOYEE_NAME, DEPARTMENT_ID)VALUES('Bette Nicholson', 1),('Christian Gable', 1),('Joe Swank', 2),('Fred Costner', 3),('Sandra Kilmer', 4),('Julia Mcqueen', NULL);COMMIT; 1SELECT * FROM DEPARTMENTS; 1SELECT * FROM EMPLOYEES; &gt;&gt; FULL OUTER JOIN 실습 12345678SELECT E.EMPLOYEE_NAME, D.DEPARTMENT_NAMEFROM EMPLOYEES EFULL OUTER JOIN DEPARTMENTS DON E.DEPARTMENT_ID = D.DEPARTMENT_ID &gt;&gt; RIGHT OUTER ONLY 실습 1234567891011-- 소속한 직원이 없는 부서만 출력-- FULL OUTER + RIGHT ONLYSELECT E.EMPLOYEE_NAME, D.DEPARTMENT_NAMEFROM EMPLOYEES EFULL OUTER JOIN DEPARTMENTS DON E.DEPARTMENT_ID = D.DEPARTMENT_IDWHERE E.EMPLOYEE_NAME IS NULL [P.S.] FULL OUTER JOIN+ RIGHT ONLY = RIGHT OUTER JOIN+ RIGHT ONLY 12345678910-- RIGHT OUTER + RIGHT ONLYSELECT E.EMPLOYEE_NAME, D.DEPARTMENT_NAMEFROM EMPLOYEES ERIGHT OUTER JOIN DEPARTMENTS DON E.DEPARTMENT_ID = D.DEPARTMENT_IDWHERE E.EMPLOYEE_NAME IS NULL &gt;&gt; LEFT OUTER ONLY 실습 1234567891011-- 소속한 부서가 없는 직원만 출력-- FULL OUTER + LEFT ONLYSELECT E.EMPLOYEE_NAME, D.DEPARTMENT_NAMEFROM EMPLOYEES EFULL OUTER JOIN DEPARTMENTS DON E.DEPARTMENT_ID = D.DEPARTMENT_IDWHERE D.DEPARTMENT_NAME IS NULL [P.S.] FULL OUTER JOIN+ LEFT ONLY = LEFT OUTER JOIN+ LEFT ONLY 12345678910-- LEFT OUTER + LEFT ONLYSELECT E.EMPLOYEE_NAME, D.DEPARTMENT_NAMEFROM EMPLOYEES ERIGHT OUTER JOIN DEPARTMENTS DON E.DEPARTMENT_ID = D.DEPARTMENT_IDWHERE D.DEPARTMENT_NAME IS NULL 3. CROSS 조인 3-1. 개념 두 개의 테이블의 CATESIAN PRODUCT 연산의 결과를 출력한다. 데이터 복제에 많이 쓰이는 기법이다. CATESIAN PRODUCT: 3-2. CROSS 조인 문법 123456SELECT *FROM CROSS_TABLE_1CROSS JOIN CROSS_TABLE_2 3-3. CROSS 조인 실습 3-3-0. 실습 준비 123456789CREATE TABLE CROSS_T1( LABEL CHAR(1) PRIMARY KEY);CREATE TABLE CROSS_T2( SCORE INT PRIMARY KEY); 1234567891011121314INSERT INTO CROSS_T1 (LABEL)VALUES('A'),('B');COMMIT;INSERT INTO CROSS_T2 (SCORE)VALUES(1),(2),(3);COMMIT; 1SELECT * FROM CROSS_T1 1SELECT * FROM CROSS_T2 3-3-1. CROSS 조인 실습 123456789-- 방법 1SELECT *FROM CROSS_T1CROSS JOIN CROSS_T2ORDER BY LABEL 1234567-- 방법 2SELECT * FROM CROSS_T1, CROSS_T2 -- INNER JOIN을 표현하는 다른 방법 (조건 없는 INNER JOIN)ORDER BY LABEL 위 두 개의 SQL 문 결과 집합이 동일하므로 같은 SQL문이라고 할 수 있다. SQL문의 목적은 집합을 출력하는 것이 때문이다. 즉, 추출한 정보가 같다면 SQL문 자체는 다르더라도 동일한 SQL 문이다. 4. NATURAL 조인 4-1. 개념 두개의 테이블에서 같은 이름을 가진 컬럼 간의 INNER 조인 집합 결과를 출력한다. SQL문 자체가 간소해지는 방법이다. 4-2. NATURAL 조인 문법 123456SELECT *FROM TABLE_ANATURAL JOIN -- 자동으로 두 테이블이 동일하게 가지고 있는 컬럼을 기준으로 INNER 조인한다 TABLE_B NATURAL 조인은 INNER 조인의 또 다른 SQL 작성 방식이다. 즉, 조인 컬럼을 명시하지 않아도 된다. 4-3. NATURAL 조인 실습 4-3-0. 실습 준비 1234567891011121314CREATE TABLE CATEGORIES( CATEGORY_ID SERIAL PRIMARY KEY, CATEGORY_NAME VARCHAR (255) NOT NULL);CREATE TABLE PRODUCTS( PRODUCT_ID SERIAL PRIMARY KEY, PRODUCT_NAME VARCHAR (255) NOT NULL, CATEGORY_ID INT NOT NULL, FOREIGN KEY (CATEGORY_ID) REFERENCES CATEGORIES (CATEGORY_ID)); 1234567891011121314151617181920INSERT INTO CATEGORIES(CATEGORY_NAME)VALUES ('Smart Phone'), ('Laptop'), ('Tablet');COMMIT;INSERT INTO PRODUCTS(PRODUCT_NAME, CATEGORY_ID)VALUES ('iPhone', 1), ('Samsung Galaxy', 1), ('HP Elite', 2), ('Lenovo Thinkpad', 2), ('iPad', 3), ('Kindle Fire', 3); COMMIT; 1SELECT * FROM CATEGORIES; 1SELECT * FROM PRODUCTS; 4-3-1. NATURAL 조인 실습 (1) 예제 데이터를 활용한 간단한 실습 123456SELECT * FROM PRODUCTS ANATURAL JOIN CATEGORIES B; 12345678910-- 대체 방법 1-- INNER JOIN으로 실현SELECT P.CATEGORY_ID, P.PRODUCT_ID, P.PRODUCT_NAME, C.CATEGORY_NAMEFROM PRODUCTS PINNER JOIN CATEGORIES CON P.CATEGORY_ID = C.CATEGORY_ID; 12345678-- 대체 방법 2 (INNER JOIN 대체 명령어)SELECT P.CATEGORY_ID, P.PRODUCT_ID, P.PRODUCT_NAME, C.CATEGORY_NAMEFROM PRODUCTS P, CATEGORIES CWHERE P.CATEGORY_ID = C.CATEGORY_ID; (2) “dvdrental” 데이터를 활용한 실습 &gt;&gt; 테이블 구성 &gt;&gt; 실습 두 테이블이 모두 “country_id” 컬럼이 존재한다. 이 두 테이블에 대해서 NATURAL JOIN을 진행해보면: 123456SELECT *FROM CITY ANATURAL JOIN COUNTRY B; 기대와 다르게 출력 결과가 0건이다. 그 이유는: 두 테이블 간에 동일한 이름으로 존재하는 컬럼이 COUNTRY_ID 외에 LAST_UPDATE 도 존재한다. 이런 경우 NATURAL JOIN 시에는 LAST_UPDATE 컬럼까지 INNER조인에 성공해야만 결과값이 나온다. 하지만 두 테이블의 LAST_UPDATE 값이 서로 다르므로 위 SQL문을 실행 후 조건에 만족하는 결과가 없다. 따라서 NATURAL 조인이 아닌 INNER 조인을 이용해야한다. 1234567SELECT * FROM CITY AINNER JOIN COUNTRY BON A.COUNTRY_ID = B.COUNTRY_ID; INNER 조인으로 ON절에 조인 컬럼을 명시하였고, 의도한 대로 데이터가 출력되었다. 1234567-- INNER JOIN 대체 명령어SELECT * FROM CITY A, COUNTRY BWHERE A.COUNTRY_ID = B.COUNTRY_ID; 이러한 이유로 NATURAL 조인은 실무에 잘 사용되지 않는다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 3. Join","slug":"【STUDY-SQL】/SQL-3-Join","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-3-Join/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Join","slug":"Join","permalink":"https://hyemin-kim.github.io/tags/Join/"}]},{"title":"SQL >> 조인 (1)","slug":"S-SQL-Join-1","date":"2020-11-12T05:34:10.000Z","updated":"2020-11-12T06:20:54.318Z","comments":true,"path":"2020/11/12/S-SQL-Join-1/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/12/S-SQL-Join-1/","excerpt":"","text":"조인 (1) 1. 조인이란? 1-1. 개념 1-2. 조인의 종류 2. 실습 준비 3. INNER 조인 3-1. 개념 3-2. INNER 조인 문법 3-3. INNER 조인 실습 3-3-1. BASKET 데이터를 활용한 간단한 실습 3-3-2. dvdrental 데이터를 활용한 실습 (1) 2개의 테이블 조인 (2) 3개의 테이블 조인 4. OUTER 조인 4-1. 개념 4-2. OUTER 조인 문법 (1) LEFT OUTER 조인 문법 (2) RIGHT OUTER 조인 문법 4-3. OUTER 조인 실습 1. 조인이란? 1-1. 개념 조인은 2개 이상의 테이블에 있는 정보 중 사용자가 필요한 집합에 맞게 가상의 테이블처럼 만들어서 결과를 보여주는 것이다. 1-2. 조인의 종류 종류 설명 INNER 조인 특정 컬럼을 기준으로 정확히 매칭된 집합을 출력한다 OUTER 조인 특정 컬럼을 기준으로 매칭된 집합을 출력하지만 한쪽의 집합은 모두 출력하고 다른 한쪽의 집합은 매칭되는 컬럼의 값 만을 출력한다 (왼쪽 집합을 기준으로 하면 LEFT OUTER, 오른쪽 집합을 기준으로 하면 RIGHT OUTER) SELT 조인 동일한 테이블 끼리의 특정 컬럼을 기준으로 매칭되는 집합을 출력한다 FULL OUTER 조인 INNER, LEFT OUTER, RIGHT OUTER 조인 집합을 모두 출력한다 CROSS 조인 Cartesian Product이라고도 하며 조인되는 두 테이블에서 곱집합을 반환한다 2. 실습 준비 실습을 위한 데이터 생성 1234567891011CREATE TABLE BASKET_A( ID INT PRIMARY KEY, FRUIT VARCHAR (100) NOT NULL);CREATE TABLE BASKET_B( ID INT PRIMARY KEY, FRUIT VARCHAR (100) NOT NULL) 123456789101112131415161718192021INSERT INTO BASKET_A (ID, FRUIT)VALUES (1, 'Apple'), (2, 'Orange'), (3, 'Banana'), (4, 'Cucumber');COMMIT;-- INSERT, UPDATE, DELETE로 데이터의 삽입 혹은 갱신을 실시한 후에 꼭 COMMIT/ROLLBACK을 실현해야함.INSERT INTO BASKET_B (ID, FRUIT)VALUES (1, 'Orange'), (2, 'Apple'), (3, 'Watermelon'), (4, 'Pear');COMMIT; 1SELECT * FROM BASKET_A; 1SELECT * FROM BASKET_B; 3. INNER 조인 3-1. 개념 INNER 조인은 대표적인 조인의 종유이다. 이는 특정 컬럼을 기준으로 정확히 매칭된 집합을 출력한다. 3-2. INNER 조인 문법 123456789SELECT A.COL_A1, A.COL_A2, ..., B.COL_B1, B.COL_B2, ...FROM TABLE_A AINNER JOIN TABLE_B BON A.COL_Z_A = B.COL_Z_B -- 조인의 기준이 되는 컬럼을 지정 3-3. INNER 조인 실습 3-3-1. BASKET 데이터를 활용한 간단한 실습 1234567891011SELECT -- 지정한 컬럼을 조회한다 A.ID ID_A, A.FRUIT FRUIT_A, B.ID ID_B, B.FRUIT FRUIT_BFROM -- BASKET_A 테이블에과 BASKET_B 테이블을 BASKET_A A -- FRUIT 컬럼 기준으로 조인한다.INNER JOIN BASKET_B BON A.FRUIT = B.FRUIT 3-3-2. dvdrental 데이터를 활용한 실습 (1) 2개의 테이블 조인 &gt;&gt; 테이블 구성 한 명의 고객은 여러 건의 결제내역을 가질 수 있다 하나의 결제는 반드시 고객을 가져야 한다 &gt;&gt; 실습 MISSION 1: CUSTOMER 테이블에 있는 고객 정보와 PAYMENT 테이블에 있는 결제정보를 종합하여 추출 12345678910SELECT A.CUSTOMER_ID, A.FIRST_NAME, A.LAST_NAME, A.EMAIL, B.AMOUNT, B.PAYMENT_DATEFROM CUSTOMER AINNER JOIN PAYMENT BON A.CUSTOMER_ID = B.CUSTOMER_ID MISSION 2: 위에서 추출된 데이터에서 CUSTOMER_ID가 2인 행만 추출 123456789101112SELECT A.CUSTOMER_ID, A.FIRST_NAME, A.LAST_NAME, A.EMAIL, B.AMOUNT, B.PAYMENT_DATEFROM CUSTOMER AINNER JOIN PAYMENT BON A.CUSTOMER_ID = B.CUSTOMER_IDWHERE A.CUSTOMER_ID = 2 (2) 3개의 테이블 조인 &gt;&gt; 테이블 구성 한 명의 직원은 여러 건의 결제내역을 처리한다 하나의 결제는 반드시 처리한 직원이 존재한다 한 명의 고객은 여러 건의 결제내역을 가질 수 있다 하나의 결제는 반드시 고객을 가져야 한다 &gt;&gt; 실습 MISSION: 결제를 진행한 고객 정보(CUSTOMER), 해당 고객의 결제내역(PAYMENT), 그리고 해당 결제를 처리하는 직원정보(STAFF)를 종합하여 추출 123456789101112SELECT A.CUSTOMER_ID, A.FIRST_NAME, A.LAST_NAME, A.EMAIL, B.AMOUNT, B.PAYMENT_DATE, C.FIRST_NAME AS S_FIRST_NAME, C.LAST_NAME AS S_LAST_NAMEFROM CUSTOMER AINNER JOIN PAYMENT B ON A.CUSTOMER_ID = B.CUSTOMER_IDINNER JOIN STAFF C ON B.STAFF_ID = C.STAFF_ID 4. OUTER 조인 4-1. 개념 특정 집합을 기준으로 매칭된 집합을 출력하지만, 한쪽의 집합은 모두 출력하고 다른 한쪽의 집합은 매칭되는 컬럼의 값 만을 출력한다. 4-2. OUTER 조인 문법 (1) LEFT OUTER 조인 문법 12345678910-- LEFT OUTER JOINSELECT A.COL_A1, A.COL_A2, ..., B.COL_B1, B.COL_B2, ...FROM TABLE_A ALEFT OUTER JOIN -- 'LEFT JOIN'만 사용해도 좋다 TABLE_B BON A.COL_Z_A = B.COL_Z_B (2) RIGHT OUTER 조인 문법 12345678910-- RIGHT OUTER JOINSELECT A.COL_A1, A.COL_A2, ..., B.COL_B1, B.COL_B2, ...FROM TABLE_A ARIGHT OUTER JOIN -- 'RIGHT JOIN'만 사용해도 좋다 TABLE_B BON A.COL_Z_A = B.COL_Z_B 4-3. OUTER 조인 실습 &gt;&gt; LEFT OUTER JOIN (1) LEFT ONLY + LEFT&amp;RIGHT 1234567891011SELECT A.ID ID_A, A.FRUIT FRUIT_A, B.ID ID_B, B.FRUIT FRUIT_BFROM BASKET_A ALEFT OUTER JOIN BASKET_B BON A.FRUIT = B.FRUIT (2) LEFT ONLY 123456789101112SELECT A.ID ID_A, A.FRUIT FRUIT_A, B.ID ID_B, B.FRUIT FRUIT_BFROM BASKET_A ALEFT OUTER JOIN BASKET_B BON A.FRUIT = B.FRUIT WHERE B.ID IS NULL &gt;&gt; RIGHT OUTER JOIN (1) RIGHT ONLY + LEFT&amp;RIGHT 12345678910SELECT A.ID ID_A, A.FRUIT FRUIT_A, B.ID ID_B, B.FRUIT FRUIT_BFROM BASKET_A ARIGHT JOIN BASKET_B BON A.FRUIT = B.FRUIT (2) RIGHT ONLY 1234567891011SELECT A.ID ID_A, A.FRUIT FRUIT_A, B.ID ID_B, B.FRUIT FRUIT_BFROM BASKET_A ARIGHT JOIN BASKET_B BON A.FRUIT = B.FRUITWHERE A.ID IS NULL document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 3. Join","slug":"【STUDY-SQL】/SQL-3-Join","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-3-Join/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Join","slug":"Join","permalink":"https://hyemin-kim.github.io/tags/Join/"}]},{"title":"【실습】 SQL >> 데이터 조회 및 필터링","slug":"E-SQL-selecting-and-filtering","date":"2020-11-10T12:01:42.000Z","updated":"2020-11-19T06:50:43.155Z","comments":true,"path":"2020/11/10/E-SQL-selecting-and-filtering/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/10/E-SQL-selecting-and-filtering/","excerpt":"","text":"【실습】 데이터 조회 및 필터링 [1] PAYMENT 테이블에서 단일 거래의 AMOUNT의 액수가 가장 많은 고객들의 CUSTOMER_ID를 추출하라. 단, CUSTOMER_ID의 값은 유일해야 한다. payment * payment_id customer_id staff_id rental_id amount payment_date &gt;&gt; 문제 풀이 우선 전체 거래 중 AMOUNT의 액수가 가장 큰 AMOUNT를 구한다. 1234SELECT AMOUNT FROM PAYMENTORDER BY AMOUNT DESCLIMIT 1 그 다음 PAYMENT 테이블에서 가장 큰 AMOUNT를 가진 CUMSTOMER_ID를 구하고 중복을 제거한다 123456789SELECT DISTINCT A.CUSTOMER_IDFROM PAYMENT AWHERE A.AMOUNT = ( SELECT B.AMOUNT FROM PAYMENT B ORDER BY B.AMOUNT DESC LIMIT 1) [2] 고객들에게 단체 이메일을 전송하고자 한다. CUSTOMER 테이블에서 고객의 EMAIL 주소를 추출하고, 이메일 형식에 맞지 않은 이메일 주소는 제외시켜라. (이메일 형식은: '@'가 존재해야 하고; '@'로 시작하지 말아야 하고; '@'로 끝나지 말아야 한다.) customer * customer_id store_id first_name email address_id activebool create_date last_update active &gt;&gt; 문제 풀이 12345SELECT EMAIL FROM CUSTOMERWHERE EMAIL LIKE '%@%' AND EMAIL NOT LIKE '@%' AND EMAIL NOT LIKE '%@' document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【EXERCISE】","slug":"【EXERCISE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/"},{"name":"SQL","slug":"【EXERCISE】/SQL","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Selecting","slug":"Selecting","permalink":"https://hyemin-kim.github.io/tags/Selecting/"},{"name":"Filtering","slug":"Filtering","permalink":"https://hyemin-kim.github.io/tags/Filtering/"}]},{"title":"SQL >> 데이터 필터링 (2)","slug":"S-SQL-Filtering-2","date":"2020-11-10T05:25:03.000Z","updated":"2020-11-12T11:43:00.149Z","comments":true,"path":"2020/11/10/S-SQL-Filtering-2/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/10/S-SQL-Filtering-2/","excerpt":"","text":"데이터 필터링 (2) 1. IN 연산자 1-1. 용도 1-2. IN 연산자 문법 1) IN 문법 2) NOT IN 문법 1-3. IN 연산자 실습 2. BETWEEN 연산자 2-1. 용도 2-2. BATWEEN 연산자 문법 1) BETWEEN 문법 2) NOT BETWEEN 문법 2-3. BETWEEN 연산자 실습 3. LIKE 연산자 3-1. 용도 3-2. LIKE 연산자 문법 1) LIKE 문법 2) NOT LIKE 문법 3) 특정 패턴 3-3. LIKE 연산자 실습 4. IS NULL 연산자 4-1. 용도 4-2. IS NULL 연산자 문법 1) IS NULL 문법 2) IS NOT NULL 문법 4-3. IS NULL 연산자 실습 1. IN 연산자 1-1. 용도 IN 연산자는 특정 집합(컬럼 혹은 리스트)에서 특정 집합 혹은 리스트가 존재하는지 판단하는 연산자이다. 1-2. IN 연산자 문법 1) IN 문법 123456-- COLUMN_NAME 집합에서 VALUE1, VALUE2등의 값이 존재하는지 확인 (조건에 만족한 행을 출력)SELECT *FROM TABLE_NAMEWHERE COLUMN_NAME IN (VALUE1, VALUE2, ...) 1234567-- COLUMN_NAME 집합에서 TABLE_NAME2 테이블의 COLUMMN_NAME2 집합이 존재하는지 확인SELECT *FROM TABEL_NAMEWHERE COLUMN_NAME IN (SELECT COLUMN_NAME2 FROM TABLE_NAME2) -- 서브 커리 2) NOT IN 문법 12345678-- NOT IN ---- COLUMN_NAME 집합에서 값이 VALUE1, VALUE2가 아닌 행을 출력SELECT * FROM TABLE_NAMEWHERE COLUMN_NAME NOT IN (VALUE1, VALUE2) 1-3. IN 연산자 실습 &gt;&gt; TABLE rental * rental_id rental_date inventory_id customer_id return_date staff_id last_update &gt;&gt; IN 실습 1234567891011-- CUSTOMER_ID가 1 혹은 2인 행을 뽑아서 RETURN_DATE 내림차순으로 출력한다 SELECT CUSTOMER_ID, RENTAL_ID, RETURN_DATEFROM RENTALWHERE CUSTOMER_ID IN (1, 2)ORDER BY RETURN_DATE DESC IN 연산자는 ‘OR’ &amp;&amp; ‘=’ 과 같다 1234567891011121314-- OR 사용 ---- CUSTOMER_ID가 1 혹은 2인 행을 뽑아서 RETURN_DATE 내림차순으로 출력한다SELECT CUSTOMER_ID, RENTAL_ID, RETURN_DATEFROM RENTALWHERE CUSTOMER_ID = 1 OR CUSTOMER_ID = 2ORDER BY RETURN_DATE DESC &gt;&gt; NOT IN 실습 123456789101112-- CUSTOMER_ID가 1 혹은 2가 아닌 행을 뽑아서 RETURN_DATE 내림차순으로 출력한다SELECT CUSTOMER_ID, RENTAL_ID, RETURN_DATEFROM RENTALWHERE CUSTOMER_ID NOT IN (1, 2)ORDER BY RETURN_DATE DESC NOT IN 연산자는 ‘AND’ &amp;&amp; ‘!=’ 과 같다 12345678910111213-- CUSTOMER_ID가 1 혹은 2가 아닌 행을 뽑아서 RETURN_DATE 내림차순으로 출력한다SELECT CUSTOMER_ID, RENTAL_ID, RETURN_DATEFROM RENTALWHERE CUSTOMER_ID != 1 AND CUSTOMER_ID != 2ORDER BY RETURN_DATE DESC &gt;&gt; 서브 커리 Mission: 2005년 5월 27일에 DVD 반납한 고객의 이름(FIRST_NAME &amp; LAST_NAME)을 출력 먼저 RENTAL 테이블에서 2005년 5월 27일에 DVD 반납한 고객의 ID(CUSTOMER_ID)를 추출 (서브 커리 부분) 그다음 CUSTOMER 테이블에서 해당 ID인 고객의 이름(FIRST_NAME &amp; LAST_NAME)을 출력 (메인 커리 부분) 12345678-- 서브 커리 부분 ---- RETURN_DATE가 2005년 5월 27일인 CUSTOMER_ID를 출력한다SELECT CUSTOMER_IDFROM RENTALWHERE CAST(RETURN_DATE AS DATE) = '2005-05-27' 123456789101112131415-- 메인 커리 부분 ---- 해당 ID인 고객의 FIRST_NAME &amp; LAST_NAME 출력SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERWHERE CUSTOMER_ID IN ( SELECT CUSTOMER_ID FROM RENTAL WHERE CAST(RETURN_DATE AS DATE) = '2005-05-27') 2. BETWEEN 연산자 2-1. 용도 BETWEEN 연산자는 특정 집합에서 어떠한 컬럼의 값이 특정 범위안에 들어가는 집합을 출력하는 연산자이다. 2-2. BATWEEN 연산자 문법 1) BETWEEN 문법 123456789-- COLUMN_NAME의 값이 VALUE_A와 VALUE_B사이에 있는 집합을 출력한다-- COLUMN_NAME &gt;= VALUE_A AND COLUMN_NAME &lt;= BSELECT *FROM TABLE_NAMEWHERE COLUMN_NAME BETWEEN VALUE_A AND VALUE_B 2) NOT BETWEEN 문법 123456789-- COLUMN_NAME의 값이 VALUE_A와 VALUE_B 사이에 있지 않은 집합을 출력한다-- COLUMN_NAME &lt; VALUE_A OR COLUMN_NAME &gt; VALUE_BSELECT * FROM TABLE_NAMEWHERE COLUMN_NAME NOT BETWEEN VALUE_A AND VALUE_B 2-3. BETWEEN 연산자 실습 &gt;&gt; TABLE payment * payment_id customer_id staff_id rental_id amount payment_date &gt;&gt; BETWEEN 실습 12345678-- PAYMENT 테이블에서 AMOUNT가 8과 9사이에 있는 행의 CUSTOMER_ID, PAYMENT_ID, AMOUNT를 출력SELECT CUSTOMER_ID, PAYMENT_ID, AMOUNTFROM PAYMENTWHERE AMOUNT BETWEEN 8 AND 9 123456789-- 위 SQL은 이 SQL과 결과가 동일함SELECT CUSTOMER_ID, PAYMENT_ID, AMOUNTFROM PAYMENTWHERE AMOUNT &gt;= 8 AND AMOUNT &lt;- 9 &gt;&gt; NOT BETWEEN 실습 12345678-- PAYMENT 테이블에서 AMOUNT가 8부터 9사이가 아닌 행의 CUSTOMER_ID, PAYMENT_ID, AMOUNT를 출력SELECT CUSTOMER_ID, PAYMENT_ID, AMOUNTFROM PAYMENTWHERE AMOUNT NOT BETWEEN 8 AND 9 123456789-- 위 SQL은 이 SQL과 결과가 동일함SELECT CUSTOMER_ID, PAYMENT_ID, AMOUNTFROM PAYMENTWHERE AMOUNT &lt; 8 OR AMOUNT &gt; 9 &gt;&gt; 일자 비교 12345678910111213141516171819-- PAYMENT_DATE가 2007년 2월 7일부터 2007년 2월 15일 데이터를 추출함-- [방법 1]SELECT CUSTOMER_ID, PAYMENT_ID, AMOUNT, PAYMENT_DATEFROM PAYMENTWHERE CAST(PAYMENT_DATE AS DATE) BETWEEN '2007-02-07' AND '2007-02-15' -- [방법 2]SELECT CUSTOMER_ID, PAYMENT_ID, AMOUNT, PAYMENT_DATEFROM PAYMENTWHERE TO_CHAR(PAYMENT_DATE, 'YYYY-MM-DD') BETWEEN '2007-02-07' AND '2007-02-15' 12345678910-- CAST( # AS DATE)와 TO_CHAR( # , 'YYYY-MM-DD')의 결과 확인SELECT CUSTOMER_ID, PAYMENT_ID, AMOUNT, PAYMENT_DATE, CAST(PAYMENT_DATE AS DATE), TO_CHAR(PAYMENT_DATE, 'YYYY-MM-DD') FROM PAYMENTWHERE TO_CHAR(PAYMENT_DATE, 'YYYY-MM-DD') BETWEEN '2007-02-07' AND '2007-02-15' 3. LIKE 연산자 3-1. 용도 LIKE연산자는 특정 집합에서 어떠한 컬럼의 값이 특정 값과 유사한 패턴을 갖는 집합을 출력하는 연산자이다. 3-2. LIKE 연산자 문법 1) LIKE 문법 1234567-- COLUMN_NAME 컬럼의 값이 특정 패턴과 유사한 집합을 출력SELECT *FROM TABLE_NAMEWHERE COLUMN_NAME LIKE 특정패턴 2) NOT LIKE 문법 1234567-- COLUMN_NAME 컬럼의 값이 특정 패턴과 유사하지 않은 집합을 출력SELECT *FROM TABLE_NAMEWHERE COLUMN_NAME NOT LIKE 특정패턴 3) 특정 패턴 특정 패턴에서 **%**는 어떠한 문자 혹은 문자열을 의미함 (길이가 상관없음) 특정 패턴에서 **_**는 한 개의 문자를 의미함 3-3. LIKE 연산자 실습 &gt;&gt; TABLE customer * customer_id store_id first_name email address_id activebool create_date last_update active &gt;&gt; LIKE 실습 [## LIKE '&amp;&amp;'] 절은 TURE / FALSE를 반환한다. '%'와 '_'를 이해하기 위해 다음 예를 살펴본다: SQL 결과값 설명 SELECT ‘FOO’ LIKE ‘FOO’, TRUE 'FOO’는 'FOO’이므로 참이다 ‘FOO’ LIKE ‘F%’, TRUE 'F%'는 'F’로 시작하면 모두 참이다 ‘FOO’ LIKE ‘_O_’, TRUE '_O_'는 3자리 문자열이고 가운든 문자가 'O’라면 모두 참이다 ‘BAR’ LIKE ‘B_’ TRUE '_B_'는 B로 시작하는 2자리 문자열이면 모두 참. 하지만 'BAR’는 'B’로 시작하는 3자리 문자열이다 12345678910-- FIRST_NAME이 'Jen'으로 시작하는 집합을 출력-- 즉, 'Jen'뒤에 어떤 문자 혹은 문자열이든 OKSELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERWHERE FIRST_NAME LIKE 'Jen%' 12345678910-- FIRST_NAME에 'er'이 존재하는 모든 집합을 출력-- 즉, 'er'앞과 뒤에 어떤 문자 혹은 문자열이든 OKSELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERWHERE FIRST_NAME LIKE '%er%' 123456789-- FIRST_NAME: 하나의 문자 + 'her' + 임의의 문자/문자열SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERWHERE FIRST_NAME LIKE '_her%' &gt;&gt; NOT LIKE 실습 123456789-- FIRST_NAME이 'jen'으로 시작하지 않는 집합을 출력SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERWHERE FIRST_NAME NOT LIKE 'Jen%' 4. IS NULL 연산자 4-1. 용도 IS NULL 연산자는 특정 컬럼 혹은 값이 NULL 값인지 아닌지를 판단하는 연산자이다. IS NULL 혹은 IS NOT NULL로 NULL 유무를 판단한다. 4-2. IS NULL 연산자 문법 1) IS NULL 문법 1234567-- COLUMN_NAME 컬럼의 값이 NULL인 집합을 출력SELECT *FROM TABLE_NAMEWHERE COLUMN_NAME IS NULL 2) IS NOT NULL 문법 1234567-- COLUMN_NAME 컬럼의 값이 NULL이 아닌 집합을 출력SELECT *FROM TABLE_NAMEWHERE COLUMN_NAME IS NULL 4-3. IS NULL 연산자 실습 &gt;&gt; 실습 준비 12345678910111213141516CREATE TABLE CONTACTS( ID INT GENERATED BY DEFAULT AS IDENTITY, FIRST_NAME VARCHAR(50) NOT NULL, LAST_NAME VARCHAR(50) NOT NULL, EMAIL VARCHAR(255) NOT NULL, PHONE VARCHAR(15), PRIMARY KEY(ID) )INSERT INTO CONTACTS(FIRST_NAME, LAST_NAME, EMIAL, PHONE) VALUES ('John', 'Doe', 'john.doe@example.com', NULL), ('Lily', 'Bush', 'lily.bush@example.com', '(408-234-2764)') 1234SELECT *FROM CONTACTS &gt;&gt; IS NULL 실습 1234567-- PHONE 컬럼의 값이 NULL인 집합을 출력SELECT *FROM CONTACTSWHERE PHONE IS NULL [주의] NULL은 “=” 연산으로 비교할 수 없다 123456SELECT *FROM CONTACTSWHERE PHONE = NULL &gt;&gt; IS NOT NULL 실습 1234567-- PHONE 컬럼의 값이 NULL이 아닌 집합을 출력SELECT *FROM CONTACTSWHERE PHONE IS NOT NULL document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 2. Data Filtering","slug":"【STUDY-SQL】/SQL-2-Data-Filtering","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-2-Data-Filtering/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Filtering","slug":"Filtering","permalink":"https://hyemin-kim.github.io/tags/Filtering/"}]},{"title":"SQL >> 데이터 필터링 (1)","slug":"S-SQL-Filtering-1","date":"2020-11-10T05:17:37.000Z","updated":"2020-11-10T10:54:39.845Z","comments":true,"path":"2020/11/10/S-SQL-Filtering-1/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/10/S-SQL-Filtering-1/","excerpt":"","text":"데이터 필터링 (1) 1. WHERE 절 1-1. 용도 1-2. WHERE 절 문법 1-3. WHERE 절 실습 1) 조건 한개 2) 조건 두개 2. LIMIT 절 2-1. 용도 2-2. LIMIT 절 문법 2-3. LIMIT 절 실습 3. FETCH 절 3-1. 용도 3-2. FETCH 절 문법 3-3. FETCH 절 실습 1. WHERE 절 1-1. 용도 WHERE 절은 집합을 가져올 때 어떤 집합을 가져올 것인지에 대한 조건을 설정하는 절이다. 1-2. WHERE 절 문법 1234567SELECT COLUMN_1, COLUMN_2FROM TABLE_NAMEWHERE &lt;조건&gt; -- 어떤 집합을 가져올지에 대한 조건을 준다 WHERE 절에 사용할 수 있는 연산자: 연산자 설명 = 같음 &gt; ~보다 큰 (초과) &lt; ~보다 작은 (미만) &gt;= ~보다 크거나 같은 (이상) &lt;= ~보다 작거나 같은 (이하) &lt;&gt; , != ~가 아닌 AND 그리고 OR 혹은 1-3. WHERE 절 실습 1) 조건 한개 123456789-- CUSTOMER 테이블에서 FIRST_NAME이 'Jamie'인 행의 FIRST_NAME &amp; LAST_NAME 출력SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERWHERE FIRST_NAME = 'Jamie' [주의] 문자열은 꼭 작은 따옴표( ’ ’ )로 묶어야 한다. 큰 따옴표( \" \" )는 안됨 2) 조건 두개 12345678910-- CUSTOMER 테이블에서 FIRST-NAME이 'Jamie'이면서 LAST_NAME이 'Rice'인 행을 출력SELECT LAST_NAME, FIRST_NAMEFROM CUSTOMERWHERE FIRST_NAME = 'Jamie' AND LAST_NAME = 'Rice' 1234567891011-- PAYMENT 테이블에서 AMOUNT가 1이하이거나 8이상인 행을 출력SELECT CUSTOMER_ID, AMOUNT, PAYMENT_DATEFROM PAYMENTWHERE AMOUNT &lt;= 1 OR AMOUNT &gt;= 8 2. LIMIT 절 2-1. 용도 LIMIT 절은 특정 집합을 출력 시 출력하는 행의 수를 한정하는 역할을 한다. 부분 법위 처리시 사용된다. PostgreSQL, MySQL 등에서 지원한다. 2-2. LIMIT 절 문법 123456-- 출력하는 행의 수를 지정한다SELECT * FROM TABLE_NAMELIMIT N -- 상위 N 행만 출력 123456-- 출력하는 행의 수를 지정하면서 시작위치를 지정한다SELECT *FROM TABLE_NAMELIMIT N OFFSET M -- M번째 뒤부터 출력 2-3. LIMIT 절 실습 &gt;&gt; TABLE film *film_id title discription release_year language_id rentall_duration rental_rate length replacement_cost rating last_update special_features fulltext &gt;&gt; LIMIT 123456789-- FILM_NO [1]번 부터 5건 데이터 출력SELECT FILM_ID, TITLE, RELEASE_YEARFROM FILMORDER BY FILM_IDLIMIT 5 12345678910-- RENTAL_RATE 내림차순으로 정렬 후 상위 10개 출력SELECT FILM_ID, TITLE, RENTAL_RATEFROM FILMORDER BY RENTAL_RATE DESCLIMIT 10 &gt;&gt; LIMIT + OFFSET 12345678910-- FILM_ID [4]번 부터 4건 데이터 출력SELECT FILM_ID, TITLE, RELEASE_YEARFROM FILMORDER BY FILM_IDLIMIT 4OFFSET 3 3. FETCH 절 3-1. 용도 FETCH 절은 LIMIT 절과 동일하게, 특정 집합을 출력 시 출력하는 행의 수를 한정하는 역할을 한다. 부분 법위 처리시 사용된다. 3-2. FETCH 절 문법 123456-- 출력하는 행의 수를 지정한다SELECT *FROM TABEL_NAMEFETCH FIRST [N] ROW ONLY -- N을 입력하지 않고 ROW ONLY만 입력하면 단 한 건만 출력한다. 1234567-- 출력하는 행의 수를 지정하면서 시작위치를 지정한다SELECT * FROM TABLE_NAMEOFFSET M ROWSFETCH FIRST [N] ROW ONLY 3-3. FETCH 절 실습 &gt;&gt; FETCH 12345678-- TITLE로 정렬한 집합 중에서 최초의 단 한 건의 행을 출력SELECT FILM_ID, TITLEFROM FILMORDER BY TITLEFETCH FIRST ROW ONLY 12345678-- TITLE로 정렬한 집합 중에서 최초의 10 건의 행을 출력SELECT FILM_ID, TITLEFROM FILMORDER BY TITLEFETCH FIRST 10 ROW ONLY &gt;&gt; FETCH + OFFSET 123456789-- TITLE로 정렬한 집합 중에서 6번째 행부터 5건 출력SELECT FILM_ID, TITLEFROM FILMORDER BY TITLEOFFSET 5 ROWSFETCH FIRST 5 ROWS ONLY document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 2. Data Filtering","slug":"【STUDY-SQL】/SQL-2-Data-Filtering","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-2-Data-Filtering/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Filtering","slug":"Filtering","permalink":"https://hyemin-kim.github.io/tags/Filtering/"}]},{"title":"SQL >> 데이터 조회","slug":"S-SQL-Selecting","date":"2020-11-06T05:56:21.000Z","updated":"2020-11-12T11:41:25.697Z","comments":true,"path":"2020/11/06/S-SQL-Selecting/","link":"","permalink":"https://hyemin-kim.github.io/2020/11/06/S-SQL-Selecting/","excerpt":"","text":"데이터 조회 1. SELECT 문 1-1. 용도 1-2. SELECT 문법 1-3. SELECT 문 실습 2. ORDER BY 문 2-1. 용도 2-2. ORDER BY 문법 2-3. ORDER BY 문 실습 1) 단일 기준 정렬 2) 다중 기준 N차 정렬 3. SELECT DISTINCT 문 3-1. 용도 3-2. SELECT DISTINCT 문법 1) 단일 컬럼 2) 다중 컬럼 3-3. SELECT DISTINCT 문 실습 0) 실습 준비 (데이터 생성) 1) 단일 컬럼 2) 다중 컬럼 1. SELECT 문 1-1. 용도 SELECT 문은 일반적으로 테이플에 저장된 데이터를 가져오는 데 쓰인다. SQL에서 가장 많이 쓰이는 문장이다. 1-2. SELECT 문법 123456SELECT COLUMN_1, COLUMN_2, 중략...FROM TABLE_NAME 1-3. SELECT 문 실습 &gt; 전체 컬럼을 조회 1234SELECT *FROM CUSTOMER &gt; 지정한 컬럼을 조회 123456SELECT FIRST_NAME, LAST_NAME, EMAILFROM CUSTOMER [주의] 여러 컬럼을 조회할 때, SELECT 명령어 뒤 컬럼 이름을 입력 시: 마지막 컬럼명을 제외한 모든 컬럼명 뒤에 따움표( , )를 붙여야 함 마지막 컬럼명 뒤에는 아무것도 입력하지 않는다 &gt; 테이블 Alias(별칭) 활용하기 테이블에 별칭을 지정하면 코드의 가독성이 높아진다. 특히 테이블이 많아 지면, 선택한 컬럼이 어느 테이블에서 추출한 건지를 햇갈릴 수 있다. 테이블 별칭을 활용하면 보다 쉽게 구별할 수 있다. 123456SELECT A.FIRST_NAME, A.LAST_NAME, A.EMAILFROM CUSTOMER A -- OR \"CUSTOMER AS A\" [주의] 테이블 Alias는 현재의 SELECT 문장에 대해서만 유효하다. 2. ORDER BY 문 2-1. 용도 ORDER BY 문은 SELECT 문에서 가져온 데이터를 정렬하는 데 사용한다. 업무 처리상 매우 중요한 기능이다. 2-2. ORDER BY 문법 ORDER BY를 활용하면 가져온 데이터를 특정 컬럼을 기준으로 오름차순(ASC) 혹은 내림차순(DESC)으로 정렬할 수 있다. 컬럼명 뒤에 ASC를 불이면 – 오름차순으로 정렬 컬럼명 뒤에 DESC를 불이면 – 내림차순으로 정렬 컬럼명 뒤에 아무것도 안 불이면 – Default로 오름차순으로 정렬 12345678SELECT COLUMN_1, COLUMN_2FROM TAL_NAMEORDER BY -- Default: 오름차순(ASC) COLUMN_1 ASC, -- 오름차순 정렬 COLUMN_2 DESC -- 내림차순 정렬 2-3. ORDER BY 문 실습 1) 단일 기준 정렬 단일 컬럼을 기준으로 한 번의 정렬만 실시함. &gt; ASC(오름차순) 정렬 ORDER BY 미사용 시 (미정렬) 12345SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMER “FIRST_NAME” 기준으로 오름차순 정렬 12345678-- ASC 명령어 명시SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERORDER BY FIRST_NAME ASC 12345678-- Default로 정렬SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERORDER BY FIRST_NAME **&gt; DESC(내림차순) 정렬 ** \"FIRST_NAME\"기준으로 내림차순 정렬 1234567SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERORDER BY FIRST_NAME DESC 2) 다중 기준 N차 정렬 여러 컬럼을 기준으로 N차 정렬을 실시함. COLUMN_1 기준으로 1차 정렬한 다음, COLUMN_1의 값이 동일한 데이터에 대해서 COLUMN_2 기준으로 2차 정렬을 실시한다, (위 규칙대로 계속 실행)… &gt; ASC(오름차순) + DESC(내림차순) 정렬 먼저 FIRST_NAME 오름차순으로 정렬, FIRST_NAME이 동일한 데이터는 LAST_NAME 내림차순으로 정렬 12345678SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERORDER BY FIRST_NAME DESC, -- 1차 정렬 (FIRST_NAME 내림차순) LAST_NAME ASC -- 2차 정렬 (LAST_NAME 오름차순) ORDER BY 기준을 정할 때, 컬럼명 내신에 SELECT 시 컬럼이 들어오는 순서로 대체해도 된다. (하지만 가독성을 위해 위 방법 더 추천) 12345678SELECT FIRST_NAME, LAST_NAMEFROM CUSTOMERORDER BY 1 DESC, -- 1: FIRST_NAME (내림차순) 2 ASC -- 2: LAST_NAME (오름차순) 3. SELECT DISTINCT 문 3-1. 용도 SELECT 시 DISTINCT를 사용하면 중복 값을 제외한 결과값이 출력된다. 즉 같은 결과의 행이라면 중복을 제거할 수 있다. 3-2. SELECT DISTINCT 문법 1) 단일 컬럼 단일 컬럼을 추출할 때 해당 컬럼의 값이 중복된 행을 제거하여 추출 1234-- COLUMN_1의 값이 중복 값 존재 시 중복 값을 제거SELECT DISTINCT COLUMN_1FROM TABLE_NAME 2) 다중 컬럼 다중 컬럼을 추출할 때 모든 컬럼의 값이 모두 중복 된 행을 제거하여 추출 12345-- COLUMN_1 + COLUMN_2의 값이 중복 값 존재 시 중복 값을 제거 SELECT DISTINCT COLUMN_1, COLUMN_2FROM TABLE_NAME &gt;&gt; 중복 값 제거 후 정렬하여 추출 12345678-- 결과를 명확하게 하기 위해 ORDER BY 절 사용SELECT DISTINCT COLUMN_1, COLUMN_2FROM TABLE_NAMEORDER BY COLUMN_1, -- default로 오름차순 정렬 COLUMN_2 -- default로 오름차순 정렬 다중 컬럼을 추출할 때 특정 컬럼의 값을 기준으로 중복된 행을 제거하여 추출 (DISTINCT ON 절) [제거 규칙] 기준 컬럼의 값이 동일한 행 중에서 하나의 행만 보류 ​ - 기본적으로 중복된 행 중의 첫 번째를 보류 ​ - ORDER BY 문을 사용할 경우 정렬 후의 첫 번째 행을 보류 12345SELECT DISTINCT ON (COLUMN_1) COLUMN_1, COLUMN_2FROM TABLE_NAME 12345678SELECT DISTINCT ON (COLUMN_1) COLUMN_1, COLUMN_2FROM TABLE_NAMEORDER BY COLUMN_1 COLUMN_2 3-3. SELECT DISTINCT 문 실습 0) 실습 준비 (데이터 생성) 1234567891011121314151617CREATE TABLE T1 (ID SERIAL NOT NULL PRIMARY KEY, BCOLOR VARCHAR, FCOLOR VARCHAR);INSERT INTO T1(BCOLOR, FCOLOR)VALUES ('red', 'red'), ('red', 'red'), ('red', NULL), (NULL, 'red'), ('red', 'green'), ('red', 'blue'), ('green', 'red'), ('green', 'blue'), ('green', 'green'), ('blue', 'red'), ('blue', 'green'), ('blue', 'blue'); 1234SELECT *FROM T1 1) 단일 컬럼 BCOLOR 컬럼의 값이 중복된 행을 제거 + BCOLOR 기준으로 정렬하여 추출 123456SELECT DISTINCT BCOLORFROM T1ORDER BY BCOLOR ​ 2) 다중 컬럼 BCOLOR &amp; FCOLOR 두 컬럼을 추출 시: 두 컬럼 의 값이 모두 중복된 행을 제거 BCOLOR &amp; FCOLOR 기준으로 정렬하여 추출 12345678SELECT DISTINCT BCOLOR, FCOLORFROM T1ORDER BY BCOLOR, FCOLOR ​ BCOLOR &amp; FCOLOR 두 컬럼을 추출 시: BCOLOR의 값을 기준으로 중복된 행을 제거 미정렬 시 BCOLOR값이 동일한 행 중에 첫 번째 행만 보류 BCOLOR, FCOLOR 기준으로 정렬 시 FCOLOR의 첫 번째 값을 가진 행만 보류 123456-- 미정렬 시SELECT DISTINCT ON (BCOLOR) BCOLOR, FCOLORFROM T1 ​ 123456789-- BCOLOR, FCOLOR 기준으로 정렬 (FCOLOR 오름차순)SELECT DISTINCT ON (BCOLOR) BCOLOR, FCOLORFROM T1ORDER BY BCOLOR, FCOLOR ​ 123456789-- BCOLOR, FCOLOR 기준으로 정렬 (FCOLOR 내림차순)SELECT DISTINCT ON (BCOLOR) BCOLOR, FCOLORFROM T1ORDER BY BCOLOR, FCOLOR DESC ​ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 1. Data Selecting","slug":"【STUDY-SQL】/SQL-1-Data-Selecting","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-1-Data-Selecting/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Selecting","slug":"Selecting","permalink":"https://hyemin-kim.github.io/tags/Selecting/"}]},{"title":"【실습】 Python >> Text Mining -- 감성 분류 분석 (호텔 리뷰 데이터)","slug":"E-Python-TextMining-2","date":"2020-08-29T14:01:54.000Z","updated":"2020-10-28T06:50:20.002Z","comments":true,"path":"2020/08/29/E-Python-TextMining-2/","link":"","permalink":"https://hyemin-kim.github.io/2020/08/29/E-Python-TextMining-2/","excerpt":"","text":"【Text Mining 실습】 – 호텔 리뷰 데이터: 감성 분류 &amp; 긍정/부정 키워드 분석 0. 개요 1. Library &amp; Data Import 2. 데이터셋 살펴보기 3. 한국어 텍스트 데이터 전처리 3-0. konlpy 설치 3-1. 정규 표현식 적용 3-2. 한국어 형태소 분석 - 명사 단위 3-3. 불용어 사전 3-4. Word Count 3-5. TF-IDF 적용 4. 감성 분류 – Logistic Regression 4-1. 데이터셋 생성 4-2. Training set / Test set 나누기 4-3. 모델 학습 4-4. 샘플링 재조정 4-5. 모델 재학습 5. 긍정 / 부정 키워드 분석 0. 개요 제주 호텔의 리뷰 데이터(평가 점수 + 평가 내용)을 활용해 다음 2가지 분석을 진행합니다: 리뷰속에 담긴 사람의 긍정 / 부정 감성을 파악하여 분류할 수 있는 감성 분류 예측 모델을 만든다 만든 모델을 활용해 긍정 / 부정 키워드를 출력해, 이용객들이 느낀 제주 호텔의 장,단점을 파악한다 1. Library &amp; Data Import &gt;&gt; 사용할 Library 123456789%matplotlib inlineimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.filterwarnings('ignore') &gt;&gt; 사용할 데이터셋 Tripadvisor 여행사이트에서 \"제주 호텔\"로 검색해서 나온 리뷰들을 활용합니다. (평점 &amp; 평가 내용 포함) 1df = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/tripadviser_review.csv\") 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rating text 0 4 여행에 집중할수 있게 편안한 휴식을 제공하는 호텔이었습니다. 위치선정 또한 적당한 ... 1 4 2일 이상 연박시 침대, 이불, 베게등 침구류 교체 및 어메니티 보강이 필요해 보입... 2 4 지인에소개로온 호텔 깨끗하고 좋은거같아요 처음에는 없는게 많아 많이 당황했는데 ... 3 5 방에 딱 들어서자마자 눈이 휘둥그레질정도로 이렇게 넓은 호텔 처음 와본 것 같아요!... 4 5 저녁에 맥주한잔 하는게 좋아서 렌트 안하고 뚜벅이 하기로 했는데 호텔 바로 앞에 버... &gt;&gt; Feature Description rating: 이용자 리뷰의 평가 점수 (1~5) text: 이용자 리뷰 평가 내용 2. 데이터셋 살펴보기 12# dimensiondf.shape (1001, 2) 12# 결측치df.isnull().sum() rating 0 text 0 dtype: int64 12# informationdf.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1001 entries, 0 to 1000 Data columns (total 2 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 rating 1001 non-null int64 1 text 1001 non-null object dtypes: int64(1), object(1) memory usage: 15.8+ KB 123# text 변수 확인df['text'][0] '여행에 집중할수 있게 편안한 휴식을 제공하는 호텔이었습니다. 위치선정 또한 적당한 편이었고 청소나 청결상태도 좋았습니다.' 1df['text'][100] '올 봄에 벚꽃기간에 방문, 협재를 바라보는 바다뷰가 좋고 대로변이라 렌트해서 가기도 좋음. 조식은 이용안했는데 근처 옹포밥집까지 아침 산책겸 걸어가서 하고옴. 루프탑 수영장과 바가 있었는데 내가 갔을때는 밤에 비바람이 너무 불어서 이용못하고옴 ㅠㅠ 단점으로는 모 유명 여행블로거 리뷰처럼 화장실 물떄가... 그거빼곤 다 만족' “text” 내용을 확인해보면, 소량의 \"특수 문자\"와 \"모음\"이 존재하는 경우가 있습니다. 이들은 Text Mining을 적용할 의미가 없기 때문에 정규표현식을 이용해서 제거해보도록 할게요. 3. 한국어 텍스트 데이터 전처리 기계가 텍스트 형식으로 되어 있는 리뷰 데이터를 이해하려면, 텍스트 데이터를 단어 단위로 분리하는 전처리 괴정이 필요합니다. 여기서 분리된 단어들은 Bag of Words로 Count 기반으로 나타날 수도 있고, TF-IDF를 통해서 점수로 나타날 수도 있습니다. 먼저 리뷰의 평가 내용을 단어화해서 형태소를 추출하고, 그 다음 Bag of Words를 생성하여 TF-IDF 변환을 진행하겠습니다. 3-0. konlpy 설치 영문이 아닌 한글을 처리해야 하기 때문에 \"konlpy\"이라는 library를 사용합니다. 참고 자료: 파이썬 한글 형태소 분석기 KoNLPy 설치방법 및 에러해결 KoNLPy 홈메이지 – 설치하기 [Anaconda에서 KoNLPy 설치하기] 삽질은 이제 그만~ 1!pip install konlpy==0.5.2 jpype1 Jpype1-py3 Requirement already satisfied: konlpy==0.5.2 in d:\\anaconda\\lib\\site-packages (0.5.2) Requirement already satisfied: jpype1 in d:\\anaconda\\lib\\site-packages (1.0.2) Requirement already satisfied: Jpype1-py3 in d:\\anaconda\\lib\\site-packages (0.5.5.4) Requirement already satisfied: tweepy&gt;=3.7.0 in d:\\anaconda\\lib\\site-packages (from konlpy==0.5.2) (3.9.0) Requirement already satisfied: lxml&gt;=4.1.0 in d:\\anaconda\\lib\\site-packages (from konlpy==0.5.2) (4.5.0) Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from konlpy==0.5.2) (0.4.3) Requirement already satisfied: numpy&gt;=1.6 in d:\\anaconda\\lib\\site-packages (from konlpy==0.5.2) (1.18.1) Requirement already satisfied: beautifulsoup4==4.6.0 in d:\\anaconda\\lib\\site-packages (from konlpy==0.5.2) (4.6.0) Requirement already satisfied: typing-extensions; python_version &lt; \"3.8\" in d:\\anaconda\\lib\\site-packages (from jpype1) (3.7.4.1) Requirement already satisfied: requests[socks]&gt;=2.11.1 in d:\\anaconda\\lib\\site-packages (from tweepy&gt;=3.7.0-&gt;konlpy==0.5.2) (2.23.0) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in d:\\anaconda\\lib\\site-packages (from tweepy&gt;=3.7.0-&gt;konlpy==0.5.2) (1.3.0) Requirement already satisfied: six&gt;=1.10.0 in d:\\anaconda\\lib\\site-packages (from tweepy&gt;=3.7.0-&gt;konlpy==0.5.2) (1.14.0) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests[socks]&gt;=2.11.1-&gt;tweepy&gt;=3.7.0-&gt;konlpy==0.5.2) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests[socks]&gt;=2.11.1-&gt;tweepy&gt;=3.7.0-&gt;konlpy==0.5.2) (2020.6.20) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests[socks]&gt;=2.11.1-&gt;tweepy&gt;=3.7.0-&gt;konlpy==0.5.2) (1.25.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in d:\\anaconda\\lib\\site-packages (from requests[socks]&gt;=2.11.1-&gt;tweepy&gt;=3.7.0-&gt;konlpy==0.5.2) (2.9) Requirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6; extra == \"socks\" in d:\\anaconda\\lib\\site-packages (from requests[socks]&gt;=2.11.1-&gt;tweepy&gt;=3.7.0-&gt;konlpy==0.5.2) (1.7.1) Requirement already satisfied: oauthlib&gt;=3.0.0 in d:\\anaconda\\lib\\site-packages (from requests-oauthlib&gt;=0.7.0-&gt;tweepy&gt;=3.7.0-&gt;konlpy==0.5.2) (3.1.0) 3-1. 정규 표현식 적용 12345678# 정규 표현식 함수 정의import redef apply_regular_expression(text): hangul = re.compile('[^ ㄱ-ㅣ 가-힣]') # 한글 추출 규칙: 띄어 쓰기(1 개)를 포함한 한글 result = hangul.sub('', text) # 위에 설정한 \"hangul\"규칙을 \"text\"에 적용(.sub)시킴 return result 만들어 놓은 정규 표현식을 \"text\"의 첫 행에 적용해서 결과 한번 확인해볼게요. 1df['text'][0] '여행에 집중할수 있게 편안한 휴식을 제공하는 호텔이었습니다. 위치선정 또한 적당한 편이었고 청소나 청결상태도 좋았습니다.' 1apply_regular_expression(df['text'][0]) '여행에 집중할수 있게 편안한 휴식을 제공하는 호텔이었습니다 위치선정 또한 적당한 편이었고 청소나 청결상태도 좋았습니다' 정규 표현식을 적용한 후 특수 문자가 잘 제거된 것을 확인할 수 있습니다. 3-2. 한국어 형태소 분석 - 명사 단위 &gt;&gt; 명사 형태소 추출 참고 자료: konlpy.tag 패키지 Documentation 12from konlpy.tag import Oktfrom collections import Counter 명사 형태소 추출 함수 Okt() 를 이용하여 정규표현식을 적용한 “text” 첫 행 내용의 형태소를 추출해 보겠습니다. 1apply_regular_expression(df['text'][0]) '여행에 집중할수 있게 편안한 휴식을 제공하는 호텔이었습니다 위치선정 또한 적당한 편이었고 청소나 청결상태도 좋았습니다' 123okt = Okt() # 명사 형태소 추출 함수nouns = okt.nouns(apply_regular_expression(df['text'][0]))nouns ['여행', '집중', '휴식', '제공', '호텔', '위치', '선정', '또한', '청소', '청결', '상태'] 이제 이를 전체 말뭉치(corpus)에 적용해서 명사 형태소를 추출해볼게요. 123# 말뭉치 생성corpus = \"\".join(df['text'].tolist())corpus '여행에 집중할수 있게 편안한 휴식을 제공하는 호텔이었습니다. 위치선정 또한 적당한 편이었고 청소나 청결상태도 좋았습니다.2일 이상 연박시 침대, 이불, 베게등 침구류 교체 및 어메니티 보강이 필요해 보입니다. 베스트 웨스턴 회원의 경우 객실 뷰와 층수 요청에 적극적으로 반영해 주시길 바랍니다.지인에소개로온 호텔 깨끗하고 좋은거같아요 처음에는 없는게 많아 많이 당황했는데 알고오시면 좋을거같아요 (세면도구와 잠옷은필수로챙기셔야해용) 그것만챙겨오시면 잘쉬었다가실수있답니당방에 딱 들어서자마자 눈이 휘둥그레질정도로 이렇게 넓은 호텔 처음 와본 것 같아요!! 다음에도 제주도 오면 꼭 여기서 지낼겁니다ㅎㅎ 1박만 머문다는게 너무 아쉽네요ㅠㅠ저녁에 맥주한잔 하는게 좋아서 렌트 안하고 뚜벅이 하기로 했는데 호텔 바로 앞에 버스정류이 있어서 너무 좋았습니다. 12시에 도착해서 가방 맡기려 했는데 일찍 정비된방이 있다며 바로 입실하고 룸도 업그레이드 해주셔서 직원분이 친절해 정말 좋았어요^^바다전망이라해서 기대했는데 영아니네요.. 전일 함덕대명콘도에서 1박했는데 그곳이 실내 분위기랑 바다전망이 훨 좋아요..손님이 없는 날이라고 가장 바다가 이쁘게 보이는 방으로 배치해 주셨다. 불편함에 대해 바로 대응 써비스 해주었고 조식도 사소한 부분까지 신경써서 아주 맛있었다. 특급 호텔은 아니지만 트랜디하고 즐거운 다양함에 대해 고민한 흔적이 였보인다. 여름에 꼭와서 수영장을 사용해 보고 싶다 ~ !엄마와 둘이 여행왔다가 가격대비 좋다고 하여 다녀왔어요 ㅎㅎ 듣던대로 깔끔했고 위치도 너무 좋았어요 주위 마트 식당들 동문시장도 가까워서 좋았고 앞에 바다가 있어서 더 좋았습니다 ㅎㅎ 또 방문의사 있어요 ~딸 둘과의 4일동안의 제주여행줌 2박 숙소로 정해진 제주 휘슬락 호텔~ 처음엔 공항과 가까운 곳으로만 생각했으나 시설도 넘 깨끗하고 직원분들도 모두 친절모드로 제주여행의 마지막을 넘넘 좋게 마무리하고 돌아가네요~~^^ 테라스에서 보이는 전경도 넘 멋지네요♡ 인근 동문재래시장도 가까워 야시장 이용도하고 너무너무 잘다녀왔어요. 다음에 또 이용할께요.제주여행 2일차!!! 호텔 휘슬락에 체크인 ㄱㄱㄱ. 뷰 기가막힙니다... 깨끗하고요~~ 주변에 갈데 많습니다. 여친과 잊지못할 추억 만들어봅니다^^ 좋은 가성비에 분위기 나쁘지않네요 추천 드립니다. 후회없으실듯!!!예전에 그랜드 호텔일 때 저희 아이 돌잔치를 여기 삼다정에서 했었더랬죠. 제주도에서는 아주 전통있는 호텔입니다. 그 후로 메종 글래드로 업그레이드 되었는데, 위치, 시설, 서비스에 비해 매우 합리적인 가격대의 만족스러운 호텔입니다. 그래서 저희는 명절에 제주도 내려오면 늘 글래드에 숙박해요. 특히 1층에 아티제와 백미당이 있어서, 여기 커피와 베이커리를 좋아하는 저희 가족에게는 플러스예요.지금까지 제주여행을 다니면서 여러호텔을 이용해보았지만, 메종 글래드에서 가장 만족스러웠습니다. 우선 공항의 접근성과 쇼핑의 편리함 그리고 조식의만족도가 최고이며 무엇보다 직원분들의 친절함이 기억에 남습니다. 앞으로 우리가족은 제주에 갈땐 메종을 찾기로했답니다.엄마랑 첫 제주도 여행인데 침구가 너무 좋았고, 직원분들도 친절하셔서 편안하게 쉬고 왔습니다. 교통도 편해서 짧은 여행이었지만 알차게 볼 수 있었도요. 3층에서 이틀 묵었었는데 정원 같은 곳이 바로 보여서 이뻤어요. 거기 돌아다니니까 수영장도 2개 있었는데 겨울이라 못써서 아쉬웠습니다 ㅠㅠ 여름에 꼭 여기 오려구요!! ㅎㅎㅎ 추천합니다~~친구가 제주도에 놀러와서 투숙을 하였는데 객실이 깔끔하고 뷰도 좋았어요. 야외 수영장이 보이는 방향이었는데 야간에는 조명을 켜서 너무 예쁘더라구요. 체크인하러 가는 그 순간조차도 벨맨에게 대접을 받았습니다. 왠만하면 실명을 거론하지 않는데 프론트에 문J.. 이름이 기억이 안나지만 체크인이 너무 친절해서 기분이 좋았습니다. 웃는 모습이 너무 선하고 밝아서 여행 첫날부터 스타트가 좋았네요. 다음에 투숙을 할 경우가 생긴다면 또 다시 이용할거에요~!!차를 좋아하는 아이들에게는 최고의 선물~~ 카운터에서 친절하게 안내해주셔서 편하게 이용하고 왔어요~~ 삼다정 디너 최고입니다. 기회가 되면 여름에 또 오고 싶네요~~직원분들은 눈만 마주쳐도 도와주려고 할 정도로 엄청 친절. 미니바 무료도 굿. 시설은 최근 지어진거라 당연 청결. 주변에 관광지도 가까운곳이 많음. 제주여행숙고소 강추~~ 내가 숙은 위치에서 찍은 야경뷰조식조타하여 일부러 저녁 많이 안 먹고 일찍 일어나서 동네 한바퀴 돌아주고 씻고 나서 2층가서 먹방 했네요. 신라호텔보다 퀄리티 좋음신제주에 위치한 매우 깨끗한 호텔입니다. 특히 조식을 추천 드립니다. 구제주에 오래된 호텔에 비할 수 없는 기분 좋은 호텔이였습니다.모녀 여행 중 제주시 1박을 위해 선택한 호텔입니다. 우선 공항에서 가깝고 오픈한 지 얼만 안 된 것 같아 예약했습니다. 호텔은 대로변에 있어 접근성이 좋고 (호텔은 5~10분거리), 더군다나 공항까지 셔틀버스 운행합니다. 데스크 직원분들 매우 친절하시고 주차안내분도 친절하셔서 첫 인상이 좋았습니다. 룸은 트윈으로 했는데 생각보다 큰 방에 퀸 사이즈가 2개여서 ㅋㅋㅋㅋ 대만족이었습니다. 스타일러가 있어서 여행 중 입었던 옷 다 돌렸습니다. 조식은 가짓수가 많지는 않으나 있어야 될 것은 다 있는 느낌? 근데 화장실이 조명이 너무 어둡고 내부인테리어도 어두워서 ㅋㅋㅋ 그 외의 호텔 분위기와 매우 이질적입니다. (이건 개인적인 취향인 것 같네요) 어메니티도 구비되어 있는데 향이 독특합니다. ㅋㅋㅋ 실망한 부분은 여행 시 산 과일을 먹으려고 나이프와 포크 부탁했는데 서비스가 안되더라고요..그리고 차와 커피 준비된 부분이 빈약합니다. (어느 부분은 특급호텔 표방인 것 같고 어떤 부분은 모텔인 것 같은 ) 하지만 결론적으로 가성비 훌륭, 접근성 훌륭, 청결도 훌륭해서 엄마와 함께 쾌적한 시간을 보냈습니다. 감사합니다.슈페리어킹룸에 하루 숙박한 후기입니다. 제주여행의 마지막날 숙박했는데요 지은지 얼마 안 된 느낌의 새 건물 이었습니다. 건물 안 인테리어는 약간 유럽 스타일이었구요 그림이나 조각들이 생각보다 많아서 놀랐어요 복도랑 객실도 그림이 전시되어 있고 전부 카펫으로 되어 있었습니다. 객실은 비슷한 등급의 다른 호텔들보다 큰 편이었고 침대는 두 명이서 자기에 충분할 정도로 정말 컸습니다. 비가 오는 날이라 비를 조금 맞았는데 객실 내에 LG 스타일러가 있어서 외투를 돌렸더니 뽀송뽀송해져서 완전 만족 합니다!! 진짜 스타일러 강추!!!! 그리고 무료셔틀 이용했는데 공항까지 한 7분 정도 걸렸습니당! 셔틀은 운행하는 시간이 정해져 있는데 미리 예약 하셔야 이용 하실 수 있어요~~ 다음에 제주도 온다면 또 이용하고 싶은 호텔이네요~^^시티뷰이지만 오름도 볼 수 있고 무엇보다도 교통이 훌륭하였다 다만 바로 도로가인 관계로 밤에 차량 소리가 다소 신경쓰였다급하게 방문했는데 방도 깔끔하고 직원분들 모두 친절하셔서 좋았습니다~ 1층 편의점도 있어서 좋았어요~ 칫솔은 챙겨야합니다 ㅎㅎ 재방문 의사 있습니다동계훈련을 신제주로 오게 되었는데̄̈, 훈련 하는동안 라마다호텔에서 편하게 잘 쉬었다̆̎ 갑니다̆̎! 가족과 연인과 함께 와도 좋을 것 같아요̆̈! 조식도 괜찮고 전체적으로 깔끔하고 편리하게 되어있네요̆̈ ◡̈⋆ 사진이 너무 많아 첨부하진 못했지만, 엘리베이터 가는̆̈ 곳 쪽에 빔으로 실시간 비행기 시간을 알려줘서 너무 편하고 신기했어요̆̈! [P.S. 브라우저 메모리 부족으로 부분만 출력] 12# 정규 표현식 적용apply_regular_expression(corpus) '여행에 집중할수 있게 편안한 휴식을 제공하는 호텔이었습니다 위치선정 또한 적당한 편이었고 청소나 청결상태도 좋았습니다일 이상 연박시 침대 이불 베게등 침구류 교체 및 어메니티 보강이 필요해 보입니다 베스트 웨스턴 회원의 경우 객실 뷰와 층수 요청에 적극적으로 반영해 주시길 바랍니다지인에소개로온 호텔 깨끗하고 좋은거같아요 처음에는 없는게 많아 많이 당황했는데 알고오시면 좋을거같아요 세면도구와 잠옷은필수로챙기셔야해용 그것만챙겨오시면 잘쉬었다가실수있답니당방에 딱 들어서자마자 눈이 휘둥그레질정도로 이렇게 넓은 호텔 처음 와본 것 같아요 다음에도 제주도 오면 꼭 여기서 지낼겁니다ㅎㅎ 박만 머문다는게 너무 아쉽네요ㅠㅠ저녁에 맥주한잔 하는게 좋아서 렌트 안하고 뚜벅이 하기로 했는데 호텔 바로 앞에 버스정류이 있어서 너무 좋았습니다 시에 도착해서 가방 맡기려 했는데 일찍 정비된방이 있다며 바로 입실하고 룸도 업그레이드 해주셔서 직원분이 친절해 정말 좋았어요바다전망이라해서 기대했는데 영아니네요 전일 함덕대명콘도에서 박했는데 그곳이 실내 분위기랑 바다전망이 훨 좋아요손님이 없는 날이라고 가장 바다가 이쁘게 보이는 방으로 배치해 주셨다 불편함에 대해 바로 대응 써비스 해주었고 조식도 사소한 부분까지 신경써서 아주 맛있었다 특급 호텔은 아니지만 트랜디하고 즐거운 다양함에 대해 고민한 흔적이 였보인다 여름에 꼭와서 수영장을 사용해 보고 싶다 엄마와 둘이 여행왔다가 가격대비 좋다고 하여 다녀왔어요 ㅎㅎ 듣던대로 깔끔했고 위치도 너무 좋았어요 주위 마트 식당들 동문시장도 가까워서 좋았고 앞에 바다가 있어서 더 좋았습니다 ㅎㅎ 또 방문의사 있어요 딸 둘과의 일동안의 제주여행줌 박 숙소로 정해진 제주 휘슬락 호텔 처음엔 공항과 가까운 곳으로만 생각했으나 시설도 넘 깨끗하고 직원분들도 모두 친절모드로 제주여행의 마지막을 넘넘 좋게 마무리하고 돌아가네요 테라스에서 보이는 전경도 넘 멋지네요 인근 동문재래시장도 가까워 야시장 이용도하고 너무너무 잘다녀왔어요 다음에 또 이용할께요제주여행 일차 호텔 휘슬락에 체크인 ㄱㄱㄱ 뷰 기가막힙니다 깨끗하고요 주변에 갈데 많습니다 여친과 잊지못할 추억 만들어봅니다 좋은 가성비에 분위기 나쁘지않네요 추천 드립니다 후회없으실듯예전에 그랜드 호텔일 때 저희 아이 돌잔치를 여기 삼다정에서 했었더랬죠 제주도에서는 아주 전통있는 호텔입니다 그 후로 메종 글래드로 업그레이드 되었는데 위치 시설 서비스에 비해 매우 합리적인 가격대의 만족스러운 호텔입니다 그래서 저희는 명절에 제주도 내려오면 늘 글래드에 숙박해요 특히 층에 아티제와 백미당이 있어서 여기 커피와 베이커리를 좋아하는 저희 가족에게는 플러스예요지금까지 제주여행을 다니면서 여러호텔을 이용해보았지만 메종 글래드에서 가장 만족스러웠습니다 우선 공항의 접근성과 쇼핑의 편리함 그리고 조식의만족도가 최고이며 무엇보다 직원분들의 친절함이 기억에 남습니다 앞으로 우리가족은 제주에 갈땐 메종을 찾기로했답니다엄마랑 첫 제주도 여행인데 침구가 너무 좋았고 직원분들도 친절하셔서 편안하게 쉬고 왔습니다 교통도 편해서 짧은 여행이었지만 알차게 볼 수 있었도요 층에서 이틀 묵었었는데 정원 같은 곳이 바로 보여서 이뻤어요 거기 돌아다니니까 수영장도 개 있었는데 겨울이라 못써서 아쉬웠습니다 ㅠㅠ 여름에 꼭 여기 오려구요 ㅎㅎㅎ 추천합니다친구가 제주도에 놀러와서 투숙을 하였는데 객실이 깔끔하고 뷰도 좋았어요 야외 수영장이 보이는 방향이었는데 야간에는 조명을 켜서 너무 예쁘더라구요 체크인하러 가는 그 순간조차도 벨맨에게 대접을 받았습니다 왠만하면 실명을 거론하지 않는데 프론트에 문 이름이 기억이 안나지만 체크인이 너무 친절해서 기분이 좋았습니다 웃는 모습이 너무 선하고 밝아서 여행 첫날부터 스타트가 좋았네요 다음에 투숙을 할 경우가 생긴다면 또 다시 이용할거에요차를 좋아하는 아이들에게는 최고의 선물 카운터에서 친절하게 안내해주셔서 편하게 이용하고 왔어요 삼다정 디너 최고입니다 기회가 되면 여름에 또 오고 싶네요직원분들은 눈만 마주쳐도 도와주려고 할 정도로 엄청 친절 미니바 무료도 굿 시설은 최근 지어진거라 당연 청결 주변에 관광지도 가까운곳이 많음 제주여행숙고소 강추 내가 숙은 위치에서 찍은 야경뷰조식조타하여 일부러 저녁 많이 안 먹고 일찍 일어나서 동네 한바퀴 돌아주고 씻고 나서 층가서 먹방 했네요 신라호텔보다 퀄리티 좋음신제주에 위치한 매우 깨끗한 호텔입니다 특히 조식을 추천 드립니다 구제주에 오래된 호텔에 비할 수 없는 기분 좋은 호텔이였습니다모녀 여행 중 제주시 박을 위해 선택한 호텔입니다 우선 공항에서 가깝고 오픈한 지 얼만 안 된 것 같아 예약했습니다 호텔은 대로변에 있어 접근성이 좋고 호텔은 분거리 더군다나 공항까지 셔틀버스 운행합니다 데스크 직원분들 매우 친절하시고 주차안내분도 친절하셔서 첫 인상이 좋았습니다 룸은 트윈으로 했는데 생각보다 큰 방에 퀸 사이즈가 개여서 ㅋㅋㅋㅋ 대만족이었습니다 스타일러가 있어서 여행 중 입었던 옷 다 돌렸습니다 조식은 가짓수가 많지는 않으나 있어야 될 것은 다 있는 느낌 근데 화장실이 조명이 너무 어둡고 내부인테리어도 어두워서 ㅋㅋㅋ 그 외의 호텔 분위기와 매우 이질적입니다 이건 개인적인 취향인 것 같네요 어메니티도 구비되어 있는데 향이 독특합니다 ㅋㅋㅋ 실망한 부분은 여행 시 산 과일을 먹으려고 나이프와 포크 부탁했는데 서비스가 안되더라고요그리고 차와 커피 준비된 부분이 빈약합니다 어느 부분은 특급호텔 표방인 것 같고 어떤 부분은 모텔인 것 같은 하지만 결론적으로 가성비 훌륭 접근성 훌륭 청결도 훌륭해서 엄마와 함께 쾌적한 시간을 보냈습니다 감사합니다슈페리어킹룸에 하루 숙박한 후기입니다 제주여행의 마지막날 숙박했는데요 지은지 얼마 안 된 느낌의 새 건물 이었습니다 건물 안 인테리어는 약간 유럽 스타일이었구요 그림이나 조각들이 생각보다 많아서 놀랐어요 복도랑 객실도 그림이 전시되어 있고 전부 카펫으로 되어 있었습니다 객실은 비슷한 등급의 다른 호텔들보다 큰 편이었고 침대는 두 명이서 자기에 충분할 정도로 정말 컸습니다 비가 오는 날이라 비를 조금 맞았는데 객실 내에 스타일러가 있어서 외투를 돌렸더니 뽀송뽀송해져서 완전 만족 합니다 진짜 스타일러 강추 그리고 무료셔틀 이용했는데 공항까지 한 분 정도 걸렸습니당 셔틀은 운행하는 시간이 정해져 있는데 미리 예약 하셔야 이용 하실 수 있어요 다음에 제주도 온다면 또 이용하고 싶은 호텔이네요시티뷰이지만 오름도 볼 수 있고 무엇보다도 교통이 훌륭하였다 다만 바로 도로가인 관계로 밤에 차량 소리가 다소 신경쓰였다급하게 방문했는데 방도 깔끔하고 직원분들 모두 친절하셔서 좋았습니다 층 편의점도 있어서 좋았어요 칫솔은 챙겨야합니다 ㅎㅎ 재방문 의사 있습니다동계훈련을 신제주로 오게 되었는데 훈련 하는동안 라마다호텔에서 편하게 잘 쉬었다 갑니다 가족과 연인과 함께 와도 좋을 것 같아요 조식도 괜찮고 전체적으로 깔끔하고 편리하게 되어있네요 사진이 너무 많아 첨부하진 못했지만 엘리베이터 가는 곳 쪽에 빔으로 실시간 비행기 시간을 알려줘서 너무 편하고 신기했어요 [P.S. 브라우저 메모리 부족으로 부분만 출력] 123# 전체 말뭉치(corpus)에서 명사 형태소 추출nouns = okt.nouns(apply_regular_expression(corpus))print(nouns) ['여행', '집중', '휴식', '제공', '호텔', '위치', '선정', '또한', '청소', '청결', '상태', '일', '이상', '연', '침대', '이불', '등', '침구', '류', '교체', '및', '어메니티', '보강', '베스트', '웨스턴', '회원', '경우', '객실', '뷰', '층수', '요청', '적극', '반영', '지인', '소개', '온', '호텔', '거', '처음', '당황', '세면', '도구', '잠옷', '필수', '그것', '방', '눈', '정도', '호텔', '처음', '것', '다음', '제주도', '꼭', '여기', '박만', '저녁', '맥주', '한잔', '렌트', '안', '뚜벅', '호텔', '바로', '앞', '버스', '정류', '시', '도착', '가방', '일찍', '정비', '방이', '바로', '입실', '룸', '업그레이드', '직원', '정말', '바다', '전망', '영', '전일', '함덕', '대명', '콘도', '곳', '실내', '분위기', '바다', '전망', '훨', '손님', '날', '가장', '바다', '방', '배치', '대해', '바로', '대응', '써비스', '조식', '부분', '신경', '아주', '특급', '호텔', '트랜디', '대해', '고민', '흔적', '여름', '꼭', '수영장', '사용', '보고', '엄마', '둘', '여행', '가격', '대비', '위치', '주위', '마트', '식당', '시장', '앞', '바다', '더', '또', '방문', '의사', '딸', '둘', '동안', '제주', '여행', '줌', '박', '숙소', '정해진', '제주', '휘슬', '락', '호텔', '처음', '공항', '곳', '생각', '시설', '직원', '모두', '친절', '모드', '여행', '마지막', '마무리', '테라스', '전경', '인근', '재래시장', '야시장', '이용도', '다음', '또', '이용', '제주', '여행', '일차', '호텔', '휘슬락', '체크', '뷰', '기', '주변', '여친', '추억', '가성', '비', '분위기', '추천', '후회', '예전', '그랜드', '호텔', '일', '때', '저희', '아이', '돌잔치', '여기', '다정', '했었더랬', '제주도', '아주', '전통', '호텔', '그', '후', '메종', '글래드', '업그레이드', '위치', '시설', '서비스', '매우', '합리', '가격', '대의', '호텔', '저희', '명절', '제주도', '늘', '글래드', '숙박', '층', '아티', '제', '백미', '여기', '커피', '베이커리', '저희', '가족', '플러스', '지금', '여행', '호텔', '이용', '메종', '글래드', '가장', '우선', '공항', '접근성', '쇼핑', '조식', '만족도', '최고', '무엇', '직원', '기억', '앞', '우리', '가족', '제주', '땐', '메종', '찾기', '엄마', '첫', '제주도', '여행', '침구', '직원', '쉬', '교통', '여행', '볼', '수', '도', '층', '이틀', '정원', '곳', '바로', '보', '거기', '수영장', '개', '겨울', '여름', '꼭', '여기', '추천', '친구', '제주도', '놀러와', '투숙', '객실', '뷰', '야외', '수영장', '방향', '야간', '조명', '체크', '그', '순간', '맨', '대접', '실명', '거론', '프론트', '문', '이름', '기억', '안나', '체크', '기분', '모습', '여행', '첫날', '스타트', '다음', '투숙', '경우', '또', '다시', '이용', '차', '아이', '최고', '선물', '카운터', '안내', '이용', '다정', '디너', '최고', '기회', '여름', '또', '직원', '눈', '정도', '친절', '미니바', '무료', '굿', '시설', '최근', '연', '청결', '주변', '관광지', '곳', '제주', '여행', '숙고', '소', '강추', '내', '숙', '위치', '야경', '뷰', '조식', '일부러', '저녁', '안', '일찍', '일어나서', '동네', '바퀴', '층', '먹방', '신라', '호텔', '퀄리티', '제주', '위치', '매우', '호텔', '조식', '추천', '제주', '호텔', '비', '수', '기분', '호텔', '모녀', '여행', '중', '제주시', '박', '위해', '선택', '호텔', '우선', '공항', '오픈', '얼', '안', '것', '예약', '호텔', '대로', '변', '접근성', '호텔', '분', '거리', '더군다나', '공항', '셔틀버스', '운행', '데스크', '직원', '매우', '차안', '분도', '첫', '인상', '룸', '트윈', '생각', '방', '퀸', '사이즈', '개', '만족', '스타', '여행', '중', '옷', '조식', '가짓수', '것', '느낌', '화장실', '조명', '내부', '인테리어', '그', '외', '호텔', '분위기', '매우', '질적', '이건', '개인', '취향', '것', '어메니티', '구비', '향', '부분', '여행', '시', '산', '과일', '나이프', '포크', '부탁', '서비스', '차', '커피', '준비', '부분', '부분', '특급', '호텔', '표방', '것', '부분', '모텔', '것', '결론', '가성', '비', '훌륭', '접근성', '훌륭', '청결', '엄마', '시간', '슈', '페리', '킹룸', '하루', '숙박', '후기', '여행', '마지막', '날', '숙박', '지은지', '얼마', '안', '느낌', '새', '건물', '건물', '안', '인테리어', '약간', '유럽', '스타일', '그림', '조각', '생각', '복도', '객실', '그림', '전시', '전부', '카펫', '객실', '등급', '다른', '호텔', '침대', '두', '명', '이서', '자기', '정도', '정말', '비', '날', '비', '조금', '객실', '내', '스타', '외투', '완전', '만족', '진짜', '스타', '강추', '무료', '셔틀', '이용', '공항', '분', '정도', '습', '셔틀', '운행', '시간', '미리', '예약', '이용', '수', '다음', '제주도', '또', '이용', '호텔', '시티', '뷰', '오름', '볼', '수', '무엇', '교통', '다만', '바로', '도로', '가인', '관계', '밤', '차량', '소리', '다소', '신경', '방문', '방도', '직원', '모두', '층', '편의점', '칫솔', '재', '방문', '의사', '동계', '훈련', '제주', '훈련', '동안', '라마', '호텔', '가족', '연인', '것', '조식', '전체', '사진', '첨부', '엘리베이터', '곳', '쪽', '빔', '실시간', '비행기', '시간', '공항', '택시', '미만', '택시', '비', '이동', '가능', '침구', '및', '룸', '상태', '최상', '욕실', '슬리퍼', '위생', '상태', '염려', '카펫', '염려', '설날', '조식', '떡국', '당황', '함', '떡국', '공항', '려고', '현장', '결재', '황스', '려운', '중국', '만두', '종류', '별로', '구만', '현장', '결재', '조식', '일', '메뉴', '답변', '첫', '도착', '제일', '첫', '주차', '걱정', '요', '할아버지', '안심', '객실', '컨디션', '최고', '조식', '문어', '처리', '최고', '닺', '회사', '출장', '차', '시설', '직원', '서울', '때', '이용', '다음', '또', '프론트', '방도', '간혹', '이벤트', '업그레이드', '해주시', '수영장', '및', '부대', '시설', '최고', '항상', '제주도', '때', '롯데', '롯데', '시티', '호텔', '제주', '비지니스', '호텔', '가족', '여행객', '손색', '객실', '욕조', '겸비', '사계절', '수풀', '무료', '이용', '층', '야외', '수영장', '중문', '표선등', '원거리', '여행지', '복귀', '후', '밤', '시', '수영장', '이용', '또한', '시내', '유명', '식당', '등', '방문', '아주', '일단', '공항', '분', '정도', '위치', '주차장', '지하', '층', '공간', '주차', '수', '체크', '때', '직원', '응대', '기분', '체크', '수', '다른', '문의사항', '신지', '감동', '습', '객실', '역시', '롯데', '생각', '정도', '풀', '장도', '이용', '수', '더', '옆', '락타', '룸', '샤워', '마련', '코인', '세탁실', '정말', '이용', '오션', '뷰', '멀리', '바다', '야경', '남자친구', '첫', '여행', '덕분', '여행', '습', '다음', '제주', '꼭', '방문', '이번', '제주도', '여행', '때', '롯데', '시티', '제주', '박', '숙박', '체크', '때', '프런트', '직원', '객실', '배정', '공항', '비행기', '이착륙', '객실', '객실'] [P.S. 브라우저 메모리 부족으로 부분만 출력] 12# 빈도 탐색counter = Counter(nouns) 1counter.most_common(10) [('호텔', 803), ('수', 498), ('것', 436), ('방', 330), ('위치', 328), ('우리', 327), ('곳', 320), ('공항', 307), ('직원', 267), ('매우', 264)] &gt;&gt; 한글자 명사 제거 위 결과에서 보이듯이, 두 글자 키워드가 대부분 의미 있는 단어지만, ‘수’, ‘것’, '곳’과 같은 한 글자 키워드는 분석에 딱히 좋은 영향을 미치지 않은 것으로 보입니다. 그래서 우리는 한글자 명사를 제거해보도록 하겠습니다. 12available_counter = Counter({x: counter[x] for x in counter if len(x) &gt; 1})available_counter.most_common(10) [('호텔', 803), ('위치', 328), ('우리', 327), ('공항', 307), ('직원', 267), ('매우', 264), ('가격', 245), ('객실', 244), ('시설', 215), ('제주', 192)] 이제 한글자 키워드 모두 제거됐습니다. 하지만 “우리”, “매우” 와 같은 실질적인 의미가 없고 꾸민 역할을 하는 불용어들 아직 존재합니다. 한국어 불용어 사전을 정의하여 불용어도 제거해줄게요. 3-3. 불용어 사전 RANKS NL에 제공해주는 한국어 불용어 사전을 활용하겠습니다. 12stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()stopwords[:10] [['휴'], ['아이구'], ['아이쿠'], ['아이고'], ['어'], ['나'], ['우리'], ['저희'], ['따라'], ['의해']] 이 외에도 우리가 분석하고자 하는 데이터셋에 특화된 불용어들이 있습니다. 예를 들면: “제주”, “호텔”, “숙소” 등. 이런 단어들도 불용어 사전에 추가해보도록 할게요. 123jeju_hotel_stopwords = ['제주', '제주도', '호텔', '리뷰', '숙소', '여행', '트립']for word in jeju_hotel_stopwords: stopwords.append(word) 3-4. Word Count &gt;&gt; BoW 벡터 생성 123456789101112131415from sklearn.feature_extraction.text import CountVectorizerdef text_cleaning(text): hangul = re.compile('[^ ㄱ-ㅣ 가-힣]') # 정규 표현식 처리 result = hangul.sub('', text) okt = Okt() # 형태소 추출 nouns = okt.nouns(result) nouns = [x for x in nouns if len(x) &gt; 1] # 한글자 키워드 제거 nouns = [x for x in nouns if x not in stopwords] # 불용어 제거 return nounsvect = CountVectorizer(tokenizer = lambda x: text_cleaning(x))bow_vect = vect.fit_transform(df['text'].tolist())word_list = vect.get_feature_names()count_list = bow_vect.toarray().sum(axis=0) 12# 단어 리스트word_list ['가가', '가게', '가격', '가격표', '가구', '가급', '가기', '가까이', '가끔', '가능', '가도', '가동', '가두', '가득', '가든', '가라', '가량', '가려움', '가로', '가면', '가몬', '가무', '가물', '가미', '가방', '가버', '가성', '가세', '가스레인지', '가스렌지', '가슴', '가시', '가신', '가야', '가옥', '가요', '가용', '가운데', '가을', '가인', '가장', '가정', '가정식', '가족', '가지', '가짓수', '가차', '가치', '가품', '각각', '각오', '각자', '각종', '각층', '간격', '간곳', '간다', '간단', '간만', '간식', '간이', '간주', '간직', '간판', '간혹', '갈껄', '갈비', '갈비탕', '갈수', '갈수록', '감각', '감동', '감명', '감사', '감상', '감소', '감안', '감자', '감히', '갑인', '갑자기', '갑작스레', '강남', '강력', '강아지', '강압', '강제', '강조', '강추', '개념', '개략', '개미', '개발', '개방', '개별', '개보', '개뿔', '개선', '개수대', '개월', '개인', '개인실', '개인정보', '개조', '개층', '객수', '객실', '갤러리', '갱스터', '거기', '거나', '거두', '거론', '거르세', '거름', '거리', '거린데', '거림', '거문도', '거미', '거부', '거실', '거여', '거울', '거위', '거의', '거절', '거주', '거지', '거참', '거품', '걱정', '건가', '건강', '건너', '건너편', '건물', '건의', '건조', '건조기', '건조대', '건축', '걷기', '걸음', '걸이', '걸즈', '검사', '검색', '검정색', '검토', '것임', '겉보기', '게다가', '게스트', '게스트하우스', '게임', '게재', '겐찮은듯', '겔상', '겨우', '겨울', '겨울철', '격인', '격하', '결과', '결론', '결석', '결재', '결정', '결제', '결코', '결함', '결항', '결혼', '결혼식', '겸비', '겸용', '겹겹', '경고', '경관', '경내', '경로', '경매', '경영', '경영학', '경우', '경쟁', '경쟁력', '경찰', '경치', '경험', '계단', '계란', '계란후라이', '계산', '계속', '계정', '계획', '고가', '고간', '고객', '고급', '고기', '고기국수', '고깃배', '고내포구', '고려', '고루', '고무줄', '고문', '고민', '고봉', '고분', '고생', '고속', '고속도로', '고아', '고양이', '고여', '고오', '고요', '고유', '고작', '고장', '고정', '고층', '고통', '고트', '고함', '고해', '곡부', '곧바로', '곧장', '골드스타', '골목', '골목길', '골퍼', '골프', '골프장', '골프텔', '곰팡이', '곱슬', '곳곳', '곳곳이', '곳도', '곳임', '공간', '공감', '공개', '공공', '공공장소', '공급', '공기', '공덕', '공률', '공물', '공사', '공시', '공실이', '공연', '공연장', '공영', '공용', '공원', '공유', '공짜', '공차', '공터', '공포', '공항', '과거', '과물', '과언', '과일', '과장', '관경', '관계', '관계자', '관광', '관광객', '관광명소', '관광지', '관덕정', '관련', '관리', '관리인', '관리자', '관리직', '관음사', '관해', '광경', '광고', '광천수', '괴체', '교대', '교수', '교외', '교욱받', '교육', '교체', '교통', '교환', '교회', '구가', '구경', '구경만', '구관', '구글', '구나', '구내', '구덩이', '구도', '구두', '구둣주걱', '구들장', '구류', '구만', '구매', '구멍', '구별', '구분', '구비', '구사', '구색', '구석', '구석구석', '구성', '구식', '구암', '구역', '구역질', '구이', '구입', '구조', '구축', '국가', '국내', '국도', '국립', '국수', '국적', '국제', '국제공항', '군더더기', '군데', '군데군데', '굳럭', '굳이', '굿굿', '굿굿굿', '굿앤굿', '굿임', '권내', '권장', '권한', '귀중', '규모', '규율', '규칙', '균형', '그거', '그것', '그게', '그냥', '그네', '그녀', '그다음', '그다지', '그닥', '그대로', '그동안', '그때', '그랜드', '그레이스', '그로', '그룹', '그릇', '그린', '그림', '극복', '극악', '근래', '근무', '근본', '근육통', '근처', '근해', '글래드', '글쎄', '금고', '금늘', '금능', '금릉', '금방', '금속', '금액', '금연', '금요일', '금은', '금지', '금토일', '급상승', '급속', '기간', '기계', '기구', '기기', '기념일', '기능', '기대', '기도', '기류', '기리', '기반', '기본', '기부', '기분', '기사', '기상', '기소', '기숙사', '기술', '기술자', '기억', '기업', '기여', '기용', '기우', '기입', '기적', '기전', '기점', '기존', '기준', '기지', '기타', '기프트샵', '기호', '기회', '기후', '긴장', '길가', '길림', '길목', '길이', '김녕', '김녕해변', '김밥', '김씨', '김치', '김포공항', '까페', '깜빡', '깜짝', '깨끗', '깨끗깔끔', '께빵', '꼭대기', '꽃꺽으러', '꽃사슴', '꾸러미', '꾸밈', '꿀잠', '끝내기', '끼리', '나기', '나누기', '나니', '나라', '나름', '나머지', '나머진', '나무', '나물', '나보', '나오니', '나우', '나은', '나이', '나이트', '나이프', '나중', '나탈리', '낙후', '낚시', '난로', '난리', '난방', '난입', '난타', '날수', '날씨', '날짜', '남녀', '남성', '남아', '남자', '남자친구', '남짓', '남쪽', '남편', '낭만', '내겐', '내내', '내년', '내부', '내부시', '내시', '내야', '내외', '내용', '내의', '내인', '내일', '냄비', '냄새', '냉동', '냉장고', '너븐팡', '넓이', '네스프레소', '네이버', '년대', '년전', '녔던', '노곤', '노래', '노래방', '노력', '노리', '노블레스', '노선', '노을', '노크', '노트북', '노화', '노후', '녹물', '녹음', '녹지', '논평', '놀러와', '놀수', '놀이', '놀이기구', '놀이터', '농부가', '농장', '높이', '놨더군', '누가', '누구', '누군가', '누락', '누리', '누울', '눈앞', '뉴타운', '느낌', '는걸', '늘송', '능리', '다가', '다그', '다다미', '다라', '다락방', '다른', '다른사람', '다리미', '다만', '다미', '다발', '다섯', '다소', '다수', '다시', '다운', '다음', '다이지', '다인', '다정', '다행', '단계', '단기', '단면', '단어', '단위', '단점', '단정', '단지', '단체', '달걀', '달걀프라이', '달라', '달러', '달리', '달성', '닭머르', '담당', '담배', '담소', '담요', '답변', '당구', '당근', '당나귀', '당분간', '당시', '당신', '당일', '당황', '대가', '대가족', '대고', '대관령', '대답', '대당', '대도', '대도시', '대뜸', '대략', '대로', '대리', '대명', '대박', '대부분', '대비', '대상', '대신', '대안', '대여', '대요', '대욕', '대응', '대의', '대입', '대적', '대접', '대정', '대중', '대중교통', '대처', '대체', '대충', '대포', '대표', '대하', '대한', '대한민국', '대한항공', '대해', '대행', '대형', '대화', '대환영', '댐핑할', '더군다나', '더더', '더러', '더블', '더블베드', '더욱', '더원', '덕림사', '덕분', '덕택', '던데', '덮어놓고', '데리', '데스크', '데스크톱', '데이', '데이즈', '델문', '도구', '도달', '도대체', '도도', '도둑', '도로', '도록', '도리어', '도미', '도보', '도서관', '도시', '도시락', '도심', '도심지', '도어', '도어락', '도움', '도움말', '도일', '도정', '도중', '도착', '도처', '도청', '도쿄', '도크', '독립', '독서', '독점', '독채', '돈까스', '돌담', '돌잔치', '동계', '동광양', '동굴', '동남', '동남아', '동네', '동도', '동료', '동문', '동물', '동물원', '동반', '동부', '동북', '동생', '동선', '동시', '동안', '동영상', '동의', '동이', '동인', '동작', '동전', '동정', '동쪽', '돼지', '돼지고기', '됏다', '될껀', '될껄', '두루', '두번째', '두봉', '두부', '두엄', '두운', '두툼', '둘러보기', '둘이서', '둘째', '둥근지붕', '뒤쪽', '뒤척', '뒷골목', '뒷마당', '뒷문', '뒷쪽', '드네', '드라이기', '드라이버', '드라이브', '드라이어', '드롭', '드릴', '드타', '드하', '득시', '듭니', '듯이', '듯해', '등급', '등대', '등등', '등반', '등산', '등정후', '디귿', '디너', '디럭스', '디봇', '디셈버', '디자이너', '디자인', '디저트', '디제이', '따라서', '때로는', '때문', '떡국', '또오', '또한', '뚜벅', '뜨근뜨근', '뜨내기', '라그', '라마', '라며', '라면', '라서', '라스베가스', '라우터', '라운지', '라이센스', '라커룸', '락스', '락심이', '락커', '락타', '란딩', '랍니', '랜드', '랜트', '랜트카', '랜트하', '램프', '러닝', '러브', '럭셔리', '런가', '렀는데', '렀습니', '렀으', '레노', '레드', '레벨', '레비', '레스토랑', '레시', '레오', '레이', '레이크', '레인지', '레저', '레프트', '렌즈', '렌탈업체', '렌터', '렌터카', '렌트', '렌트카', '려고', '려운', '로고', '로그', '로만', '로맨틱', '로부터', '로비', '로서', '로션', '로얄', '로움', '로컬', '로터리', '로프트', '롯데', '롯데리아', '롱보드', '루온토', '루트', '루프', '룸메이트', '룸바닥', '룸상태', '룸서비스', '룸안', '룸키', '룸타입', '를위', '리가', '리기', '리넨', '리뉴', '리뉴얼', '리더', '리도', '리모콘', '리베라', '리베로', '리빙룸', '리셉션', '리움', '리젠시', '리조트', '리지', '리치', '리트', '리플렛', '린스', '링잉', '마누카꿀', '마늘', '마다', '마담', '마당', '마레', '마련', '마루', '마리', '마모', '마무리', '마사지', '마술', '마스코트', '마스크', '마스터', '마시기', '마안', '마운트', '마을', '마음', '마이너스', '마인드', '마일리지', '마자', '마저', '마주', '마지막', '마지막여행', '마차', '마찬가지', '마치', '마침내', '마켓', '마트', '마틸다', '막걸리', '만끽', '만난', '만날', '만남', '만다린', '만두', '만들기', '만료', '만약', '만요', '만원', '만점', '만족', '만족도', '만천원', '만큼', '만하', '만해', '만화책', '말레이시아', '말레이시아인', '말로', '말리', '말씀', '말투', '말함', '맘스', '맛사지', '맛잇엇어', '맛집', '망각', '망신', '망치', '맞은편', '맞이', '매년', '매니', '매니저', '매달', '매듭', '매력', '매번', '매우', '매운탕', '매일', '매장', '매점', '매칭', '매트', '매트리스', '매트릭스', '매하', '맥도날드', '맥도널드', '맥주', '맥주잔', '맨발', '머리', '머리카락', '머신', '머싱', '먹거리', '먹기', '먹방', '먹이', '먼저', '먼지', '멀리', '메가박스', '메뉴', '메리', '메리어트', '메시지', ...] 12# 각 단어가 전체 리뷰중에 등장한 총 횟수count_list array([ 4, 8, 245, ..., 1, 7, 14], dtype=int64) 12# 각 단어의 리뷰별 등장 횟수bow_vect.toarray() array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 2, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=int64) 1bow_vect.shape (1001, 3599) 1234# \"단어\" - \"총 등장 횟수\" Matchingword_count_dict = dict(zip(word_list, count_list))word_count_dict {'가가': 4, '가게': 8, '가격': 245, '가격표': 1, '가구': 8, '가급': 1, '가기': 20, '가까이': 20, '가끔': 5, '가능': 10, '가도': 7, '가동': 2, '가두': 1, '가득': 2, '가든': 1, '가라': 3, '가량': 1, '가려움': 1, '가로': 2, '가면': 14, '가몬': 1, '가무': 1, '가물': 1, '가미': 1, '가방': 4, '가버': 1, '가성': 49, '가세': 3, '가스레인지': 1, '가스렌지': 1, '가슴': 1, '가시': 4, '가신': 3, '가야': 10, '가옥': 1, '가요': 5, '가용': 1, '가운데': 3, '가을': 4, '가인': 1, '가장': 42, '가정': 4, '가정식': 2, '가족': 94, '가지': 55, '가짓수': 3, '가차': 1, '가치': 15, '가품': 1, '각각': 7, '각오': 1, '각자': 2, '각종': 3, '각층': 1, '간격': 2, '간곳': 1, '간다': 4, '간단': 1, '간만': 1, '간식': 5, '간이': 3, '간주': 1, '간직': 1, '간판': 2, '간혹': 1, '갈껄': 1, '갈비': 1, '갈비탕': 1, '갈수': 7, '갈수록': 1, '감각': 1, '감동': 12, '감명': 1, '감사': 6, '감상': 3, '감소': 1, '감안': 5, '감자': 1, '감히': 1, '갑인': 1, '갑자기': 4, '갑작스레': 1, '강남': 1, '강력': 9, '강아지': 7, '강압': 2, '강제': 1, '강조': 1, '강추': 8, '개념': 1, '개략': 1, '개미': 1, '개발': 3, '개방': 2, '개별': 3, '개보': 1, '개뿔': 1, '개선': 4, '개수대': 1, '개월': 1, '개인': 23, '개인실': 1, '개인정보': 2, '개조': 5, '개층': 1, '객수': 1, '객실': 244, '갤러리': 2, '갱스터': 1, '거기': 24, '거나': 6, '거두': 1, '거론': 1, '거르세': 1, '거름': 2, '거리': 156, '거린데': 1, '거림': 1, '거문도': 1, '거미': 1, '거부': 4, '거실': 29, '거여': 1, '거울': 5, '거위': 1, '거의': 27, '거절': 3, '거주': 1, '거지': 1, '거참': 1, '거품': 2, '걱정': 27, '건가': 1, '건강': 2, '건너': 8, '건너편': 11, '건물': 55, '건의': 1, '건조': 2, '건조기': 3, '건조대': 2, '건축': 2, '걷기': 2, '걸음': 3, '걸이': 2, '걸즈': 1, '검사': 1, '검색': 13, '검정색': 1, '검토': 3, '것임': 3, '겉보기': 2, '게다가': 5, '게스트': 25, '게스트하우스': 30, '게임': 2, '게재': 1, '겐찮은듯': 1, '겔상': 1, '겨우': 3, '겨울': 15, '겨울철': 2, '격인': 1, '격하': 1, '결과': 2, '결론': 3, '결석': 1, '결재': 2, '결정': 12, '결제': 1, '결코': 2, '결함': 1, '결항': 2, '결혼': 1, '결혼식': 2, '겸비': 1, '겸용': 1, '겹겹': 2, '경고': 1, '경관': 3, '경내': 1, '경로': 1, '경매': 1, '경영': 2, '경영학': 1, '경우': 41, '경쟁': 1, '경쟁력': 2, '경찰': 2, '경치': 17, '경험': 26, '계단': 4, '계란': 11, '계란후라이': 1, '계산': 2, '계속': 23, '계정': 1, '계획': 13, '고가': 1, '고간': 1, '고객': 14, '고급': 8, '고기': 8, '고기국수': 1, '고깃배': 1, '고내포구': 1, '고려': 9, '고루': 1, '고무줄': 1, '고문': 2, '고민': 9, '고봉': 1, '고분': 2, '고생': 1, '고속': 2, '고속도로': 2, '고아': 1, '고양이': 3, '고여': 1, '고오': 1, '고요': 3, '고유': 2, '고작': 1, '고장': 3, '고정': 3, '고층': 2, '고통': 1, '고트': 1, '고함': 2, '고해': 1, '곡부': 1, '곧바로': 2, '곧장': 2, '골드스타': 1, '골목': 6, '골목길': 2, '골퍼': 2, '골프': 9, '골프장': 5, '골프텔': 2, '곰팡이': 14, '곱슬': 1, '곳곳': 4, '곳곳이': 1, '곳도': 8, '곳임': 2, '공간': 73, '공감': 1, '공개': 1, '공공': 2, '공공장소': 1, '공급': 2, '공기': 8, '공덕': 1, '공률': 1, '공물': 1, '공사': 12, '공시': 1, '공실이': 1, '공연': 8, '공연장': 2, '공영': 1, '공용': 8, '공원': 17, '공유': 5, '공짜': 1, '공차': 1, '공터': 1, '공포': 1, '공항': 307, '과거': 1, '과물': 2, '과언': 1, '과일': 9, '과장': 2, '관경': 1, '관계': 3, '관계자': 2, '관광': 38, '관광객': 15, '관광명소': 4, '관광지': 12, '관덕정': 4, '관련': 6, '관리': 39, '관리인': 1, '관리자': 3, '관리직': 2, '관음사': 1, '관해': 5, '광경': 2, '광고': 4, '광천수': 1, '괴체': 1, '교대': 1, '교수': 1, '교외': 1, '교욱받': 1, '교육': 5, '교체': 7, '교통': 30, '교환': 2, '교회': 2, '구가': 3, '구경': 7, '구경만': 1, '구관': 4, '구글': 2, '구나': 2, '구내': 1, '구덩이': 1, '구도': 1, '구두': 2, '구둣주걱': 1, '구들장': 1, '구류': 1, '구만': 2, '구매': 14, '구멍': 7, '구별': 1, '구분': 3, '구비': 11, '구사': 6, '구색': 2, '구석': 2, '구석구석': 5, '구성': 7, '구식': 1, '구암': 1, '구역': 3, '구역질': 2, '구이': 1, '구입': 5, '구조': 12, '구축': 1, '국가': 3, '국내': 1, '국도': 1, '국립': 1, '국수': 3, '국적': 3, '국제': 11, '국제공항': 1, '군더더기': 1, '군데': 8, '군데군데': 2, '굳럭': 1, '굳이': 7, '굿굿': 1, '굿굿굿': 1, '굿앤굿': 1, '굿임': 1, '권내': 1, '권장': 5, '권한': 2, '귀중': 1, '규모': 12, '규율': 2, '규칙': 1, '균형': 1, '그거': 3, '그것': 70, '그게': 1, '그냥': 42, '그네': 1, '그녀': 20, '그다음': 1, '그다지': 4, '그닥': 4, '그대로': 11, '그동안': 4, '그때': 3, '그랜드': 6, '그레이스': 3, '그로': 3, '그룹': 9, '그릇': 3, '그린': 1, '그림': 4, '극복': 1, '극악': 1, '근래': 1, '근무': 4, '근본': 1, '근육통': 1, '근처': 164, '근해': 1, '글래드': 3, '글쎄': 2, '금고': 2, '금늘': 1, '금능': 2, '금릉': 1, '금방': 3, '금속': 1, '금액': 8, '금연': 6, '금요일': 1, '금은': 1, '금지': 1, '금토일': 1, '급상승': 1, '급속': 1, '기간': 3, '기계': 4, '기구': 2, '기기': 4, '기념일': 1, '기능': 4, '기대': 15, '기도': 7, '기류': 3, '기리': 1, '기반': 4, '기본': 45, '기부': 1, '기분': 29, '기사': 8, '기상': 1, '기소': 1, '기숙사': 7, '기술': 3, '기술자': 1, '기억': 11, '기업': 2, '기여': 1, '기용': 1, '기우': 1, '기입': 1, '기적': 1, '기전': 1, '기점': 1, '기존': 1, '기준': 4, '기지': 1, '기타': 5, '기프트샵': 2, '기호': 1, '기회': 11, '기후': 1, '긴장': 1, '길가': 4, '길림': 1, '길목': 2, '길이': 2, '김녕': 1, '김녕해변': 1, '김밥': 1, '김씨': 1, '김치': 4, '김포공항': 1, '까페': 5, '깜빡': 1, '깜짝': 3, '깨끗': 5, '깨끗깔끔': 1, '께빵': 1, '꼭대기': 2, '꽃꺽으러': 1, '꽃사슴': 1, '꾸러미': 1, '꾸밈': 1, '꿀잠': 2, '끝내기': 1, '끼리': 18, '나기': 2, '나누기': 6, '나니': 1, '나라': 2, '나름': 13, '나머지': 6, '나머진': 1, '나무': 13, '나물': 1, '나보': 1, '나오니': 2, '나우': 1, '나은': 5, '나이': 3, '나이트': 2, '나이프': 2, '나중': 8, '나탈리': 2, '낙후': 3, '낚시': 3, '난로': 3, '난리': 3, '난방': 30, '난입': 2, '난타': 9, '날수': 1, '날씨': 12, '날짜': 1, '남녀': 1, '남성': 2, '남아': 5, '남자': 6, '남자친구': 2, '남짓': 1, '남쪽': 1, '남편': 10, '낭만': 2, '내겐': 1, '내내': 8, '내년': 1, '내부': 40, '내부시': 1, '내시': 1, '내야': 1, '내외': 2, '내용': 2, '내의': 2, '내인': 1, '내일': 2, '냄비': 1, '냄새': 58, '냉동': 1, '냉장고': 35, '너븐팡': 2, '넓이': 1, '네스프레소': 1, '네이버': 3, '년대': 2, '년전': 1, '녔던': 1, '노곤': 2, '노래': 1, '노래방': 3, '노력': 8, '노리': 1, '노블레스': 1, '노선': 2, '노을': 1, '노크': 1, '노트북': 2, '노화': 1, '노후': 6, '녹물': 1, '녹음': 4, '녹지': 1, '논평': 1, '놀러와': 2, '놀수': 1, '놀이': 3, '놀이기구': 2, '놀이터': 2, '농부가': 1, '농장': 3, '높이': 2, '놨더군': 1, '누가': 5, '누구': 5, '누군가': 4, '누락': 1, '누리': 1, '누울': 2, '눈앞': 3, '뉴타운': 1, '느낌': 49, '는걸': 2, '늘송': 3, '능리': 1, '다가': 1, '다그': 1, '다다미': 1, '다라': 1, '다락방': 1, '다른': 88, '다른사람': 1, '다리미': 2, '다만': 54, '다미': 1, '다발': 1, '다섯': 1, '다소': 21, '다수': 2, '다시': 93, '다운': 4, '다음': 102, '다이지': 1, '다인': 1, '다정': 2, '다행': 3, '단계': 4, '단기': 1, '단면': 1, '단어': 2, '단위': 2, '단점': 40, '단정': 1, '단지': 16, '단체': 19, '달걀': 3, '달걀프라이': 1, '달라': 13, '달러': 7, '달리': 6, '달성': 1, '닭머르': 1, '담당': 2, '담배': 19, '담소': 2, '담요': 1, '답변': 3, '당구': 2, '당근': 2, '당나귀': 2, '당분간': 1, '당시': 1, '당신': 21, '당일': 3, '당황': 7, '대가': 3, '대가족': 2, '대고': 1, '대관령': 1, '대답': 3, '대당': 1, '대도': 3, '대도시': 2, '대뜸': 1, '대략': 6, '대로': 8, '대리': 3, '대명': 1, '대박': 3, '대부분': 23, '대비': 64, '대상': 1, '대신': 8, '대안': 2, '대여': 3, '대요': 2, '대욕': 1, '대응': 2, '대의': 4, '대입': 1, '대적': 1, '대접': 1, '대정': 1, '대중': 9, '대중교통': 6, '대처': 2, '대체': 2, '대충': 3, '대포': 1, '대표': 4, '대하': 1, '대한': 19, '대한민국': 2, '대한항공': 1, '대해': 21, '대행': 1, '대형': 10, '대화': 11, '대환영': 1, '댐핑할': 1, '더군다나': 1, '더더': 2, '더러': 1, '더블': 29, '더블베드': 4, '더욱': 5, '더원': 1, '덕림사': 1, '덕분': 6, '덕택': 3, '던데': 1, '덮어놓고': 1, '데리': 5, '데스크': 30, '데스크톱': 1, '데이': 1, '데이즈': 1, '델문': 2, '도구': 18, '도달': 3, '도대체': 1, '도도': 1, '도둑': 1, '도로': 41, '도록': 1, '도리어': 1, '도미': 9, '도보': 35, '도서관': 1, '도시': 18, '도시락': 4, '도심': 14, '도심지': 1, '도어': 3, '도어락': 1, '도움': 51, '도움말': 1, '도일': 1, '도정': 1, '도중': 2, '도착': 69, '도처': 1, '도청': 2, '도쿄': 1, '도크': 1, '독립': 6, '독서': 1, '독점': 1, '독채': 5, '돈까스': 1, '돌담': 1, '돌잔치': 1, '동계': 1, '동광양': 1, '동굴': 1, '동남': 1, '동남아': 2, '동네': 7, '동도': 1, '동료': 2, '동문': 14, '동물': 9, '동물원': 2, '동반': 3, '동부': 2, '동북': 1, '동생': 3, '동선': 3, '동시': 7, '동안': 48, '동영상': 1, '동의': 3, '동이': 1, '동인': 2, '동작': 1, '동전': 1, '동정': 1, '동쪽': 5, '돼지': 16, '돼지고기': 4, '됏다': 1, '될껀': 1, '될껄': 1, '두루': 2, '두번째': 2, '두봉': 2, '두부': 1, '두엄': 1, '두운': 2, '두툼': 1, '둘러보기': 1, '둘이서': 3, '둘째': 5, '둥근지붕': 1, '뒤쪽': 4, '뒤척': 1, '뒷골목': 1, '뒷마당': 1, '뒷문': 1, '뒷쪽': 2, '드네': 1, '드라이기': 7, '드라이버': 1, '드라이브': 11, '드라이어': 11, '드롭': 1, '드릴': 1, '드타': 1, '드하': 2, '득시': 1, '듭니': 5, '듯이': 1, '듯해': 1, '등급': 3, '등대': 3, '등등': 8, '등반': 3, '등산': 6, '등정후': 1, '디귿': 1, '디너': 4, '디럭스': 6, '디봇': 1, '디셈버': 2, '디자이너': 1, '디자인': 11, '디저트': 1, '디제이': 2, '따라서': 4, '때로는': 1, '때문': 112, '떡국': 2, '또오': 1, '또한': 76, '뚜벅': 3, '뜨근뜨근': 1, '뜨내기': 1, '라그': 1, '라마': 4, '라며': 3, '라면': 15, '라서': 1, '라스베가스': 1, '라우터': 1, '라운지': 9, '라이센스': 1, '라커룸': 1, '락스': 2, '락심이': 1, '락커': 2, '락타': 1, '란딩': 1, '랍니': 1, '랜드': 1, '랜트': 1, '랜트카': 1, '랜트하': 1, '램프': 2, '러닝': 1, '러브': 3, '럭셔리': 5, '런가': 2, '렀는데': 1, '렀습니': 2, '렀으': 1, '레노': 1, '레드': 1, '레벨': 1, '레비': 1, '레스토랑': 64, '레시': 1, '레오': 2, '레이': 1, '레이크': 1, '레인지': 3, '레저': 1, '레프트': 1, '렌즈': 1, '렌탈업체': 1, '렌터': 1, '렌터카': 4, '렌트': 17, '렌트카': 8, '려고': 4, '려운': 1, '로고': 1, '로그': 3, '로만': 1, '로맨틱': 2, '로부터': 2, '로비': 49, '로서': 2, '로션': 1, '로얄': 1, '로움': 1, '로컬': 3, '로터리': 1, '로프트': 1, '롯데': 6, '롯데리아': 2, '롱보드': 1, '루온토': 1, '루트': 1, '루프': 17, '룸메이트': 1, '룸바닥': 1, '룸상태': 2, '룸서비스': 9, '룸안': 1, '룸키': 2, '룸타입': 1, '를위': 1, '리가': 2, '리기': 1, '리넨': 1, '리뉴': 1, '리뉴얼': 1, '리더': 1, '리도': 1, '리모콘': 3, '리베라': 2, '리베로': 1, '리빙룸': 2, '리셉션': 29, '리움': 2, '리젠시': 1, '리조트': 53, '리지': 1, '리치': 1, '리트': 1, '리플렛': 1, '린스': 2, '링잉': 1, '마누카꿀': 1, '마늘': 1, '마다': 1, '마담': 2, '마당': 2, '마레': 2, '마련': 7, '마루': 5, '마리': 11, '마모': 1, '마무리': 3, '마사지': 4, '마술': 1, '마스코트': 2, '마스크': 1, '마스터': 2, '마시기': 2, '마안': 1, '마운트': 1, '마을': 9, '마음': 31, '마이너스': 1, '마인드': 4, '마일리지': 2, '마자': 2, '마저': 1, '마주': 4, '마지막': 21, '마지막여행': 1, '마차': 1, '마찬가지': 4, '마치': 12, '마침내': 3, '마켓': 9, '마트': 14, '마틸다': 2, '막걸리': 1, '만끽': 1, '만난': 1, '만날': 1, '만남': 1, '만다린': 2, '만두': 1, '만들기': 1, '만료': 1, '만약': 6, '만요': 1, '만원': 20, '만점': 1, '만족': 12, '만족도': 1, '만천원': 1, '만큼': 2, '만하': 2, '만해': 2, '만화책': 1, '말레이시아': 1, '말레이시아인': 1, '말로': 2, '말리': 1, '말씀': 7, '말투': 3, '말함': 2, '맘스': 1, '맛사지': 1, '맛잇엇어': 1, '맛집': 25, '망각': 1, '망신': 2, '망치': 2, '맞은편': 7, '맞이': 5, '매년': 2, '매니': 1, '매니저': 3, '매달': 1, '매듭': 1, '매력': 5, '매번': 1, '매우': 265, '매운탕': 1, '매일': 36, '매장': 3, '매점': 3, '매칭': 1, '매트': 5, '매트리스': 13, '매트릭스': 1, '매하': 1, '맥도날드': 5, '맥도널드': 1, '맥주': 22, '맥주잔': 1, '맨발': 3, '머리': 7, '머리카락': 4, '머신': 3, '머싱': 1, '먹거리': 7, '먹기': 2, '먹방': 1, '먹이': 3, '먼저': 3, '먼지': 3, '멀리': 14, '메가박스': 1, '메뉴': 15, '메리': 1, '메리어트': 1, '메시지': 1, ...} 3-5. TF-IDF 적용 &gt;&gt; TF-IDF 변환 Bag of Words 벡터에 대해서 TF-IDF변환 진행합니다. 1234from sklearn.feature_extraction.text import TfidfTransformertfidf_vectorizer = TfidfTransformer()tf_idf_vect = tfidf_vectorizer.fit_transform(bow_vect) 1print(tf_idf_vect.shape) (1001, 3599) 변환 후 1001*3599 matrix가 출력됩니다. 여기서 한 행(row)은 한 리뷰를 의미하고 한 열(column)은 한 단어를 의미합니다. 12# 첫 번째 리뷰에서의 단어 중요도(TF-IDF 값) -- 0이 아닌 것만 출력print(tf_idf_vect[0]) (0, 3588) 0.35673213299026796 (0, 2927) 0.2582351368959594 (0, 2925) 0.320251680858207 (0, 2866) 0.48843555212083145 (0, 2696) 0.23004450213863206 (0, 2311) 0.15421663035331626 (0, 1584) 0.48843555212083145 (0, 1527) 0.2928089229786031 (0, 790) 0.2528176728459411 123# 첫 번째 리뷰에서 모든 단어의 중요도 -- 0인 값까지 포함print(tf_idf_vect[0].toarray().shape)print(tf_idf_vect[0].toarray()) (1, 3599) [[0. 0. 0. ... 0. 0. 0.]] &gt;&gt; “벡터” - “단어” mapping 1vect.vocabulary_ {'집중': 2866, '휴식': 3588, '제공': 2696, '위치': 2311, '선정': 1584, '또한': 790, '청소': 2927, '청결': 2925, '상태': 1527, '이상': 2392, '침대': 3022, '이불': 2388, '침구': 3021, '교체': 299, '어메니티': 2013, '보강': 1296, '베스트': 1277, '웨스턴': 2299, '회원': 3564, '경우': 185, '객실': 106, '층수': 3009, '요청': 2234, '적극': 2606, '반영': 1188, '지인': 2837, '소개': 1629, '처음': 2910, '당황': 611, '세면': 1607, '도구': 675, '잠옷': 2555, '필수': 3358, '그것': 361, '정도': 2673, '다음': 578, '여기': 2074, '박만': 1171, '저녁': 2595, '맥주': 981, '한잔': 3414, '렌트': 838, '뚜벅': 791, '바로': 1159, '버스': 1247, '정류': 2676, '도착': 697, '가방': 24, '일찍': 2487, '정비': 2685, '방이': 1225, '입실': 2500, '업그레이드': 2038, '직원': 2849, '정말': 2680, '바다': 1148, '전망': 2623, '전일': 2636, '함덕': 3425, '대명': 624, '콘도': 3091, '실내': 1861, '분위기': 1384, '손님': 1659, '가장': 40, '배치': 1241, '대해': 651, '대응': 634, '써비스': 1889, '조식': 2730, '부분': 1351, '신경': 1838, '아주': 1922, '특급': 3208, '트랜디': 3191, '고민': 210, '흔적': 3593, '여름': 2082, '수영장': 1700, '사용': 1483, '보고': 1297, '엄마': 2035, '가격': 2, '대비': 627, '주위': 2769, '마트': 924, '식당': 1826, '시장': 1816, '방문': 1217, '의사': 2361, '동안': 726, '정해진': 2695, '휘슬': 3580, '공항': 269, '생각': 1539, '시설': 1809, '모두': 1028, '친절': 3017, '모드': 1029, '마지막': 917, '마무리': 900, '테라스': 3156, '전경': 2612, '인근': 2427, '재래시장': 2583, '야시장': 1979, '이용도': 2402, '이용': 2400, '일차': 2488, '휘슬락': 3581, '체크': 2940, '주변': 2761, '여친': 2094, '추억': 2975, '가성': 26, '추천': 2977, '후회': 3575, '예전': 2145, '그랜드': 372, '저희': 2605, '아이': 1918, '돌잔치': 708, '다정': 581, '했었더랬': 3470, '전통': 2642, '메종': 1004, '글래드': 387, '서비스': 1563, '매우': 969, '합리': 3428, '대의': 635, '명절': 1018, '숙박': 1721, '아티': 1928, '백미': 1243, '커피': 3061, '베이커리': 1280, '가족': 43, '플러스': 3342, '지금': 2816, '우선': 2257, '접근성': 2664, '쇼핑': 1671, '만족도': 940, '최고': 2957, '무엇': 1078, '기억': 423, '우리': 2251, '찾기': 2902, '교통': 300, '이틀': 2418, '정원': 2691, '거기': 109, '겨울': 160, '친구': 3016, '놀러와': 541, '투숙': 3185, '야외': 1981, '방향': 1235, '야간': 1976, '조명': 2726, '순간': 1723, '대접': 638, '실명': 1868, '거론': 112, '프론트': 3337, '이름': 2378, '안나': 1938, '기분': 416, '모습': 1037, '첫날': 2921, '스타트': 1761, '다시': 576, '선물': 1578, '카운터': 3032, '안내': 1939, '디너': 777, '기회': 438, '미니바': 1121, '무료': 1067, '최근': 2959, '관광지': 281, '숙고': 1719, '강추': 88, '야경': 1977, '일부러': 2475, '일어나서': 2476, '동네': 714, '바퀴': 1168, '먹방': 990, '신라': 1843, '퀄리티': 3105, '모녀': 1024, '제주시': 2711, '위해': 2313, '선택': 1586, '오픈': 2180, '예약': 2143, '대로': 622, '거리': 115, '더군다나': 657, '셔틀버스': 1627, '운행': 2271, '데스크': 670, '차안': 2880, '분도': 1374, '인상': 2438, '트윈': 3201, '사이즈': 1489, '만족': 939, '스타': 1758, '가짓수': 45, '느낌': 558, '화장실': 3538, '내부': 505, '인테리어': 2455, '질적': 2859, '이건': 2368, '개인': 100, '취향': 3005, '구비': 321, '과일': 273, '나이프': 480, '포크': 3295, '부탁': 1364, '준비': 2786, '표방': 3305, '모텔': 1046, '결론': 165, '훌륭': 3577, '시간': 1799, '슈페리어킹룸': 1737, '하루': 3376, '후기': 3570, '지은지': 2836, '얼마': 2033, '건물': 136, '약간': 1985, '유럽': 2318, '스타일': 1760, '그림': 378, '조각': 2715, '복도': 1323, '전시': 2633, '전부': 2630, '카펫': 3039, '등급': 770, '다른': 567, '이서': 2394, '자기': 2512, '조금': 2722, '외투': 2218, '완전': 2206, '진짜': 2856, '셔틀': 1626, '미리': 1127, '시티': 1821, '오름': 2162, '다만': 570, '도로': 680, '가인': 39, '관계': 276, '차량': 2876, '소리': 1638, '다소': 574, '방도': 1215, '편의점': 3273, '칫솔': 3027, '동계': 709, '훈련': 3576, '라마': 795, '연인': 2116, '전체': 2641, '사진': 1493, '첨부': 2920, '엘리베이터': 2064, '실시간': 1873, '비행기': 1440, '택시': 3151, '미만': 1128, '이동': 2374, '가능': 9, '최상': 2961, '욕실': 2235, '슬리퍼': 1791, '위생': 2304, '염려': 2123, '설날': 1590, '떡국': 788, '려고': 840, '현장': 3495, '결재': 167, '황스': 3559, '려운': 841, '중국': 2789, '만두': 932, '종류': 2744, '별로': 1293, '구만': 316, '메뉴': 996, '답변': 603, '제일': 2709, '주차': 2779, '걱정': 131, '할아버지': 3423, '안심': 1945, '컨디션': 3067, '문어': 1095, '처리': 2909, '회사': 3561, '출장': 2988, '서울': 1567, '간혹': 64, '이벤트': 2386, '해주시': 3460, '부대': 1344, '항상': 3435, '롯데': 855, '비지니스': 1434, '여행객': 2097, '손색': 1664, '욕조': 2236, '겸비': 175, '사계절': 1459, '수풀': 1715, '중문': 2794, '표선등': 3306, '원거리': 2281, '여행지': 2101, '복귀': 1322, '시내': 1805, '유명': 2323, '일단': 2462, '주차장': 2781, '지하': 2844, '공간': 246, '응대': 2356, '문의사항': 1097, '신지': 1850, '감동': 71, '역시': 2106, '장도': 2566, '락타': 807, '샤워': 1550, '마련': 896, '코인': 3089, '세탁실': 1620, '오션': 2167, '멀리': 994, '남자친구': 497, '덕분': 665, '이번': 2385, '프런트': 3331, '배정': 1239, '이착륙': 2415, '클리닝': 3120, '근무': 382, '아주머니': 1923, '인사': 2436, '코로나': 3085, '사태': 1496, '노화': 535, '벽지': 1286, '주름': 2756, '지고': 2814, '타일': 3133, '가구': 4, '코너': 3082, '곳곳이': 243, '관광객': 279, '인지': 2449, '사람': 1463, '북적': 1369, '전반': 2627, '관리': 284, '편이': 3274, '피드백': 3346, '편입': 3276, '도시': 686, '관광': 278, '특화': 3214, '지역': 2834, '근처': 385, '카페': 3034, '가기': 6, '고유': 221, '특색': 3210, '스테이': 1768, '가도': 10, '스타벅스': 1759, '번화가': 1263, '아침': 1924, '커서': 3057, '방음': 1224, '먹거리': 988, '단점': 588, '라면': 797, '오심': 2170, '이중': 2411, '혼자': 3523, '겐찮은듯': 157, '다그': 563, '도일': 694, '께빵': 456, '중국인': 2793, '타고': 3127, '눈앞': 556, '가지': 44, '힐링': 3598, '피트니스': 3354, '패키지': 3254, '미닫이': 1122, '로비': 847, '비롯': 1421, '모든': 1030, '각종': 52, '물건': 1103, '아침식사': 1926, '밥맛': 1207, '피아노': 3349, '연주': 2118, '룸타입': 867, '패밀리': 3250, '거실': 121, '한실': 3412, '서부': 1562, '수산시장': 1690, '새벽': 1534, '경매': 182, '구경': 304, '추가': 2972, '어차피': 2022, '현재': 3496, '묵고': 1090, '인터넷': 2453, '지정': 2840, '안해': 1952, '프런터': 3330, '문의': 1096, '영화': 2135, '시스템': 1810, '티브이': 3219, '채널': 2903, '몇개': 1020, '콘센트': 3092, '플러그': 3341, '와이프': 2201, '충전': 2995, '결정': 168, '달라': 594, '경험': 190, '장점': 2579, '인생': 2439, '최악': 2966, '중심': 2797, '바닷가': 1153, '비교': 1413, '편임': 3275, '한번': 3408, '예정': 2146, '휴가': 3585, '스위트룸': 1750, '유리창': 2322, '위트': 2312, '캠핑': 3050, '테이블': 3160, '세트': 1621, '텐트': 3161, '에어컨': 2054, '작은방': 2543, '매트': 975, '자리': 2520, '탑동': 3142, '공원': 263, '프리': 3338, '마켓': 923, '공연': 259, '매일': 971, '식사': 1833, '관덕정': 282, '정문': 2682, '도보': 684, '소요': 1645, '해장국': 3458, '동문': 717, '서문시장': 1560, '목관': 1047, '맞은편': 960, '슬슬': 1792, '별관': 1291, '본관': 1330, '갈수': 68, '최신': 2965, '그린': 377, '환경': 3545, '때문': 787, '별도': 1292, '대중교통': 641, '구매': 317, '심플': 1882, '비품': 1438, '트윈침대': 3205, '스탠다드': 1763, '높이': 548, '한라산': 3404, '노곤': 525, '터미널': 3152, '온돌룸': 2189, '세면대': 1608, '물이': 1114, '치약': 3013, '것임': 150, '생수': 1545, '환승': 3550, '곧바로': 231, '동광양': 710, '정류장': 2677, '시청': 1818, '스텝': 1771, '협소하': 3502, '거품': 130, '타월': 3131, '화장': 3537, '대가': 612, '거울': 123, '쇼파': 1670, '조합': 2739, '금액': 395, '주차공간': 2780, '공터': 267, '태풍': 3150, '개층': 104, '오후': 2184, '한시': 3409, '사우나': 1485, '부모님': 1349, '할머니': 3422, '모시': 1038, '계획': 197, '일로': 2464, '빠듯해': 1445, '어디': 2004, '길가': 441, '골목길': 235, '이륙': 2377, '착륙': 2887, '보이': 1310, '그냥': 363, '일반': 2469, '대욕': 633, '헬스장': 3489, '가면': 19, '고려': 206, '렌터카': 837, '입구': 2496, '반대쪽': 1182, '한정': 3416, '단체': 591, '더블': 660, '제외': 2708, '여유': 2088, '반대편': 1183, '도심': 688, '불구': 1387, '소음': 1648, '거의': 125, '세명': 1609, '트리플': 3199, '전날': 2618, '남아': 495, '자체': 2534, '당일': 610, '정보': 2683, '햇반': 3468, '장조림': 2580, '전자': 2637, '레인지': 831, '가야': 33, '세미나': 1610, '한국인': 3400, '그대로': 369, '나름': 469, '제과점': 2697, '정거장': 2670, '모기': 1022, '만해': 944, '마리': 898, '가량': 16, '방안': 1222, '에프킬라': 2058, '비치': 1435, '계속': 195, '뿌리': 1456, '잡고': 2557, '찬장': 2891, '천장': 2915, '곳곳': 242, '측은': 3008, '벌레': 1265, '기본': 414, '요금': 2223, '건너편': 135, '번호': 1262, '샐러드': 1538, '음식': 2349, '구성': 326, '전복죽': 2629, '저번': 2601, '핸드폰': 3464, '충전기': 2996, '불편': 1400, '센터': 1622, '공사': 256, '숙면': 1720, '무난': 1061, '노력': 528, '지불': 2826, '비용': 1430, '로맨틱': 845, '독립': 702, '북유럽': 1368, '유리': 2321, '커튼': 3059, '바깥': 1145, '설치': 1595, '모던': 1025, '비데': 1418, '효율': 3569, '인치': 2451, '삼성': 1512, '만끽': 927, '편안함': 3270, '여정': 2091, '바닥': 1151, '목적': 1052, '커플': 3060, '끼리': 464, '직진': 2852, '운전': 2268, '공영': 261, '애기': 1962, '무선인터넷': 1074, '갑자기': 80, '서전': 1568, '항공': 3432, '다가': 562, '가급': 5, '결제': 169, '은방': 2342, '상황': 1533, '아시': 1914, '고트': 227, '지도': 2822, '세심': 1614, '공시': 257, '시작': 1815, '맥주잔': 982, '쿠폰': 3102, '일도': 2463, '크기': 3108, '룸서비스': 864, '정신': 2690, '평가': 3277, '편의': 3272, '요즘': 2233, '여러': 2078, '직언': 2848, '블룸': 1412, '변기': 1288, '물질': 1115, '얘기': 1997, '수건': 1675, '환불': 3548, '월일': 2293, '푸른': 3313, '파도': 3223, '철썩': 2918, '풍경': 3320, '생선회': 1544, '장소': 2573, '놀이': 543, '양도': 1989, '돼지': 735, '전골': 2613, '매운탕': 970, '형편': 3512, '브런치': 1406, '뒤쪽': 751, '맛집': 956, '쭈욱': 2869, '갑인': 79, '드타': 764, '방파제': 1233, '횟집': 3567, '타운': 3129, '해산물': 3448, '규모': 356, '가까이': 7, '어르신': 2008, '깨끗': 454, '대신': 629, '작고': 2538, '수영': 1697, '아쉬움': 1912, '인도': 2430, '소독약': 1634, '스비': 1745, '홀로': 3525, '등정후': 775, '부터': 1365, '혹시': 3520, '확인': 3543, '전화': 2645, '통해': 3180, '휘트니': 3582, '운동복': 2266, '수영모': 1698, '고오': 219, '바람': 1157, '대여': 631, '여직원': 2093, '무시': 1077, '말투': 951, '원래': 2284, '제로': 2700, '운영': 2267, '일반인': 2470, '입장': 2503, '선심': 1583, '필요': 3360, '기전': 430, '대뜸': 620, '화가': 3528, '고객': 200, '플레인': 3343, '시경': 1800, '아웃': 1917, '하야': 3382, '메리어트': 998, '여럿': 2081, '이어도': 2397, '수준': 1707, '이군': 2370, '실망': 1867, '종일': 2747, '리셉션': 880, '컨시어': 3070, '태도': 3147, '유료': 2319, '사고': 1460, '학생': 3393, '선생님': 1580, '어딘': 2005, '사항': 1498, '무슨': 1075, '거지': 128, '리빙룸': 879, '하나요': 3368, '트윈룸': 3202, '무려': 1065, '게재': 156, '실물': 1869, '차이': 2881, '실화': 1879, '파우더': 3231, '곰팡이': 240, '자국': 2511, '작동': 2539, '골드스타': 233, '냉장고': 517, '절대': 2649, '레스토랑': 826, '자꾸': 2513, '정색': 2687, '살짝': 1510, '서버': 1561, '데리': 669, '가신': 32, '나머지': 470, '물기': 1105, '직접': 2851, '체계': 2931, '엉망': 2048, '소규모': 1630, '시기': 1803, '마음': 910, '리트': 886, '라그': 794, '기대': 409, '실내수영장': 1862, '수온': 1701, '스위트': 1749, '비수': 1427, '상대': 1515, '노후': 536, '정가': 2669, '무리': 1072, '할인': 3424, '행사': 3473, '참고': 2893, '포함': 3298, '마일리지': 913, '대한항공': 650, '하룻밤': 3377, '확실': 3542, '연식': 2115, '고급': 201, '원가': 2280, '서우': 1566, '해수욕장': 3453, '한눈': 3402, '창문': 2900, '겨울철': 161, '온도': 2187, '리기': 870, '방기': 1214, '동시': 725, '방식': 1221, '대략': 621, '도정': 695, '부스': 1352, '옆방': 2137, '티비': 3220, '취침': 3001, '초등학생': 2946, '양호': 1996, '한식당': 3411, '차라리': 2875, '반드시': 1184, '크게': 3107, '씨유': 1896, '델문': 674, '카페나': 3035, '해수욕': 3452, '최적': 2969, '디럭스': 778, '발코니': 1203, '모로': 1034, '트윈룸입니': 3203, '선착순': 1585, '예술': 2142, '물놀이': 1106, '모래': 1031, '피크': 3352, '시즌': 1817, '날씨': 491, '선선': 1581, '근래': 381, '편리': 3269, '코르': 3086, '기지': 434, '사양': 1481, '주체': 2782, '교회': 302, '절반': 2653, '인수': 2440, '보임': 1314, '아트': 1927, '추정': 2976, '프로': 3333, '페셔': 3264, '인력': 2432, '매트리스': 976, '나무': 472, '허리': 3482, '안감': 1934, '해변': 3447, '워낙': 2275, '가시': 31, '주말': 2757, '한국': 3397, '벚꽃': 1270, '기간': 403, '협재': 3503, '옹포': 2197, '밥집': 1209, '산책': 1505, '루프': 860, '비바람': 1425, '불어': 1396, '블로거': 1410, '물떄': 1108, '그거': 360, '위주': 2308, '풀이': 3317, '온수': 2192, '미온수': 1133, '미온': 1132, '자쿠지': 2536, '잠깐': 2551, '인피니트': 2458, '환상': 3549, '하나': 3365, '스페': 1782, '치킨': 3015, '빠에야': 1446, '스파': 1779, '구조': 333, '스파룸': 1780, '자쿠': 2535, '이용권': 2401, '하니': 3371, '사이': 1487, '시트': 1820, '정리': 2678, '삼다수': 1511, '병과': 1294, '네스프레소': 520, '캡슐': 3051, '여자': 2089, '피로': 3347, '스페인': 1784, '영업': 2132, '종료': 2743, '버거': 1245, '오른쪽': 2161, '마담': 893, '나탈리': 482, '음악': 2351, '필터': 3362, '실외수영장': 1875, '검색': 147, '옥상': 2186, '결과': 164, '대형': 653, '만큼': 942, '번잡': 1260, '개수대': 98, '이전': 2407, '아무': 1909, '리지': 884, '제한': 2714, '경치': 189, '이웃': 2403, '일몰': 2467, '실제': 1877, '리조트': 883, '홍보': 3527, '상통': 1529, '화산': 3532, '바위': 1166, '정상': 2686, '기점': 431, '스템': 1770, '잠자리': 2556, '기타': 435, '부족함': 1359, '길이': 444, '다행': 582, '중국어': 2792, '영어': 2131, '도움': 692, '해결': 3438, '목적지': 1053, '해피': 3461, '음료': 2346, '사이다': 1488, '주스': 2764, '달걀': 592, '치즈': 3014, '히터': 3597, '투어': 3187, '호스텔': 3515, '질문': 2858, '상품': 1530, '과거': 270, '게스트하우스': 154, '자고': 2510, '오기': 2150, '일이': 2479, '발전': 1201, '예상': 2141, '토스터': 3165, '이후': 2424, '소등': 1635, '불키': 1399, '조용조': 2733, '방키': 1232, '고여': 218, '남자': 496, '안전': 1946, '차로': 2878, '이내': 2373, '뷔페': 1403, '여느': 2076, '롯데리아': 856, '방만': 1216, '고정': 224, '무궁화': 1060, '수기': 1676, '사이트': 1490, '홈페이지': 3526, '마치': 921, '무척': 1088, '전기차': 2617, '건너': 134, '급속': 402, '소가': 1628, '유치원': 2333, '주택가': 2784, ...} 12invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()}print(str(invert_index_vectorizer)[:100]+'...') {2866: '집중', 3588: '휴식', 2696: '제공', 2311: '위치', 1584: '선정', 790: '또한', 2927: '청소', 2925: '청결', 1527... 4. 감성 분류 – Logistic Regression 이제 전처리된 리뷰 데이터를 활용하여 감성 분류 예측 모델을 만들겠습니다. 감성 분류 예측 모델이란, 이용자 리뷰의 평가 내용을 통해 이 리뷰가 긍정적인지, 부정적인지를 예측하여, 이용자의 감성을 파악하는 겁니다. 따라서, 모델의 X 값(즉, feature 값)은 이용자 리뷰의 평가 내용이 되겠고, 모델의 Y 값(즉, label 값)은 이용자의 긍/부정 감성이 되겠습니다. 4-1. 데이터셋 생성 &gt;&gt; Label 우리는 이용자의 리뷰를 “긍정” / “부정” 두가지 부류로 나누고자 합니다. 하지만 이러한 이용자의 감성을 대표할 수 있는 “평가 점수” 변수는 1 ~ 5의 value를 가지고 있습니다. 따라서 \"평가 점수\"변수 (rating: 1 ~ 5)를 이진 변수 (긍정: 1, 부정:0)으로 변환해야 합니다. 1df.sample(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rating text 951 3 나는 그것이 수영장을 가지고 있기 때문에 여기에서 예약했다. 그러나 수영장에 물이 ... 47 4 시설은 좀 오래 되었지만 동문시장, 서문시장 도보 10분거리이고 공항에서도 가깝습니... 574 4 호스트는 아주 친절하고 도움이 되었습니다. 그는 우리를 픽업해서 근처에 있는 동안 ... 637 4 제주시에 있는 호텔로 깔끔한 편이었고 나름 전망도 괜찮았습니다. 주차장은 주차타워에... 113 4 루프탑 바와 수영장이 있어서 사용가능하고 깨끗하고 친절하셔서 좋은곳이었습니다. 다만... 416 3 다양한 음식과 음료를 걸어갈 수 있는 곳. 저녁 식사는 늦은 밤에 음식을 찾는 문제... 671 2 리뷰보고 기대했는데 호텔이라기 보단 모텔이나 펜션 느낌이네요 생긴지 얼마 안된걸로 ... 235 5 위치가 바로 해변 근처라, 룸에서 보이는 뷰가 너무 좋습니다. 세화해변 자체가 조용... 875 3 Jeju 섬에서 4 일째되는 주제와 같이 Jeju 공항에서 터치 다운 이후로 이동 ... 254 4 출장 때문에 제주도에 오게 됐습니다. 가성비가 좋고 전체적으로 깨끗했습니다. 난방이... 리뷰 내용와 평점을 살펴보면, 4 ~ 5점 리뷰는 대부분 긍정적이었지만, 1 ~ 3점 리뷰에서는 부정적인 평가가 좀 많이 보였습니다. 그래서 4점, 5점인 리뷰는 \"긍정적인 리뷰\"로 분류하여 1를 부여하고, 1 ~ 3점 리뷰는 \"부정적인 리뷰\"로 분류하여 0을 부여하도록 할게요. 1df['rating'].hist() &lt;matplotlib.axes._subplots.AxesSubplot at 0x154887d0b08&gt; 1234567def rating_to_label(rating): if rating &gt; 3: return 1 else: return 0 df['y'] = df['rating'].apply(lambda x: rating_to_label(x)) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rating text y 0 4 여행에 집중할수 있게 편안한 휴식을 제공하는 호텔이었습니다. 위치선정 또한 적당한 ... 1 1 4 2일 이상 연박시 침대, 이불, 베게등 침구류 교체 및 어메니티 보강이 필요해 보입... 1 2 4 지인에소개로온 호텔 깨끗하고 좋은거같아요 처음에는 없는게 많아 많이 당황했는데 ... 1 3 5 방에 딱 들어서자마자 눈이 휘둥그레질정도로 이렇게 넓은 호텔 처음 와본 것 같아요!... 1 4 5 저녁에 맥주한잔 하는게 좋아서 렌트 안하고 뚜벅이 하기로 했는데 호텔 바로 앞에 버... 1 1df[\"y\"].value_counts() 1 726 0 275 Name: y, dtype: int64 &gt;&gt; Feature 모델의 Feature 변수는 리뷰에서 추출된 형태소와 그들의 중요도를 나타나는 tf_idf_vect로 대체하겠습니다. 4-2. Training set / Test set 나누기 12345from sklearn.model_selection import train_test_splitx = tf_idf_vecty = df['y']x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=1) 1x_train.shape, y_train.shape ((700, 3599), (700,)) 1x_test.shape, y_test.shape ((301, 3599), (301,)) 4-3. 모델 학습 &gt;&gt; Logistic Regression 모델 학습 123456789from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score# fit in training setlr = LogisticRegression(random_state = 0)lr.fit(x_train, y_train)# predict in test sety_pred = lr.predict(x_test) &gt;&gt; 분류 결과 평가 123456# classification result for test setprint('accuracy: %.2f' % accuracy_score(y_test, y_pred))print('precision: %.2f' % precision_score(y_test, y_pred))print('recall: %.2f' % recall_score(y_test, y_pred))print('F1: %.2f' % f1_score(y_test, y_pred)) accuracy: 0.72 precision: 0.72 recall: 1.00 F1: 0.84 12345678910# confusion matrixfrom sklearn.metrics import confusion_matrixconfu = confusion_matrix(y_true = y_test, y_pred = y_pred)plt.figure(figsize=(4, 3))sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')plt.title('Confusion Matrix')plt.show() 모델 평가결과를 살펴보면, 모델이 지나치게 긍정(“1”)으로만 예측하는 경향이 있습니다. 따라서 긍정 리뷰를 잘 예측하지만, 부정 리뷰에 대한 예측 정확도가 매우 낮습니다. 이는 샘플데이터의 클래스 불균형으로 인한 문제로 보입니다. 따라서, 클래스 불균형 조정을 진행하겠습니다. 4-4. 샘플링 재조정 &gt;&gt; 1:1 Sampling 1df['y'].value_counts() 1 726 0 275 Name: y, dtype: int64 12positive_random_idx = df[df['y']==1].sample(275, random_state=12).index.tolist()negative_random_idx = df[df['y']==0].sample(275, random_state=12).index.tolist() 1234random_idx = positive_random_idx + negative_random_idxx = tf_idf_vect[random_idx]y = df['y'][random_idx]x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1) 1x_train.shape, y_train.shape ((412, 3599), (412,)) 1x_test.shape, y_test.shape ((138, 3599), (138,)) 4-5. 모델 재학습 &gt;&gt; 모델 학습 123lr2 = LogisticRegression(random_state = 0)lr2.fit(x_train, y_train)y_pred = lr2.predict(x_test) &gt;&gt; 분류 결과 평가 123456# classification result for test setprint('accuracy: %.2f' % accuracy_score(y_test, y_pred))print('precision: %.2f' % precision_score(y_test, y_pred))print('recall: %.2f' % recall_score(y_test, y_pred))print('F1: %.2f' % f1_score(y_test, y_pred)) accuracy: 0.72 precision: 0.70 recall: 0.74 F1: 0.72 12345678910# confusion matrixfrom sklearn.metrics import confusion_matrixconfu = confusion_matrix(y_true = y_test, y_pred = y_pred)plt.figure(figsize=(4, 3))sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')plt.title('Confusion Matrix')plt.show() 이제 모델이 “긍정적인” 케이스와 “부정적인” 케이스를 모두 적당히 잘 맞춘 것을 확인할 수 있습니다. 5. 긍정 / 부정 키워드 분석 기계는 이처럼 리뷰 내용에 나타나는 사람의 감성을 구별할 수 있을 뿐만 아니라, 학습된 Logistic Regression 모델을 이용하여 긍/부정 키워드도 추출해낼 수 있습니다. 추출된 키워드를 통해서 이용자가 느끼는 제주호델의 장,단점을 파악할 수 있고, 이를 기반으로 앞으로 유지해야 할 좋은 서비스와 개선이 필요한 아쉬운 서비스에 대해서도 어느정도 판단할 수 있습니다. 긍 / 부정 키워드를 추출하기 위해 먼저 Logistic Regression 모델에 각 단어의 coeficient를 시각화해보겠습니다. 1lr2.coef_ array([[ 0.28196772, 0.10796991, -0.04978601, ..., 0. , -0.18315162, 0.28434689]]) 1234# print logistic regression's coefplt.figure(figsize=(10, 8))plt.bar(range(len(lr2.coef_[0])), lr2.coef_[0]) &lt;BarContainer object of 3599 artists&gt; 여기서 계수가 양인 경우는 단어가 긍정적인 영향을 미쳤다고 볼 수 있고, 반면에, 음인 경우는 부정적인 영향을 미쳤다고 볼 수 있습니다. 이 계수들을 크기순으로 정렬하면, 긍정 / 부정 키워드를 출력하는 지표가 되겠습니다. 먼저 \"긍정 키워드\"와 \"부정 키워드\"의 Top 5를 각각 출력해볼게요. 123print(sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)[:5])print(sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)[-5:])# enumerate: 인덱스 번호와 컬렉션의 원소를 tuple형태로 반환함 [(1.2644550507381787, 1217), (0.9079356150239053, 2400), (0.895609472071521, 1148), (0.8859075267474583, 2730), (0.8795111499693716, 43)] [(-0.7201222787741572, 1310), (-0.7519681298547074, 3022), (-0.8672956005075485, 567), (-0.9190158099937462, 515), (-0.9945592515966041, 2143)] 이처럼 단어의 coeficient와 index가 출력이 됩니다. 이제 전체 단어가 포함한 \"긍정 키워드 리스트\"와 \"부정 키워드 리스트\"를 정의하고 출력해볼게요. 123coef_pos_index = sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)coef_neg_index = sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = False)coef_pos_index [(1.2644550507381787, 1217), (0.9079356150239053, 2400), (0.895609472071521, 1148), (0.8859075267474583, 2730), (0.8795111499693716, 43), (0.8541915649753757, 26), (0.8362541212560809, 578), (0.7714811231976703, 2957), (0.7375280889735719, 1491), (0.7203390936359615, 956), (0.6503260268852225, 2977), (0.6488836121942877, 115), (0.6467914172687944, 910), (0.6264469987695738, 1159), (0.5943145305412955, 883), (0.5505354129422678, 2988), (0.5294632094678557, 692), (0.5240729254152497, 2455), (0.5207834696883535, 1922), (0.5153917648445299, 2361), (0.49372825531123943, 1215), (0.49360869707006777, 246), (0.4854919888814009, 680), (0.4854086392859413, 269), (0.4748702738145659, 686), (0.4703566749364605, 1194), (0.45896954031613635, 790), (0.45586600022584467, 2834), (0.45372800281293535, 19), (0.4500540795581468, 2779), (0.44412671542407905, 416), (0.4425720292652355, 1805), (0.4364684750426245, 263), (0.4308546124247834, 3267), (0.4255898116545533, 2797), (0.4212941284904679, 2680), (0.41423556597532674, 489), (0.4121691212201187, 826), (0.4118068110883225, 937), (0.402234791816506, 353), (0.39915216217204813, 136), (0.3906502087724938, 131), (0.38863745777148795, 2180), (0.38330015115324956, 322), (0.37909051494174656, 1027), (0.3776598665737952, 1926), (0.36980255314279886, 2760), (0.3609360208520378, 3034), (0.3601455791606223, 1342), (0.3511486012544648, 344), (0.35030155043520095, 3447), (0.34810767211689553, 2674), (0.3448646839750021, 3021), (0.34466834073677766, 2070), (0.3341929597049384, 1750), (0.3332295460784216, 2849), (0.3291725978477463, 3435), (0.3276653378330499, 2530), (0.3221942756247239, 1483), (0.3217689173991826, 3397), (0.32047891465181083, 2013), (0.3080484198419967, 3029), (0.3062772550220064, 939), (0.30478483149478314, 627), (0.30438930158469363, 2606), (0.3027703771200994, 3454), (0.2989615171043252, 4), (0.2960030540205178, 1991), (0.2941392017845358, 3347), (0.290078280630203, 3593), (0.28793362240859766, 1090), (0.2861006595602784, 3404), (0.28434689214226744, 3598), (0.2819677181770861, 0), (0.2818546705602182, 1809), (0.2796034993266096, 1440), (0.2783451949177126, 3410), (0.27700777474574007, 1078), (0.2764799249539472, 278), (0.2757845449039994, 2074), (0.27449374217124756, 1697), (0.27211400542938347, 860), (0.26958773489776877, 276), (0.26894024258506205, 2642), (0.2679652653240931, 451), (0.26659353628773785, 2605), (0.2642541001710917, 1073), (0.2602933330998526, 71), (0.2601225988013697, 2727), (0.25898398896658026, 395), (0.25523176043119644, 109), (0.25400402242362763, 1120), (0.25346771390562256, 2311), (0.2525923917872476, 1760), (0.252483390540294, 3549), (0.25243556295192593, 44), (0.2523758090983611, 410), (0.25096842816549325, 3286), (0.25030837769014164, 3013), (0.2478963860060586, 3064), (0.24711051623367455, 1700), (0.24639626607741794, 532), (0.24423718622136711, 2339), (0.24419509835066824, 817), (0.24268824175871553, 1646), (0.24020390663531643, 847), (0.23613168717319782, 1277), (0.2356739894439279, 3547), (0.23333780634901968, 1966), (0.2331264370168398, 1350), (0.23268102047017378, 769), (0.23262817998676638, 3166), (0.23066409348184064, 1384), (0.23010041799635683, 210), (0.22586220283294378, 1885), (0.22523607425083558, 260), (0.2222412951070305, 281), (0.22091253142370337, 68), (0.21922527479299828, 1147), (0.21868875489181047, 1465), (0.21807039000574996, 3299), (0.21807039000574996, 172), (0.2177926484819077, 3223), (0.21751093007081115, 2634), (0.2163036212275728, 1171), (0.2162361260915301, 1671), (0.21591443393848506, 1552), (0.21530945315363187, 2038), (0.21450361502341533, 385), (0.2130004456328932, 2018), (0.21283349877228466, 2752), (0.21171382727695384, 1523), (0.21156875238749653, 2906), (0.21149936066156635, 3273), (0.2107420524603348, 3062), (0.2089604714261007, 2167), (0.20857560294926117, 705), (0.20849174650373709, 751), (0.2084323782944626, 2374), (0.2083850088267226, 1421), (0.2076716418538995, 1823), (0.20734493904041118, 2837), (0.20719074367652907, 1455), (0.20632098174995006, 2421), (0.20497429689912247, 2866), (0.20497429689912247, 1584), (0.20460844733702388, 1309), (0.20460135810671237, 1721), (0.2026617971957564, 192), (0.2021896255723423, 574), (0.20076231770123473, 1122), (0.20007368092656116, 981), (0.1994615301156886, 2722), (0.1987204932053518, 1895), (0.1982476502888129, 1622), (0.19668791935266033, 1816), (0.19602558824910057, 767), (0.195808499032481, 581), (0.1954442410972148, 2551), (0.19543920090503364, 2244), (0.19535829875348126, 1249), (0.19513316286959848, 2451), (0.1932943416840944, 2561), (0.19241691136362254, 1278), (0.19188249800399276, 221), (0.19059569732190523, 341), (0.18931948208829832, 2965), (0.18863137833398097, 2225), (0.188233216963001, 1067), (0.18823116402871, 1510), (0.18591329718666444, 2556), (0.1858513716606046, 1853), (0.18475568229990585, 1553), (0.1842332027380814, 15), (0.18385370069092052, 490), (0.183698940971588, 646), (0.18352145151744345, 2709), (0.18179588928699872, 1821), (0.18141472902160485, 356), (0.18101860699904043, 3325), (0.17968533410962978, 1517), (0.17894437469027485, 1441), (0.1785803255429378, 203), (0.17749094181502081, 1291), (0.17745689661710926, 423), (0.17691919755039834, 748), (0.17681729295382825, 629), (0.17650785942100156, 2042), (0.17521436890986974, 2822), (0.17420775259072946, 3525), (0.17407554594971275, 1376), (0.17390397512207328, 1508), (0.17390397512207328, 584), (0.1735238781146927, 509), (0.1729437410073151, 2188), (0.17166919021535662, 2761), (0.1711332476240707, 1505), (0.17079932615633314, 300), (0.1706243328180457, 795), (0.16989041829422863, 1577), (0.16965179412684628, 1028), (0.16919925883678436, 2705), (0.1687459695340302, 667), (0.16815742142636886, 1625), (0.1676232210748594, 2755), (0.16757499046872387, 2274), (0.16736563458006257, 1485), (0.16735261847037675, 2476), (0.16642204341549788, 2035), (0.16595006133727253, 760), (0.16475334154520693, 1208), (0.1646740028459598, 1937), (0.16464847364532936, 690), (0.1636808432150993, 3274), (0.16348761826549618, 1676), (0.16301112079103763, 2399), (0.1629849886257, 3591), (0.1629118596848168, 3186), (0.1626689828520234, 3210), (0.16142787916860177, 1598), (0.1613522761276804, 3417), (0.1603451229219545, 3116), (0.1599203302911822, 2487), (0.15983057428397826, 589), (0.1586108439130326, 2331), (0.15849368672169892, 1647), (0.15826740340037068, 625), (0.15743176994516933, 3490), (0.1571722075528101, 3567), (0.15712518230005013, 3027), (0.15687460543471862, 3428), (0.15651108129304298, 1307), (0.15600166572103646, 1961), (0.15583305075516699, 106), (0.15577941820129187, 1617), (0.155766902206714, 3017), (0.15491622812261568, 382), (0.15485968714815496, 1528), (0.1545939535424059, 259), (0.15411614699130288, 684), (0.1535726280760412, 3293), (0.1535726280760412, 2892), (0.15341384775377642, 2726), (0.15325471176173253, 1533), (0.1529359648993235, 1983), (0.1523311300168501, 1432), (0.15231484413753682, 1030), (0.15231397485990963, 556), (0.15139635636201898, 2078), (0.1511214654825413, 1312), (0.15101670800091527, 2998), (0.14887052306393284, 3096), (0.14887052306393284, 1457), (0.1485864638932176, 2600), (0.14818094009147514, 2223), (0.14810636314705725, 1859), (0.14763883239448744, 1738), (0.14724688251552864, 2328), (0.1471683691325973, 1841), (0.14705200941823965, 2268), (0.14704245351541728, 660), (0.14646359476664114, 3297), (0.14621204479849256, 2226), (0.14615889360176976, 3350), (0.14576749911151465, 2851), (0.14545079981853645, 2275), (0.1447568799500706, 2798), (0.1444313715254008, 2022), (0.1444180222221931, 1654), (0.1444180222221931, 1548), (0.14434325571263137, 2445), (0.14412764469019354, 1396), (0.1439309933360322, 3220), (0.14364997884743677, 3272), (0.14359122893612475, 1814), (0.14339034972990414, 2983), (0.14305710787294673, 1085), (0.14276222540097244, 348), (0.14269270587505128, 996), (0.14200348649786682, 3275), (0.14175159317612532, 3189), (0.14173146008479248, 714), (0.14095242066301328, 306), (0.14092668743912232, 2678), (0.14086483039942074, 570), (0.14061627821721584, 3414), (0.14047941175558679, 387), (0.1404004427633594, 3476), (0.1404004427633594, 1112), (0.13978500519793755, 2847), (0.13978500519793755, 780), (0.13964073789423023, 665), (0.13934374266681135, 3500), (0.13829992453370865, 3161), (0.13817992752504213, 1768), (0.13745049759016625, 3511), (0.13665192952568253, 757), (0.13642162742786484, 1351), (0.1359845468368403, 271), (0.13597476793796576, 534), (0.135852916769958, 2497), (0.13574633450491594, 3009), (0.13574633450491594, 2299), (0.13574633450491594, 1296), (0.13574633450491594, 1188), (0.13549762485752148, 3571), (0.13549762485752148, 3391), (0.13549762485752148, 2640), (0.13541454362686076, 1332), (0.13519719011184664, 238), (0.13515286340239754, 2047), (0.13500163846144472, 3152), (0.1347989845038155, 2353), (0.13477156350779324, 1527), (0.13474112790096265, 1889), (0.13474112790096265, 1196), (0.13474112790096265, 462), (0.13469292571022845, 11), (0.13458956235978478, 924), (0.13442001799674802, 2069), (0.1342034640274507, 2869), (0.1341559282111133, 2145), (0.13415237816410766, 3471), (0.13415237816410766, 3406), (0.13388090777365988, 94), (0.13342324387367194, 2971), (0.1332930603370377, 3432), (0.1330404844897235, 1506), (0.13293873503860765, 3087), (0.13272932864456133, 3073), (0.13232986210850453, 2246), (0.13218242328559932, 2082), (0.13197846164836427, 3285), (0.13197846164836427, 2297), (0.13197846164836427, 1741), (0.13197846164836427, 69), (0.13179675529668713, 1549), (0.1316814762162865, 3441), (0.13154581005598, 3192), (0.1314192595003537, 1386), (0.13129169059042645, 969), (0.1311653774611966, 244), (0.13071289286611526, 3003), (0.13067314069850974, 697), (0.12989273592089953, 466), (0.1297746134602387, 411), (0.12976187780923948, 39), (0.12975779444471916, 3176), (0.12975779444471916, 3066), (0.1296539223346817, 3169), (0.1295761221334622, 2132), (0.12915915165160174, 3585), (0.12912049655247929, 3475), (0.12912049655247929, 455), (0.12897380261630323, 1053), (0.12884028806560763, 2910), (0.1283431735744972, 2344), (0.1283431735744972, 1542), (0.12780627572336126, 2207), (0.12780627572336126, 531), (0.1277445628861703, 2975), (0.12741425172305826, 669), (0.1268676275287745, 424), (0.12600280097170596, 2053), (0.1255687931546876, 1578), (0.1251082717170878, 3287), (0.12501636694175655, 3082), (0.12485375424445369, 988), (0.12485077138187695, 1247), (0.12477317035474626, 1638), (0.12470562190989118, 1234), (0.12418537178635555, 3201), (0.12412494478481371, 1572), (0.12356427400475413, 720), (0.12346465010011236, 3190), (0.1232984083336477, 3384), (0.12290996269246392, 2930), (0.12272802616213961, 3448), (0.12252951158801757, 73), (0.12243269952395211, 3105), (0.12240812373820147, 2735), (0.12169970934221791, 2781), (0.12150853668051385, 1722), (0.12148978639712985, 1301), (0.12147421457137794, 2320), (0.12147421457137794, 2289), (0.12135904501371637, 3339), (0.12135904501371637, 1602), (0.12085540050886351, 3206), (0.12085540050886351, 2067), (0.12085540050886351, 1282), (0.12085540050886351, 1170), (0.12085540050886351, 199), (0.12073634202256898, 2460), (0.12051291677661294, 2250), (0.12051291677661294, 730), (0.12036501072581905, 3263), (0.12036501072581905, 373), (0.11982407529162177, 74), (0.11941421355306232, 2814), (0.11941421355306232, 2193), (0.11909411357560733, 2028), (0.11903875427006184, 403), (0.11887476984730765, 1374), (0.11855602046019094, 912), (0.11826793727408907, 2664), (0.11797129443799506, 147), (0.11792307582322151, 1752), (0.11792307582322151, 518), (0.1177499125052935, 1838), (0.11735274081083348, 2526), (0.11728847189963591, 1299), (0.11682555059780918, 702), (0.11662484295496746, 3453), (0.11615131711727657, 777), (0.11608095184078544, 771), (0.11598871515918448, 2677), (0.11589077346213462, 1341), (0.11572533411469992, 2492), (0.11567709862499206, 2587), (0.11567709862499206, 2077), (0.11542281429727001, 2221), (0.11525735836070924, 3389), (0.11501634646006889, 2886), (0.11501634646006889, 835), (0.11449108387629879, 267), (0.11419455143266674, 41), (0.11410478259093633, 2499), (0.11410478259093633, 636), (0.11391082531008434, 1939), (0.11379525876290054, 1779), (0.11369189505531267, 3279), (0.11359623354805377, 3362), (0.11328311707861126, 1626), (0.11309515954317759, 2033), (0.11309203979609482, 1163), (0.11306723080679525, 446), (0.11300787365565511, 88), (0.11281954183130295, 2799), (0.1125643845359886, 359), (0.1125643845359886, 87), (0.11236486622722328, 1599), (0.11236486622722328, 1462), (0.11202792916061027, 1320), (0.11168083926782998, 2088), (0.1116425153114436, 801), (0.11160861926991839, 2298), (0.11095938208098642, 304), (0.11091238300534989, 773), (0.1100077175343078, 2162), (0.10976564909688623, 1790), (0.10961263739649914, 258), (0.10941413976516044, 3488), (0.10941413976516044, 2819), (0.10941413976516044, 1828), (0.10917777932688458, 723), (0.10864303256976575, 747), (0.10864303256976575, 148), (0.10852304491119706, 1106), (0.10844777870744055, 1289), (0.10844777870744055, 619), (0.10832336961288917, 2692), (0.10832336961288917, 1454), (0.10830603618528553, 84), (0.10796991064699014, 1), (0.10795721696924253, 2315), (0.10795721696924253, 425), (0.1077710568146692, 3317), (0.1077710568146692, 2663), (0.10769380388975194, 2008), (0.10720615786237517, 583), (0.10673796206207481, 888), (0.10671454696616327, 601), (0.10656205646300153, 201), (0.1064891137904478, 1554), (0.10637660107629578, 1759), (0.10634735402569995, 3408), (0.10626212376589807, 2386), (0.10623995311965392, 3319), (0.10622168812753784, 333), (0.10615533943965197, 772), (0.10595924232583608, 337), (0.10595924232583608, 272), (0.10508229441497294, 3028), (0.10485989916431773, 2462), (0.10473698952461245, 3086), (0.10473698952461245, 434), (0.10473698952461245, 381), (0.10461042411497755, 3129), (0.10461042411497755, 1233), (0.10461042411497755, 764), (0.10461042411497755, 79), (0.10445117600612656, 3513), (0.10445117600612656, 3143), (0.10445117600612656, 401), (0.1041925044133613, 1207), (0.10388289797906243, 3088), (0.10359537183826453, 2766), (0.10359537183826453, 822), (0.10357097224199376, 3528), (0.10298171781942682, 3460), (0.10261708591617978, 527), (0.10237985140859761, 1290), (0.10223721967543753, 863), (0.10222967216366273, 582), (0.10189163932285969, 903), (0.10174059857006298, 31), (0.1015489083625263, 696), (0.10152723059467353, 1012), (0.10132218984016486, 2342), (0.10132218984016486, 1914), (0.10132218984016486, 1568), (0.10132218984016486, 562), (0.10132218984016486, 257), (0.10132218984016486, 227), (0.10132218984016486, 169), (0.10132218984016486, 5), (0.10129687975732969, 2385), (0.10126776819935551, 3117), (0.10063103908292556, 284), (0.10055954160111301, 3081), (0.10031435374070032, 1260), (0.10028094106951953, 1317), (0.10027605850365232, 3072), (0.1000059209109059, 2336), (0.1000059209109059, 479), (0.09995417863737927, 1670), (0.09946866203075326, 299), (0.09937460223662734, 1955), (0.09937460223662734, 355), (0.0991931372321024, 3367), (0.0991931372321024, 3052), (0.0991931372321024, 2987), (0.0991931372321024, 614), (0.09885234039929348, 1496), (0.0985825853170792, 2753), (0.09833844719113422, 3349), (0.09833844719113422, 2118), (0.09828284940929058, 900), (0.09826909686300346, 2262), (0.09826909686300346, 1765), (0.09826909686300346, 904), (0.09826909686300346, 652), (0.09820798725090112, 2536), (0.09820798725090112, 1967), (0.0979795780346747, 2907), (0.09782691886062657, 377), (0.09760724210433858, 3516), (0.09755885308766699, 3164), (0.09755885308766699, 683), (0.09730186511202356, 3329), (0.09730186511202356, 1733), (0.09730186511202356, 1226), (0.097102913934461, 2625), (0.09610978450473183, 2087), (0.09582399417186592, 1439), (0.09577717071956568, 47), (0.09574004674208489, 2532), (0.09567949353604623, 153), (0.09563595118779827, 1248), (0.09563595118779827, 800), (0.0955218528925641, 2265), (0.0954330687710674, 1715), (0.09490960244197376, 3079), (0.09479895154722345, 3167), (0.09479895154722345, 2182), (0.09464204684604893, 2919), (0.09464204684604893, 1007), (0.09464204684604893, 421), (0.0945793543479756, 3346), (0.0945793543479756, 3214), (0.09456116933710224, 1829), (0.09452473946342346, 736), (0.09441875395888008, 3102), (0.09441875395888008, 1541), (0.09418492749174653, 52), (0.0939222882360469, 3558), (0.0939222882360469, 3419), (0.0939222882360469, 2608), (0.0939222882360469, 2598), (0.09392068680284914, 3344), (0.09392068680284914, 2173), (0.09392068680284914, 1138), (0.09392068680284914, 461), (0.09392068680284914, 34), (0.09379123891116847, 2584), (0.09353081179807116, 3259), (0.09353081179807116, 1302), (0.09353081179807116, 293), (0.09309370020530645, 2639), (0.09309370020530645, 2294), (0.09309370020530645, 2083), (0.09309370020530645, 1944), (0.09309370020530645, 162), (0.09298771310954623, 741), (0.09298771310954623, 140), (0.09292587512205373, 1929), (0.09289423937435062, 3561), (0.09274784637141024, 3307), (0.09274784637141024, 2126), (0.09274784637141024, 2025), (0.09274784637141024, 1418), (0.09254637783999758, 217), (0.09226460305753335, 3403), (0.09226460305753335, 1687), (0.09226460305753335, 1592), (0.09192383607443329, 3315), (0.09192383607443329, 875), (0.09183495854900056, 2985), (0.09183495854900056, 1596), (0.09173493202889069, 1770), (0.09173493202889069, 431), (0.09173143027976542, 2876), (0.09113344147288725, 3120), (0.09060526255265101, 3381), (0.09060526255265101, 2287), (0.09060526255265101, 1836), (0.09060526255265101, 1769), (0.09050760723810075, 326), (0.08995529378430658, 340), (0.08987429575682436, 1969), (0.08982977958803778, 1812), (0.08974209619577075, 838), (0.08963253567327513, 2169), (0.08951903081805541, 2618), (0.08947267121555101, 989), (0.08926541208781298, 1369), (0.0890462273393359, 893), (0.0890462273393359, 482), (0.0888304568052451, 1118), (0.08864375178731801, 2759), (0.08862582335907176, 3128), (0.08856874594603148, 2408), (0.08847924280717331, 3541), (0.08847924280717331, 2706), (0.08845733589313011, 1614), (0.08843074951990057, 1330), (0.08827578191477906, 2529), (0.08827578191477906, 548), (0.08811593995641924, 2768), (0.08795612661377235, 1848), (0.0878632294425667, 3204), (0.0878632294425667, 2905), (0.0877915209166029, 2732), (0.08724002705412705, 3125), (0.08681879523440901, 472), (0.0867590941431764, 1371), (0.08591449282742145, 3413), (0.08591449282742145, 1793), (0.08591449282742145, 439), (0.08569361517932625, 2429), (0.08569361517932625, 1886), (0.08564019433508445, 2981), (0.08564019433508445, 2571), (0.08564019433508445, 1852), (0.08564019433508445, 145), (0.0855146046919488, 3001), (0.08482470727509915, 107), (0.08474214619355472, 291), (0.08474214619355472, 92), (0.08464630577025285, 1353), (0.08447795462677392, 83), (0.08442221636455624, 2373), (0.08435306298004447, 3302), (0.08435306298004447, 1842), (0.08435306298004447, 885), (0.08406143202879186, 961), (0.08374180878487095, 2901), (0.08352466261109656, 1451), (0.08248922005074939, 1131), (0.08238017336779274, 2415), (0.08216565036306289, 922), (0.08170778577083812, 503), (0.08139930660843568, 749), (0.0810873859250381, 2703), (0.08099924967644831, 3375), (0.08036733484765383, 103), (0.08024334048387935, 1346), (0.08024334048387935, 1055), (0.08024334048387935, 828), (0.07995951135307515, 2913), (0.07995254170536943, 3142), (0.07975739071961907, 1238), (0.07961368503404338, 3145), (0.07961368503404338, 1102), (0.0791011342722481, 2629), (0.07909734484039603, 375), (0.07903458017917456, 960), (0.07865148217479348, 1387), (0.0779646282219197, 3122), (0.0779646282219197, 350), (0.07794818207773628, 2696), (0.07791539520575948, 2197), (0.07791539520575948, 1425), (0.07791539520575948, 1410), (0.07791539520575948, 1270), (0.07791539520575948, 1209), (0.07791539520575948, 1108), (0.07777326253628294, 2052), (0.07777326253628294, 954), (0.07777326253628294, 812), (0.07776976305397286, 435), (0.0776760937385194, 3588), (0.07766249907323657, 1286), (0.0768120675986713, 979), (0.07633605683651937, 1618), (0.07610302341355955, 1345), (0.07610302341355955, 1329), (0.07610302341355955, 938), (0.07610302341355955, 754), (0.07610302341355955, 502), (0.07493632088882671, 1281), (0.07486036923106522, 3114), (0.07486036923106522, 759), (0.07479212341391864, 3109), (0.07479212341391864, 1125), (0.07479212341391864, 711), (0.07479212341391864, 445), (0.07479212341391864, 25), (0.0744210537557175, 3470), (0.0744210537557175, 1928), (0.0744210537557175, 1280), (0.0744210537557175, 1243), (0.0744210537557175, 1018), (0.0744210537557175, 708), (0.0740577155592499, 726), (0.07358418456629864, 1743), (0.07309840122961515, 1219), (0.07303416054815363, 252), (0.07271877218739466, 3241), (0.07184102679685105, 121), (0.07182717404842677, 587), (0.07177500880745105, 2964), (0.07127209654963827, 120), (0.07087579658806266, 1316), (0.07069880693740326, 339), (0.07068023149332683, 2306), (0.07068023149332683, 253), (0.07065433840741699, 1033), (0.07058992698239872, 3555), (0.07043170609309371, 360), (0.07023970587779339, 1004), (0.06996622272883518, 3203), (0.06996622272883518, 1585), (0.06996622272883518, 1581), (0.06943213017724599, 1461), (0.06931620630006176, 1538), (0.06923101778513037, 1918), (0.06921851239026898, 2146), (0.06914996226685433, 2312), (0.06911832282266693, 206), (0.06905411184938241, 2212), (0.06879868009883609, 1498), (0.06879341064374712, 3369), (0.06829991205551972, 485), (0.06793743237297642, 3094), (0.06793743237297642, 2568), (0.06793743237297642, 2509), (0.06793743237297642, 1624), (0.06793743237297642, 1311), (0.06793743237297642, 830), (0.06793743237297642, 193), (0.06771012152570943, 896), (0.06760842116575855, 3518), (0.06728576906303034, 3478), (0.06728576906303034, 2945), (0.06728576906303034, 2592), (0.06728576906303034, 2586), (0.06728576906303034, 1111), (0.06726031274341861, 2835), (0.0668936390304386, 3283), (0.06683941035275678, 1153), (0.06670561404578652, 3300), (0.06670561404578652, 2952), (0.06670561404578652, 2949), (0.06670561404578652, 2890), (0.06670561404578652, 2481), (0.06670561404578652, 1070), (0.06616432949033017, 3503), (0.0660445367892345, 3515), (0.06603516959037964, 3352), (0.06603516959037964, 1701), (0.06594330439800165, 3537), (0.06529883585157686, 3418), (0.06529883585157686, 2650), (0.06466761703140791, 2780), (0.06451756127861434, 2546), (0.06412036684718043, 906), (0.06406825327936377, 393), (0.06389147199964199, 1562), (0.06389147199964199, 547), (0.06296277600534345, 1898), (0.06295775824880319, 1832), (0.06295775824880319, 1144), (0.06273519650642477, 2108), (0.06257557985594793, 2438), (0.06250818347087828, 2543), (0.06237266652354673, 1818), (0.062097593396214186, 2744), (0.06179423738961337, 925), (0.06141211575679981, 2260), (0.060781144623716234, 3570), (0.06066322422717442, 2844), (0.060370008276426454, 3338), (0.06029861224248649, 2784), (0.06029861224248649, 1986), (0.06029861224248649, 1543), (0.06005526613023247, 897), (0.05947175378230543, 1763), (0.05909565017478048, 1849), (0.05909352311701952, 1352), (0.058961537911610754, 3510), (0.058961537911610754, 3055), (0.058961537911610754, 2737), (0.058961537911610754, 2292), (0.058961537911610754, 1475), (0.058958973694166104, 3589), (0.058958973694166104, 2651), (0.058958973694166104, 1731), (0.058958973694166104, 474), (0.05832232735424722, 484), (0.05776314858186377, 2356), (0.057539284349787875, 2724), (0.057539284349787875, 2716), (0.057539284349787875, 2272), (0.057539284349787875, 1980), (0.057539284349787875, 1713), (0.057539284349787875, 1604), (0.057539284349787875, 1449), (0.057539284349787875, 75), (0.057539284349787875, 61), (0.05679666092137163, 997), (0.05679666092137163, 580), (0.056729924333008365, 2637), (0.056729924333008365, 831), (0.05655015464811865, 487), (0.05652766823426947, 2030), (0.05640871242395038, 735), (0.05614386926874535, 758), (0.0559063955350394, 3184), (0.0559063955350394, 3031), (0.0559063955350394, 2944), (0.0559063955350394, 2647), (0.0559063955350394, 2528), (0.0559063955350394, 2261), (0.0559063955350394, 1269), (0.0559063955350394, 1109), (0.0559063955350394, 554), (0.055648783133438656, 1407), (0.055648783133438656, 586), (0.055646362986584645, 1466), (0.05516895684450839, 2793), (0.054882824548443114, 3173), (0.054882824548443114, 2982), (0.054882824548443114, 2845), (0.054882824548443114, 2542), (0.054882824548443114, 1756), (0.054882824548443114, 899), (0.054882824548443114, 681), (0.054882824548443114, 285), (0.05486846085587583, 3026), (0.05484348857927268, 3469), (0.05484348857927268, 3465), (0.05484348857927268, 2989), (0.05484348857927268, 2879), (0.05484348857927268, 2754), (0.05484348857927268, 2406), (0.05484348857927268, 1730), (0.05484348857927268, 1667), (0.05484348857927268, 1665), (0.05484348857927268, 1615), (0.05484348857927268, 1471), (0.05484348857927268, 1191), (0.05484348857927268, 1100), (0.05484348857927268, 1079), (0.05484348857927268, 851), (0.05484348857927268, 458), (0.054487189903820656, 2204), (0.05430643890657519, 138), (0.054058095179772864, 944), (0.054022724280419834, 3212), (0.05402202766157935, 1762), (0.0533750821753445, 1695), (0.05306742820929221, 2192), (0.05276529397122469, 2809), (0.05276529397122469, 2057), (0.05276529397122469, 1691), (0.0523101884772147, 1923), (0.05215992194245664, 1550), (0.052012698561421676, 2377), (0.052012698561421676, 1984), (0.052012698561421676, 1039), (0.052012698561421676, 149), (0.05179923233388806, 3394), (0.05179923233388806, 846), (0.051762106456659567, 2863), (0.051472967058906464, 1912), (0.051217574472353734, 2778), (0.05110058750877395, 2041), (0.050966072229566624, 1630), (0.05077445418126315, 3336), (0.05077445418126315, 2656), (0.05077445418126315, 2570), (0.05077445418126315, 2522), (0.05077445418126315, 2121), (0.05077445418126315, 2045), (0.05077445418126315, 2010), (0.05077445418126315, 1726), (0.05077445418126315, 1655), (0.05077445418126315, 1509), (0.05077445418126315, 1507), (0.05077445418126315, 732), (0.05077445418126315, 524), (0.05077445418126315, 422), (0.05077445418126315, 196), (0.05077445418126315, 171), (0.05077445418126315, 99), (0.050539089094784995, 100), (0.05053664694441642, 3568), (0.050492996404728846, 2961), (0.050036275129686114, 396), (0.05003297559110967, 2029), (0.04984891386860787, 917), (0.04961138883973778, 143), (0.04958598961322054, 3056), (0.049575831047697035, 1283), (0.048727246055628254, 2248), (0.04872402143683834, 1101), (0.047921690809483164, 1603), (0.047921690809483164, 1321), (0.047921690809483164, 1017), (0.04791436558471306, 2622), (0.04791436558471306, 1241), (0.04770759660795472, 717), (0.04694083335282966, 289), (0.04677698779883914, 2757), (0.04666642228091436, 2141), (0.04647288993080121, 2484), (0.04637599875831547, 2512), (0.045897622985651414, 139), (0.045799053702209366, 1344), (0.04570679344810075, 2147), (0.04516689835379344, 1172), (0.04492238369553648, 1149), (0.04452311366966795, 3051), (0.04452311366966795, 2743), (0.04452311366966795, 2535), (0.04452311366966795, 1784), (0.04452311366966795, 1780), (0.04452311366966795, 1511), (0.04452311366966795, 1245), (0.04452311366966795, 520), (0.04431291167953588, 3124), (0.04431291167953588, 2471), (0.04431291167953588, 2230), (0.04431291167953588, 1380), (0.04379233051095153, 3250), (0.04370894283540041, 2372), (0.04342726715205229, 477), (0.04339655458297072, 1349), (0.04289303347380985, 1420), (0.04209795683366029, 2573), (0.04202158194623495, 2401), (0.04202158194623495, 1294), (0.0419602893158302, 3107), (0.04182319015766572, 3536), (0.04182319015766572, 2839), (0.04182319015766572, 2765), (0.04182319015766572, 1597), (0.04182319015766572, 475), (0.04120970047603527, 2595), (0.04118433740575306, 2043), (0.0410712841055371, 3050), (0.04099650527849089, 1442), (0.04049252050189614, 1797), (0.04024671694278536, 1938), (0.040121670241939675, 3222), (0.040121670241939675, 2395), (0.040121670241939675, 1682), (0.040121670241939675, 1195), (0.040121670241939675, 844), (0.040121670241939675, 294), (0.040056704625578095, 1482), (0.03984593632828475, 2878), (0.03939272315558761, 3076), (0.0390809923939327, 405), (0.038398821000790784, 3545), (0.03814509952790994, 1161), (0.03801064136229472, 321), (0.03786743367501903, 468), (0.037566983103707936, 2969), (0.03725182003350967, 1723), (0.037238370858689175, 2767), (0.03696154627105317, 3205), (0.03679414699666566, 864), (0.036268027377411485, 2345), (0.035970320215902754, 3057), (0.034899356647054146, 3270), (0.03454974613444602, 2350), (0.034383816197631145, 2436), ...] 마지막으로 index를 단어로 변환하여 \"긍정 키워드 리스트\"와 \"부정 키워드 리스트\"의 Top 20 단어를 출력해볼게요. 12invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()}invert_index_vectorizer {2866: '집중', 3588: '휴식', 2696: '제공', 2311: '위치', 1584: '선정', 790: '또한', 2927: '청소', 2925: '청결', 1527: '상태', 2392: '이상', 3022: '침대', 2388: '이불', 3021: '침구', 299: '교체', 2013: '어메니티', 1296: '보강', 1277: '베스트', 2299: '웨스턴', 3564: '회원', 185: '경우', 106: '객실', 3009: '층수', 2234: '요청', 2606: '적극', 1188: '반영', 2837: '지인', 1629: '소개', 2910: '처음', 611: '당황', 1607: '세면', 675: '도구', 2555: '잠옷', 3358: '필수', 361: '그것', 2673: '정도', 578: '다음', 2074: '여기', 1171: '박만', 2595: '저녁', 981: '맥주', 3414: '한잔', 838: '렌트', 791: '뚜벅', 1159: '바로', 1247: '버스', 2676: '정류', 697: '도착', 24: '가방', 2487: '일찍', 2685: '정비', 1225: '방이', 2500: '입실', 2038: '업그레이드', 2849: '직원', 2680: '정말', 1148: '바다', 2623: '전망', 2636: '전일', 3425: '함덕', 624: '대명', 3091: '콘도', 1861: '실내', 1384: '분위기', 1659: '손님', 40: '가장', 1241: '배치', 651: '대해', 634: '대응', 1889: '써비스', 2730: '조식', 1351: '부분', 1838: '신경', 1922: '아주', 3208: '특급', 3191: '트랜디', 210: '고민', 3593: '흔적', 2082: '여름', 1700: '수영장', 1483: '사용', 1297: '보고', 2035: '엄마', 2: '가격', 627: '대비', 2769: '주위', 924: '마트', 1826: '식당', 1816: '시장', 1217: '방문', 2361: '의사', 726: '동안', 2695: '정해진', 3580: '휘슬', 269: '공항', 1539: '생각', 1809: '시설', 1028: '모두', 3017: '친절', 1029: '모드', 917: '마지막', 900: '마무리', 3156: '테라스', 2612: '전경', 2427: '인근', 2583: '재래시장', 1979: '야시장', 2402: '이용도', 2400: '이용', 2488: '일차', 3581: '휘슬락', 2940: '체크', 2761: '주변', 2094: '여친', 2975: '추억', 26: '가성', 2977: '추천', 3575: '후회', 2145: '예전', 372: '그랜드', 2605: '저희', 1918: '아이', 708: '돌잔치', 581: '다정', 3470: '했었더랬', 2642: '전통', 1004: '메종', 387: '글래드', 1563: '서비스', 969: '매우', 3428: '합리', 635: '대의', 1018: '명절', 1721: '숙박', 1928: '아티', 1243: '백미', 3061: '커피', 1280: '베이커리', 43: '가족', 3342: '플러스', 2816: '지금', 2257: '우선', 2664: '접근성', 1671: '쇼핑', 940: '만족도', 2957: '최고', 1078: '무엇', 423: '기억', 2251: '우리', 2902: '찾기', 300: '교통', 2418: '이틀', 2691: '정원', 109: '거기', 160: '겨울', 3016: '친구', 541: '놀러와', 3185: '투숙', 1981: '야외', 1235: '방향', 1976: '야간', 2726: '조명', 1723: '순간', 638: '대접', 1868: '실명', 112: '거론', 3337: '프론트', 2378: '이름', 1938: '안나', 416: '기분', 1037: '모습', 2921: '첫날', 1761: '스타트', 576: '다시', 1578: '선물', 3032: '카운터', 1939: '안내', 777: '디너', 438: '기회', 1121: '미니바', 1067: '무료', 2959: '최근', 281: '관광지', 1719: '숙고', 88: '강추', 1977: '야경', 2475: '일부러', 2476: '일어나서', 714: '동네', 1168: '바퀴', 990: '먹방', 1843: '신라', 3105: '퀄리티', 1024: '모녀', 2711: '제주시', 2313: '위해', 1586: '선택', 2180: '오픈', 2143: '예약', 622: '대로', 115: '거리', 657: '더군다나', 1627: '셔틀버스', 2271: '운행', 670: '데스크', 2880: '차안', 1374: '분도', 2438: '인상', 3201: '트윈', 1489: '사이즈', 939: '만족', 1758: '스타', 45: '가짓수', 558: '느낌', 3538: '화장실', 505: '내부', 2455: '인테리어', 2859: '질적', 2368: '이건', 100: '개인', 3005: '취향', 321: '구비', 273: '과일', 480: '나이프', 3295: '포크', 1364: '부탁', 2786: '준비', 3305: '표방', 1046: '모텔', 165: '결론', 3577: '훌륭', 1799: '시간', 1737: '슈페리어킹룸', 3376: '하루', 3570: '후기', 2836: '지은지', 2033: '얼마', 136: '건물', 1985: '약간', 2318: '유럽', 1760: '스타일', 378: '그림', 2715: '조각', 1323: '복도', 2633: '전시', 2630: '전부', 3039: '카펫', 770: '등급', 567: '다른', 2394: '이서', 2512: '자기', 2722: '조금', 2218: '외투', 2206: '완전', 2856: '진짜', 1626: '셔틀', 1127: '미리', 1821: '시티', 2162: '오름', 570: '다만', 680: '도로', 39: '가인', 276: '관계', 2876: '차량', 1638: '소리', 574: '다소', 1215: '방도', 3273: '편의점', 3027: '칫솔', 709: '동계', 3576: '훈련', 795: '라마', 2116: '연인', 2641: '전체', 1493: '사진', 2920: '첨부', 2064: '엘리베이터', 1873: '실시간', 1440: '비행기', 3151: '택시', 1128: '미만', 2374: '이동', 9: '가능', 2961: '최상', 2235: '욕실', 1791: '슬리퍼', 2304: '위생', 2123: '염려', 1590: '설날', 788: '떡국', 840: '려고', 3495: '현장', 167: '결재', 3559: '황스', 841: '려운', 2789: '중국', 932: '만두', 2744: '종류', 1293: '별로', 316: '구만', 996: '메뉴', 603: '답변', 2709: '제일', 2779: '주차', 131: '걱정', 3423: '할아버지', 1945: '안심', 3067: '컨디션', 1095: '문어', 2909: '처리', 3561: '회사', 2988: '출장', 1567: '서울', 64: '간혹', 2386: '이벤트', 3460: '해주시', 1344: '부대', 3435: '항상', 855: '롯데', 1434: '비지니스', 2097: '여행객', 1664: '손색', 2236: '욕조', 175: '겸비', 1459: '사계절', 1715: '수풀', 2794: '중문', 3306: '표선등', 2281: '원거리', 2101: '여행지', 1322: '복귀', 1805: '시내', 2323: '유명', 2462: '일단', 2781: '주차장', 2844: '지하', 246: '공간', 2356: '응대', 1097: '문의사항', 1850: '신지', 71: '감동', 2106: '역시', 2566: '장도', 807: '락타', 1550: '샤워', 896: '마련', 3089: '코인', 1620: '세탁실', 2167: '오션', 994: '멀리', 497: '남자친구', 665: '덕분', 2385: '이번', 3331: '프런트', 1239: '배정', 2415: '이착륙', 3120: '클리닝', 382: '근무', 1923: '아주머니', 2436: '인사', 3085: '코로나', 1496: '사태', 535: '노화', 1286: '벽지', 2756: '주름', 2814: '지고', 3133: '타일', 4: '가구', 3082: '코너', 243: '곳곳이', 279: '관광객', 2449: '인지', 1463: '사람', 1369: '북적', 2627: '전반', 284: '관리', 3274: '편이', 3346: '피드백', 3276: '편입', 686: '도시', 278: '관광', 3214: '특화', 2834: '지역', 385: '근처', 3034: '카페', 6: '가기', 221: '고유', 3210: '특색', 1768: '스테이', 10: '가도', 1759: '스타벅스', 1263: '번화가', 1924: '아침', 3057: '커서', 1224: '방음', 988: '먹거리', 588: '단점', 797: '라면', 2170: '오심', 2411: '이중', 3523: '혼자', 157: '겐찮은듯', 563: '다그', 694: '도일', 456: '께빵', 2793: '중국인', 3127: '타고', 556: '눈앞', 44: '가지', 3598: '힐링', 3354: '피트니스', 3254: '패키지', 1122: '미닫이', 847: '로비', 1421: '비롯', 1030: '모든', 52: '각종', 1103: '물건', 1926: '아침식사', 1207: '밥맛', 3349: '피아노', 2118: '연주', 867: '룸타입', 3250: '패밀리', 121: '거실', 3412: '한실', 1562: '서부', 1690: '수산시장', 1534: '새벽', 182: '경매', 304: '구경', 2972: '추가', 2022: '어차피', 3496: '현재', 1090: '묵고', 2453: '인터넷', 2840: '지정', 1952: '안해', 3330: '프런터', 1096: '문의', 2135: '영화', 1810: '시스템', 3219: '티브이', 2903: '채널', 1020: '몇개', 3092: '콘센트', 3341: '플러그', 2201: '와이프', 2995: '충전', 168: '결정', 594: '달라', 190: '경험', 2579: '장점', 2439: '인생', 2966: '최악', 2797: '중심', 1153: '바닷가', 1413: '비교', 3275: '편임', 3408: '한번', 2146: '예정', 3585: '휴가', 1750: '스위트룸', 2322: '유리창', 2312: '위트', 3050: '캠핑', 3160: '테이블', 1621: '세트', 3161: '텐트', 2054: '에어컨', 2543: '작은방', 975: '매트', 2520: '자리', 3142: '탑동', 263: '공원', 3338: '프리', 923: '마켓', 259: '공연', 971: '매일', 1833: '식사', 282: '관덕정', 2682: '정문', 684: '도보', 1645: '소요', 3458: '해장국', 717: '동문', 1560: '서문시장', 1047: '목관', 960: '맞은편', 1792: '슬슬', 1291: '별관', 1330: '본관', 68: '갈수', 2965: '최신', 377: '그린', 3545: '환경', 787: '때문', 1292: '별도', 641: '대중교통', 317: '구매', 1882: '심플', 1438: '비품', 3205: '트윈침대', 1763: '스탠다드', 548: '높이', 3404: '한라산', 525: '노곤', 3152: '터미널', 2189: '온돌룸', 1608: '세면대', 1114: '물이', 3013: '치약', 150: '것임', 1545: '생수', 3550: '환승', 231: '곧바로', 710: '동광양', 2677: '정류장', 1818: '시청', 1771: '스텝', 3502: '협소하', 130: '거품', 3131: '타월', 3537: '화장', 612: '대가', 123: '거울', 1670: '쇼파', 2739: '조합', 395: '금액', 2780: '주차공간', 267: '공터', 3150: '태풍', 104: '개층', 2184: '오후', 3409: '한시', 1485: '사우나', 1349: '부모님', 3422: '할머니', 1038: '모시', 197: '계획', 2464: '일로', 1445: '빠듯해', 2004: '어디', 441: '길가', 235: '골목길', 2377: '이륙', 2887: '착륙', 1310: '보이', 363: '그냥', 2469: '일반', 633: '대욕', 3489: '헬스장', 19: '가면', 206: '고려', 837: '렌터카', 2496: '입구', 1182: '반대쪽', 3416: '한정', 591: '단체', 660: '더블', 2708: '제외', 2088: '여유', 1183: '반대편', 688: '도심', 1387: '불구', 1648: '소음', 125: '거의', 1609: '세명', 3199: '트리플', 2618: '전날', 495: '남아', 2534: '자체', 610: '당일', 2683: '정보', 3468: '햇반', 2580: '장조림', 2637: '전자', 831: '레인지', 33: '가야', 1610: '세미나', 3400: '한국인', 369: '그대로', 469: '나름', 2697: '제과점', 2670: '정거장', 1022: '모기', 944: '만해', 898: '마리', 16: '가량', 1222: '방안', 2058: '에프킬라', 1435: '비치', 195: '계속', 1456: '뿌리', 2557: '잡고', 2891: '찬장', 2915: '천장', 242: '곳곳', 3008: '측은', 1265: '벌레', 414: '기본', 2223: '요금', 135: '건너편', 1262: '번호', 1538: '샐러드', 2349: '음식', 326: '구성', 2629: '전복죽', 2601: '저번', 3464: '핸드폰', 2996: '충전기', 1400: '불편', 1622: '센터', 256: '공사', 1720: '숙면', 1061: '무난', 528: '노력', 2826: '지불', 1430: '비용', 845: '로맨틱', 702: '독립', 1368: '북유럽', 2321: '유리', 3059: '커튼', 1145: '바깥', 1595: '설치', 1025: '모던', 1418: '비데', 3569: '효율', 2451: '인치', 1512: '삼성', 927: '만끽', 3270: '편안함', 2091: '여정', 1151: '바닥', 1052: '목적', 3060: '커플', 464: '끼리', 2852: '직진', 2268: '운전', 261: '공영', 1962: '애기', 1074: '무선인터넷', 80: '갑자기', 1568: '서전', 3432: '항공', 562: '다가', 5: '가급', 169: '결제', 2342: '은방', 1533: '상황', 1914: '아시', 227: '고트', 2822: '지도', 1614: '세심', 257: '공시', 1815: '시작', 982: '맥주잔', 3102: '쿠폰', 2463: '일도', 3108: '크기', 864: '룸서비스', 2690: '정신', 3277: '평가', 3272: '편의', 2233: '요즘', 2078: '여러', 2848: '직언', 1412: '블룸', 1288: '변기', 1115: '물질', 1997: '얘기', 1675: '수건', 3548: '환불', 2293: '월일', 3313: '푸른', 3223: '파도', 2918: '철썩', 3320: '풍경', 1544: '생선회', 2573: '장소', 543: '놀이', 1989: '양도', 735: '돼지', 2613: '전골', 970: '매운탕', 3512: '형편', 1406: '브런치', 751: '뒤쪽', 956: '맛집', 2869: '쭈욱', 79: '갑인', 764: '드타', 1233: '방파제', 3567: '횟집', 3129: '타운', 3448: '해산물', 356: '규모', 7: '가까이', 2008: '어르신', 454: '깨끗', 629: '대신', 2538: '작고', 1697: '수영', 1912: '아쉬움', 2430: '인도', 1634: '소독약', 1745: '스비', 3525: '홀로', 775: '등정후', 1365: '부터', 3520: '혹시', 3543: '확인', 2645: '전화', 3180: '통해', 3582: '휘트니', 2266: '운동복', 1698: '수영모', 219: '고오', 1157: '바람', 631: '대여', 2093: '여직원', 1077: '무시', 951: '말투', 2284: '원래', 2700: '제로', 2267: '운영', 2470: '일반인', 2503: '입장', 1583: '선심', 3360: '필요', 430: '기전', 620: '대뜸', 3528: '화가', 200: '고객', 3343: '플레인', 1800: '시경', 1917: '아웃', 3382: '하야', 998: '메리어트', 2081: '여럿', 2397: '이어도', 1707: '수준', 2370: '이군', 1867: '실망', 2747: '종일', 880: '리셉션', 3070: '컨시어', 3147: '태도', 2319: '유료', 1460: '사고', 3393: '학생', 1580: '선생님', 2005: '어딘', 1498: '사항', 1075: '무슨', 128: '거지', 879: '리빙룸', 3368: '하나요', 3202: '트윈룸', 1065: '무려', 156: '게재', 1869: '실물', 2881: '차이', 1879: '실화', 3231: '파우더', 240: '곰팡이', 2511: '자국', 2539: '작동', 233: '골드스타', 517: '냉장고', 2649: '절대', 826: '레스토랑', 2513: '자꾸', 2687: '정색', 1510: '살짝', 1561: '서버', 669: '데리', 32: '가신', 470: '나머지', 1105: '물기', 2851: '직접', 2931: '체계', 2048: '엉망', 1630: '소규모', 1803: '시기', 910: '마음', 886: '리트', 794: '라그', 409: '기대', 1862: '실내수영장', 1701: '수온', 1749: '스위트', 1427: '비수', 1515: '상대', 536: '노후', 2669: '정가', 1072: '무리', 3424: '할인', 3473: '행사', 2893: '참고', 3298: '포함', 913: '마일리지', 650: '대한항공', 3377: '하룻밤', 3542: '확실', 2115: '연식', 201: '고급', 2280: '원가', 1566: '서우', 3453: '해수욕장', 3402: '한눈', 2900: '창문', 161: '겨울철', 2187: '온도', 870: '리기', 1214: '방기', 725: '동시', 1221: '방식', 621: '대략', 695: '도정', 1352: '부스', 2137: '옆방', 3220: '티비', 3001: '취침', 2946: '초등학생', 1996: '양호', 3411: '한식당', 2875: '차라리', 1184: '반드시', 3107: '크게', 1896: '씨유', 674: '델문', 3035: '카페나', 3452: '해수욕', 2969: '최적', 778: '디럭스', 1203: '발코니', 1034: '모로', 3203: '트윈룸입니', 1585: '선착순', 2142: '예술', 1106: '물놀이', 1031: '모래', 3352: '피크', 1817: '시즌', 491: '날씨', 1581: '선선', 381: '근래', 3269: '편리', 3086: '코르', 434: '기지', 1481: '사양', 2782: '주체', 302: '교회', 2653: '절반', 2440: '인수', 1314: '보임', 1927: '아트', 2976: '추정', 3333: '프로', 3264: '페셔', 2432: '인력', 976: '매트리스', 472: '나무', 3482: '허리', 1934: '안감', 3447: '해변', 2275: '워낙', 31: '가시', 2757: '주말', 3397: '한국', 1270: '벚꽃', 403: '기간', 3503: '협재', 2197: '옹포', 1209: '밥집', 1505: '산책', 860: '루프', 1425: '비바람', 1396: '불어', 1410: '블로거', 1108: '물떄', 360: '그거', 2308: '위주', 3317: '풀이', 2192: '온수', 1133: '미온수', 1132: '미온', 2536: '자쿠지', 2551: '잠깐', 2458: '인피니트', 3549: '환상', 3365: '하나', 1782: '스페', 3015: '치킨', 1446: '빠에야', 1779: '스파', 333: '구조', 1780: '스파룸', 2535: '자쿠', 2401: '이용권', 3371: '하니', 1487: '사이', 1820: '시트', 2678: '정리', 1511: '삼다수', 1294: '병과', 520: '네스프레소', 3051: '캡슐', 2089: '여자', 3347: '피로', 1784: '스페인', 2132: '영업', 2743: '종료', 1245: '버거', 2161: '오른쪽', 893: '마담', 482: '나탈리', 2351: '음악', 3362: '필터', 1875: '실외수영장', 147: '검색', 2186: '옥상', 164: '결과', 653: '대형', 942: '만큼', 1260: '번잡', 98: '개수대', 2407: '이전', 1909: '아무', 884: '리지', 2714: '제한', 189: '경치', 2403: '이웃', 2467: '일몰', 1877: '실제', 883: '리조트', 3527: '홍보', 1529: '상통', 3532: '화산', 1166: '바위', 2686: '정상', 431: '기점', 1770: '스템', 2556: '잠자리', 435: '기타', 1359: '부족함', 444: '길이', 582: '다행', 2792: '중국어', 2131: '영어', 692: '도움', 3438: '해결', 1053: '목적지', 3461: '해피', 2346: '음료', 1488: '사이다', 2764: '주스', 592: '달걀', 3014: '치즈', 3597: '히터', 3187: '투어', 3515: '호스텔', 2858: '질문', 1530: '상품', 270: '과거', 154: '게스트하우스', 2510: '자고', 2150: '오기', 2479: '일이', 1201: '발전', 2141: '예상', 3165: '토스터', 2424: '이후', 1635: '소등', 1399: '불키', 2733: '조용조', 1232: '방키', 218: '고여', 496: '남자', 1946: '안전', 2878: '차로', 2373: '이내', 1403: '뷔페', 2076: '여느', 856: '롯데리아', 1216: '방만', 224: '고정', 1060: '무궁화', 1676: '수기', 1490: '사이트', 3526: '홈페이지', 921: '마치', 1088: '무척', 2617: '전기차', 134: '건너', 402: '급속', 1628: '소가', 2333: '유치원', 2784: '주택가', ...} 12for coef in coef_pos_index[:20]: print(invert_index_vectorizer[coef[1]], coef[0]) 방문 1.2644550507381787 이용 0.9079356150239053 바다 0.895609472071521 조식 0.8859075267474583 가족 0.8795111499693716 가성 0.8541915649753757 다음 0.8362541212560809 최고 0.7714811231976703 사장 0.7375280889735719 맛집 0.7203390936359615 추천 0.6503260268852225 거리 0.6488836121942877 마음 0.6467914172687944 바로 0.6264469987695738 리조트 0.5943145305412955 출장 0.5505354129422678 도움 0.5294632094678557 인테리어 0.5240729254152497 아주 0.5207834696883535 의사 0.5153917648445299 12for coef in coef_neg_index[:20]: print(invert_index_vectorizer[coef[1]], coef[0]) 예약 -0.9945592515966041 냄새 -0.9190158099937462 다른 -0.8672956005075485 침대 -0.7519681298547074 보이 -0.7201222787741572 최악 -0.7142499739127354 에어컨 -0.6786616478611768 별로 -0.6742178511586063 찾기 -0.6584721911054098 취소 -0.6464141509409321 사람 -0.6451323735594592 정도 -0.6240099604615805 사진 -0.6089303470147718 대부분 -0.5889712626646347 다시 -0.5601302753897155 대해 -0.5518124209379022 노후 -0.5484791097700695 느낌 -0.5423970967095598 필요 -0.5413974621071783 문제 -0.5287746123667489 키워드를 살펴보면: 이용객들이 보통 제주 호텔의 바다뷰 혹은 바다 접근성, 주변 맛집 그리고 인테리어 등에 만족하는 것으로 보입니다. 하지만 숙소의 냄새 그리고 침대, 에어컨 등 시설의 상태가 많이 아쉬워 보이고 개선이 필요해보입니다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【EXERCISE】","slug":"【EXERCISE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/"},{"name":"Python","slug":"【EXERCISE】/Python","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Text Mining","slug":"Text-Mining","permalink":"https://hyemin-kim.github.io/tags/Text-Mining/"}]},{"title":"【실습】 Python >> Text Mining -- Word Cloud & TF-IDF 중요도 분석 (영화 시나리오)","slug":"E-Python-TextMining-1","date":"2020-08-20T12:58:32.394Z","updated":"2020-11-22T08:38:21.356Z","comments":true,"path":"2020/08/20/E-Python-TextMining-1/","link":"","permalink":"https://hyemin-kim.github.io/2020/08/20/E-Python-TextMining-1/","excerpt":"","text":"【Text Mining 실습】-- 영화 시나리오: Word Cloud &amp; 단어 중요도(TF-IDF) 분석 0. 개요 1. Library &amp; Data Import 2. 데이터셋 살펴보기 2-1. 기본 정보 탐색 3. 텍스트 데이터 전처리 3-1. 정규 표현식 적용 3-2. Word Count (1) 말뭉치(코퍼스) 생성 (2) BoW (Bag of Words) 벡터 생성 (3) 단어 분포 탐색 4. 택스트 마이닝 4-1. 단어별 빈도 분석 (+ Word Cloud) (1) 상위 빈도수 단어 출력 (2) Word Cloud 시각화 4-2. 장면별 중요 단어 시각화 (TF-IDF) (1) TF-IDF 변환 (2) “벡터” - “단어” mapping (3) 중요 단어 추출 - Top 3 TF-IDF 0. 개요 영화 시나리오 데이터를 활용해 2가지의 Text Mining 분석을 진행합니다. 단어별 빈도 분석 (Word Cloud 산출) 장면별 중요 단어 시각화 (TF-IDF 분석) 1. Library &amp; Data Import &gt;&gt; 사용할 Library 123456789%matplotlib inlineimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.filterwarnings(\"ignore\") &gt;&gt; 사용할 데이터셋 영화 \"The Bourne Supermacy\"의 시나리오를 활용하겠습니다. Link (pdf파일) 1df = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/bourne_scenario.csv\") 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } page_no scene_title text 0 1 1 EXT. MERCEDES WINDSHIELD -- DUSK 1 It's raining... ... 1 1 A1 INT. MERCEDES -- NIGHT A1 On his knee -- a syringe an... 2 1 2 INT. COTTAGE BEDROOM -- NIGHT 2 BOURNE'S EYES OPEN! -- panic... 3 1 A2 INT. COTTAGE LIVING AREA/BATHROOM ... A2 BOURNE moving for the medic... 4 2 3 INT./EXT. COTTAGE LIVING ROOM/VERA... 3 One minute later. BOURNE mo... &gt;&gt; Feature Description page_no: 데이터가 위치한 pdf파일의 페이지 수 scene_title: 씬 제목 text: 씬에 해당하는 지문/대본 텍스트 정보 2. 데이터셋 살펴보기 2-1. 기본 정보 탐색 12# dimensiondf.shape (320, 3) 12# 결측치df.isnull().sum() page_no 0 scene_title 0 text 0 dtype: int64 12# informationdf.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 320 entries, 0 to 319 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 page_no 320 non-null int64 1 scene_title 320 non-null object 2 text 320 non-null object dtypes: int64(1), object(2) memory usage: 7.6+ KB 12# text 변수 확인df['text'][0] \" 1 It's raining... Light strobes across the wet glass at a rhythmic pace... Suddenly -- through the window a face -- JASON BOURNE -- riding in the backseat -- his gaze fixed. \" \"text\"내용을 확인해보면, 여기에 우리가 필요없는 내용들이 포함되어있습니다: 맨 앞에 있는 씬 번호, 공백, 특수 문자 등. 이들을 제거하는 전처리 과정이 필요해보입니다. 또한, Text mining을 진행할 때, 대소분자의 구분이 의미가 없습니다. 따라서, 대문자를 소문자로 변환하는 작업도 함계 진행하겠습니다. 3. 텍스트 데이터 전처리 3-1. 정규 표현식 적용 1df['text'][0] \" 1 It's raining... Light strobes across the wet glass at a rhythmic pace... Suddenly -- through the window a face -- JASON BOURNE -- riding in the backseat -- his gaze fixed. \" 12345678910# 정규 표현식 함수 정의import redef apply_regular_expression(text): text = text.lower() # 대문자 -&gt; 소문자 변환 english = re.compile('[^ a-z]') # 영어 추출 규칙: 띄어 쓰기를 포함한 알파벳 result = english.sub('', text) # 위에 설정한 \"english\"규칙을 \"text\"에 적용(.sub)시킴 result = re.sub(' +', ' ', result) # 2개 이상의 공백을(' +') 하나의 공백(' ')으로 바꿈 return result 만들어 놓은 정규 표현식을 \"text\"의 첫번째 데이터에 적용해보면: 1apply_regular_expression(df['text'][0]) ' its raining light strobes across the wet glass at a rhythmic pace suddenly through the window a face jason bourne riding in the backseat his gaze fixed ' 우리의 예상대로 소문자만 존재하고, 공백과 특수문자가 모두 제거됐습니다. 그럼 이제 이 규칙을 전체 데이터셋에 적용해볼게요. 123# 정규 표현식 적용df['processed_text'] = df['text'].apply(lambda x: apply_regular_expression(x))df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } page_no scene_title text processed_text 0 1 1 EXT. MERCEDES WINDSHIELD -- DUSK 1 It's raining... ... its raining light strobes across the wet glas... 1 1 A1 INT. MERCEDES -- NIGHT A1 On his knee -- a syringe an... a on his knee a syringe and a gun the eyes of... 2 1 2 INT. COTTAGE BEDROOM -- NIGHT 2 BOURNE'S EYES OPEN! -- panic... bournes eyes open panicked gasping trying to ... 3 1 A2 INT. COTTAGE LIVING AREA/BATHROOM ... A2 BOURNE moving for the medic... a bourne moving for the medicine cabinet digs... 4 2 3 INT./EXT. COTTAGE LIVING ROOM/VERA... 3 One minute later. BOURNE mo... one minute later bourne moves out onto the ve... 3-2. Word Count (1) 말뭉치(코퍼스) 생성 123# make corpuscorpus = df['processed_text'].tolist()corpus [' its raining light strobes across the wet glass at a rhythmic pace suddenly through the window a face jason bourne riding in the backseat his gaze fixed ', ' a on his knee a syringe and a gun the eyes of the driver jarda watching bournes pov the passenger back of his head cell phone rings the head turns its conklin bourne returns his stare ', ' bournes eyes open panicked gasping trying to stay quiet marie sleeps ', ' a bourne moving for the medicine cabinet digs through the medicine cabinet downs something specific ', ' one minute later bourne moves out onto the veranda marie pads in watching him for a moment concerned clearly its not the first time this has happened they both look different than last we saw them his hair is longer shes a blonde hippie travelers their cottage is humble but sweet the bedroom opens to a beach and a town just down the hill club music from some all night rave wafting in from the far distance marie where were you jason bourne in the car conklin up front marie ill get the book bourne no theres nothing new marie youre sure he nods we should still we should write it down bourne two years were scribbling in a notebook marie it hasnt been two years bourne its always bad and its never anything but bits and pieces anyway shes gone quiet you ever think that maybe its just making it worse you dont wonder that she lays her hands on his shoulders steadies him marie we write them down because sooner or later youre going to remember something good bourne softens i do remember something good all the time i remember you she smiles kisses him leads him back in ', ' marie getting bourne into the bed turning down the light getting him settled waiting for that pill to kick in what would he do without her bourne im trying marie okay marie i worry when you get like this bourne its just a nightmare marie i dont mean that i worry when you try to ignore it he hesitates but that gets him he knows shes right and with that opening hes letting go resistance folding almost childlike shes gathering him in hes letting her do it marie contd sleep sleep now bourne i should be better by now marie you are better and i think its not memories at all its just a dream you keep having over and over bourne but it ends up the same marie one day it will be different it just takes time beat well make new memories you and me silence she strokes his face he gives in to her tenderness hes fading two waifs in the dark ', ' bourne running in the sun a punishing pace along the sand moving strong effortless deep into it focused the stunning conjunction of sun and scenery are lost on him ', ' a busy market town fishing town hippie town lots of young western faces rundown and happening at the same time marie shopping filling a bag with local produce ', ' bourne still running leaving the beach behind ', ' marie back from the market putting the groceries away almost done when she stops for a moment a photograph there on the windowsill a snapshot jason and marie on a beach her arms around him as if she were the protector big smiles young alive in love marie smiles ', ' funky busy colonial facades in vivid subcontinental technicolor loud morning traffic camera finds bourne coming out of a store with a big bottle of water hes just finished his run standing there chugging away checking the scene when something catches his eye his pov the street a silver car something newish pulling down the block cant quite see whos driving but back to bourne watching this silver car so serious hes casual nobody passing would notice but we do hes on alert moving with him as bourne follows the silver car on foot natural cruising the busy sidewalk blending into the mix chugging on that water bottle and up ahead the silver car making the corner and turning now back to bourne slowing as he reaches the corner his pov the silver car has parked theres a guy welldressed casual physical sunglasses call him kirill hes out of the car and heading across the street toward a building there a telegraph office back to bourne checking his watch the car the guy perimeter ', ' mr mohan at his desk hes a crisp proper man of fifty hes just been handed something a photograph of marie an old passport picture mr mohan and your question sir kirill across the desk kirill shes my sister theres been a death in the family this is the last place we know she called from ', ' a note on the table im at the beach bourne has just come in just read the note balling it quickly in fact everything is quickly now because bourne is bailing fast calm methodical some exfil procedure that hes honed and choreographed packing like a machine rapid time cuts backpacks thrown open on the bed house cash pulled from a lamp base credit cards taped under the counter ', ' kirill coming out of the bank mission accomplished heading back to the silver car getting in and ', ' kirill starting it up glancing around nice and easy hes cool putting the car into gear he makes a slow pass through the marketplace eyes everywhere ', ' bourne done the place is stripped pulling on the backpacks glancing around one last thing shit he almost missed it the photograph the one of he and marie on the beach the one we saw her looking at earlier there it is on the windowsill jamming it into his pocket and ', ' a kirill now parked and out of the car on the move on foot he begins a sweep of the beach ', ' bourne out the back jogging keeping low into the neighborhood through the alleys nothing random about it this has all been worked out and ', ' crowded with tourists sunbathers marie at her favorite spot talking with two women laughing with them happy ', ' a a burly jeep comes roaring up bourne spots the silver car parks at the other end takes off towards the beach ', ' kirill methodically making his way up the beach checking every blue tent every towel ', ' bourne coming up the beach the opposite way one eye on kirill one eye on marie he arrives just as kirill looks up and sees them a hundred yards away a hard stare between them bourne bends down bourne we gotta go marie we gotta go now from the tone of his voice she knows its serious marie grabs her bag a quick goodbye to the friends they hurry off bourne uses the sunbathers as cover kirill retreats ', ' they reach the jeep she knows the drill bag tossed in the back even as the jeep pulls away and ', ' bourne driving marie beside him bourne were blown she hesitates one minute ago everything was fine marie no how bourne the telegraph office marie but we were so careful bourne we pushed it we got lazy ', ' kirill already back at the silver car following them out onto the main street blocked by the local traffic pulling a huge automatic pistol out from his travel bag ', ' the jeep pulling down this narrow little passageway and bournes windshield pov main street packed with traffic and back to bourne not liking this eyes all over trying to decide marie but youre sure bourne he was at the campground yesterday marie so bourne its wrong guy with a rental car and hundred dollar sneakers sleeps in a tent trying to decide whether to pull out or back up marie thats crazy bourne no not this this is real suddenly and hes right there throwing the car into reverse marie where bourne back there at the corner hyundai silver ', ' kirill trapped in some main street gridlock glancing back for a way out freezing suddenly because there his pov the jeep the alley right there twenty yards back a good look at bourne and marie as they disappear and ', ' the jeep backing up the way it came blowing its horn because an old van pulls in and blocks him from behind ', ' bourne leaning on the horn shit now theyve got to wait marie but youre not youre not sure bourne we cant wait to be sure marie i dont want to move againi like it here bourne look we clear out we get to the shack we get safe we hang there awhile ill come back ill check it out but right now we cant marie wheres left to go bourne theres places we cant afford to be wrong ', ' kirill calm possessed of a familiar tactical patience he cant get the hyundai to the alley from where he is and it doesnt make sense to go on foot he checks his rearview fuck it theres an opening ahead and hes taking it even though its away from them hell find another way ', ' bourne sees the hyundai move forward into traffic the old van is still blocking them from behind bourne you drive marie what bourne already squeezing over switch you drive marie where bourne make the left toward the bridge marie scrambling over the seat bourne eyes everywhere checks his watch the jeep squirts back on the main street and ', ' marie at the wheel adrenaline pumping clear running for thirty yards ahead and marie skidding them into the right turn clipping another vehicle mirror shattering speeding up bourne scanning behind them marie moving out to pass veering back an oncoming bus just in time and marie jesus glancing over is he back there bourne not yet marie its just him bourne yeah one guy i dont think he was ready marie hang on marie bearing down pulling out gives him a quick smile bourne knowing hes got a good one here ', ' kirill stopping short on a rise bit of a view from here gets half out the car to look below the jeep headed for a bridge hes gonna lose them kirills mind racing grabs duffle from the back abandons car ', ' marie driving bourne preps his pistol eye out for kirill bourne you keep going to the shack ill meet you there in an hour marie concerned where are you going bourne im going to bail on the other side and wait this bridge is the only way he can follow marie what if its not who you think it is bourne if he crosses the bridge it is marie there must be another way bourne i warned them marie i told them to leave us alone marie jason please dont do thisit wont ever be over like this bourne theres no choice her pov the old concrete bridge ahead almost there ', ' kirill slams into it quick precise grabs into the bag only a moment and hes got a sniper rifle ', ' a bourne pistol in hand spare clip in the other checks his watch bourne at the end make the left when i roll out do not slow down marie nods got it after a beat marie i love you too bourne tell me later marie looks ahead ', ' b kirill eye to the scope sniper scope pov there the jeep rumbling across the bridge no clear target just the back of the full drivers side headrest kirills finger squeezing firing ', ' the jeep jerking front fender tearing into and along the guard rail cement shards fill the air bourne reaching for the wheel too late as the jeep finally crashes through the flimsy guardrail plummets splashes hard begins to sink out of sight ', ' kirill lowers the scope takes a quick look around hes basically gone unnoticed in this little nook with his silenced rifle but people are already rushing toward the bridge then there an old woman looking directly at kirill from a doorway not quite sure what but an old indian woman in goa so what kirill drills her with a look as she sinks back inside ', ' swallowed up bourne and marie gone ', ' kirill scans the surface of the river under the bridge waiting ', ' mud plumes as the jeep settles bourne reaches over to marie tries to urge her out ', ' kirill with a killers patience waiting almost done scope pov the surface of the water unbroken kirill scans his perimeter theres the old woman again but more people with her people coming out of the woodwork kirill checks the surface one last time nothing he breaks down the rifle in moments goes ', ' bourne up into an air pocket held by the jeeps canvas top a big gulp of air and hes back to marie frantic trying to unclip her seatbelt pull her out but its all jammed up ', ' bag chucked in the back all he has left is the scope one last look to the unbroken surface then its time to go kirill drifting away disappears ', ' the red halo growing bigger blood bourne pauses maries face is blank shes dead bourne finally pulling back realizing this is goodbye ', ' we pick up a man with a briefcase on a telephoto lens teddyradio vo the seller has arrived berlin as the man comes to a chinese restaurant he stops squarely so he can be seen clearly then he enters a stark glass office building teddyradio vo contd contd hes inside ', ' two men cross the square to the chinese restaurant vic is forty steelass intel operator he carries a large samples case beside him mike younger exnavyseal ', ' the hub secure anonymous office space somewhere in the city shades drawn lots of gear cabled around the stale improvised feel of a temporary outpost four serious people alone in this room pamela landy is a senior cia counterintelligence officer hovering over the communications console cronin pamelas early forties stonecold facade quarterbacking the operation over the radio kurt and kim are the techs here his and her headphones ruggedized laptops and comm gear spread around them cronin what have you got survey one ', ' dark teddy at the window another military face radio rig night scope watching vic and mike pass below him teddyradio over hub this is survey one mobile one is in motion seller is inside and waiting ', ' vic and mike slow as they come to the same stark glass office building teddyradio over we are ready to go ', ' mike and vic shake hands two tired coworkers parting ways mike will keep walking vic entering the building through the big glass doors smiling as hes approached by a night shift security guard and we hear mike still walking alone now heading away from the glass office building toward a van parked up the block mikeradio sleeve mike earpiece this is escort one im clear ', ' the command post cronin works the communications board cronin all teams listen up we are standing by for final green turning now to pamela who has been listening just as shes about to give the final word kim raises a finger kim langley she hands pamela a phone thats patched into her board pamela a bit surprised martin ', ' three men cia mandarins sit around a round table martin marshall deputy vicedirector hes in charge all is tense marshall im here so is donnie and jack weller we understand youre using the full allocation for this buy pamela thats where we came out marshall its a lot of money pam pamela were talking raw unprocessed kgb files its not something we can go out and comparison shop marshall still pamela for a thief a mole i vetted the source marty hes real if it does nothing more than narrow the list of suspects its a bargain at ten times the price mandarin pamela jack weller here its the quality thats at issue pamela yes sir im in total agreement if theyre fakes theyre expensive furious impatient gentlemen ive got the seller on site and in play quite honestly theres not much more to talk about marshall looks to his mandarians not convinced but doesnt want to lose the opportunity time to wash his hands marshall all right pam your game your call ', ' all eyes on pamela as she puts down the phone to langley nodding to cronin yes croninradio final green you are go repeat you are go for final green ', ' vic has just passed muster with the security guard hes standing alone at an elevator bank vicradio sleeve mike earpiece on my way up vic pulling his earpiece going dark waits for an elevator ', ' a dark a small room full of wiring and infrastructure lit by the glare of someones maglight gloved hands quickly pass over racks of gear and wiring and then stopping at the main electrical risers they carefully place an explosive device no bigger than a pack of cigarettes onto the main riser done with that here comes a second small explosive device but this ones special its being taken from a plastic bag and mounted down by the floor on a subpanel done the hands hold up what looks like a piece of tape ', ' transferring it onto the charge ', ' vic alone with the samples case pressing the button for the top floor the doors close the car rises and then it stops vic bracing himself as the door opens and ivan russian the guy we saw outside with the briefcase standing in an empty darkened hallway ivan show me vic here ivan holding open the door now show now vic flips open the case cash three million dollars ', ' a glass door a suite of offices beyond clean anonymous one light on deep inside caspiexpetroleum cherbourg moscow rome tehran', ' curtains drawn lights low ivan sitting with the samples case counting the cash vic poring over russian document files dozens of kgb files old and new spread sheets financial data incomprehensibly cyrillic marked up but judging by the seals and clearance sign offs all topsecret vic this is everything ivan is there is all there suddenly music a radio some tinny pop tune just started playing from somewhere down the hall vic what the hell is that alone you said alone both of them sure theyre being doublecrossed vic contd contd reaching for his ankle who who else is here ivan no not me no other people vic coming up with a pistol shut up just shut the freaked by the gun ivan to his feet vic pushing him back as he rushes past the sample case spilling cash and wrong snapph snapph snapph snapph snapph five fast suppressed small caliber shots vic falls first ivan crashing back across a desk as the bullets tear into him both of them dead before they hit the floor and reverse to find the gloved hands unscrewing a silencer tucking away the weapon already in motion before we know whats happened pulling a climbing duffel out from his back pack stuffing in the samples case and ivans briefcase all the files all the money except wait hes left out one old kgb file cover and now he pulls a plastic bag from his backpack gloved hands carefully remove a single sheet of paper from inside the bag and this paper looks exactly like all the stuff hes just tucked away another page full of cyrillic blur hes putting this sheet of paper inside the file cover now hes slipping them both underneath the desk tossing them there as if they fell in the struggle and ', ' the electrical risers as one of the two detonation decives blows a single tidy selfcontained explosion and ', ' as the lights flicker and fail and the night shift security guard is suddenly cast into darkness and ', ' as they were waiting but only a moment before teddyradio sudden urgent hub we just we lost power the building the whole place just went dark cronin looking at pamela the first whiff of dread as cronin repeat who is dark the target building or your location radio voices piling up panicked confusion cascading as ab ', ' anonymous drone barn kirill stepping out of a car hes carrying the duffle ', ' kirill heading down the hall ', ' kirill enters its a small room gretkov is waiting hes forty professional trim and polished dominant gretkov russian youre early kirill youre complaining gretkov its clean kirill would i bring it gretkov taking over now tosses some money on the bed checks out the photocopy of the files gretkov what are you doing kirill stripping quickly kirill im taking a shower its been a long day gretkov make it fast my plane is waiting gretkov dumping three million dollars over the bed as kirill sheds his clothes and we ', ' a workmen cluster as a cable winches the jeep is raised from the river bottom as water pours off of it bourne watching from a distance empty ', ' b crime scene police blocking office workers from getting in the building media vans clogging the street pamela and cronin across the street watching the mood is black ashes pamela we need to get in there cronin im working on it pamela stands there silent staring at the disaster across the street a ', ' a bourne is bailing exfil procedure but this is a heartbroken exfil a footlocker open bournes main stash bourne going through the footlocker setting aside his work clothes other things he needs but he also has to separate a growing pile of marie memories bank cards phony student ids loose passport photos with a mix of looks and hairdos clothes vacuumpacked bags spare shoes ', ' b a gasolinestoked fire burning in a rocklined pit bourne feeding his papers and all of maries belongings into the fire a passport cover crinkles back to reveal her photo her face begins to burn gassoaked clothes tossed in nothing left except the photograph the picture of he and marie at the beach the one from his desk bourne hesitates holds the photo out to the flames the rules of exfil say drop it but he cant wont he reaches to his bag sticks the photo on top of his gear then hefting the bag bourne strides away ', ' a folding table covered with xeroxed berlin police paperwork pamela getting a showandtell from cronin and teddy cronin so there were two of these explosive charges placed on the power lines one of them failed the fingerprint pamelas got it thats from the one that didnt go off pamela and the germans cant match it teddy nobodys got it we checked every database we could access nothing cronin show her the other thing teddy this is a kgb file that mustve fallen somehow and then slipped under i guess a desk there or handing it to her pamela do we know what this says teddy yup a scrap of paper the main word there the file heading translates as treadstone pamela what the hell is a treadstone cronin shaking his head nobody knows ', ' c bourne bouncing around on an old punjab bus alone in a crush of humanity going only god knows where ', ' a pamelas pov as she drives toward the entrance cia headquarters virginia ', ' a long bright sterile hallway pamela and cronin walking briskly alongside a uniformed sps officer ', ' pamela and cronin watching the sps officer unlock the operation panel coding in they begin to descend and ', ' drab and desolate pamela and cronin come around a corner walking with a new escort officer passing a sign that reads operations library center ', ' sealed triplelocked numbered door it swings open lights flicker on tons of shit packed away in here shelves bulging boxes tapes binders hard drives pamela steps in a huge filing cabinet labeled treadstone pamelaphone over ward abbott os yes pamelaphone pamela landy a ', ' ward abbott at his desk the cluttered clubhouse hq of a man whos spent the last thirtyfive years in the spy game a picture window offers a commanders view of the bullpen abbottphone what can i do for you pam pamelaphone i was hoping you had some time for me abbottphone time for what pamelaphone im free right now actually abbottphone that sounds ominous let me check my schedule abbott holds the phone eyes drifting out the window and abbotts pov the bullpen cronin is standing with daniel zorn one of abbotts trusted s clearly zorn is getting the less polite version of pamelas invitation zorn managing to shoot a quick questioning glance to abbott as ', ' a cold room desk two chairs abbott and pamela alone pamela treadstone abbott never heard of it pamela thats not gonna fly abbott with all due respect pam i think you mightve wandered a little past your pay grade she has a piece of paper she slides it forward pamela thats a warrant from director marshall granting me unrestricted access to all personnel and materials associated with treadstone abbott rocked and trying to hide it abbott and what are we looking for pamela i want to know about treadstone abbott to know about it almost amused it was a kill squad black on black closed down two years ago more abbott contd nobody wants to know about treadstone not around here the warrant you better take this back to marty and make sure he knows what youre doing pamela trump card he does ive been down to the archives i have the files ward ', ' a a hard working port a big mediterranean ferry coming in naples ferry bourne at the rail unchanged from india staring ahead as europe looms ', ' b bourne disembarking to an immigration queue looking unremarkable just one of many passing through ', ' as they were abbott watching pamela pull a photo from her file sliding it over conklins face peering back pamela lets talk about conklin abbott what are you after pam you want to fry me you want my desk is that it pamela i want to know what happened abbott what happened jason bourne happened fury focusing youve got the files then lets cut the crap it went wrong conklin had these guys wound so tight they were bound to snap more abbott contd bourne was his number one guy went out to work screwed the op and never came back conklin couldnt fix it couldnt find bourne couldnt adjust it all went sideways finally there were no options left pamela so you had conklin killed silence i mean if were cutting the crap abbott ive given thirty years and two marriages to this agency ive shoveled shit on four continents im due to retire next year and believe me i need my pension but if you think im gonna sit here and let you dangle me with this you can go to hell marshall too flat it had to be done pamela and bourne wheres he now abbott shrugs dead in a ditch drunk in a bar in mogadishu who knows pamela i think i do we had a deal going down in berlin last week during the buy both our field agent and the seller were killed we pulled a fingerprint from a timing charge that didnt go off beat they were killed by jason bourne abbott hesitates blindsided what a courtesy knock at the door cronin appearing in the doorway theyre ready for us upstairs ', ' a now at the immigration officer booth bourne hands over an old blue passport it reads jason bourne whats he up to is he giving up immigration officer where you coming from mr bourne bourne tangiers the officer runs the code on the passport through the scanner ', ' a tech turns as a computer alarm begins an incessant beeping the screen as jason bournes passport data begins scrolling through a sleeper waking up on the grid then his photo work station as an interpol supervisor leans in over the techs shoulder to see whats up after a beat as the tech begins typing and hits send ', ' crewcut turns from his monitor to his own superior as at the same time ', ' looking up from his computer the immigration officer gestures bourne to one side immigration officer sir would you be so kind as to step over here please bourne uh sure the immigration officer comes out of his booth as a carabinieri joins him and they escort bourne to a small room at the side of the customs hall immigration officer please wait in here bourne scans the hall as he walks enters room pamelas vo seven years ago twelve million dollars was stolen from a cia account bourne takes a seat carabinieri guards the room ', ' same table more faces marshall back in the throne abbott three cia mandarins plus their s and pamela in warsaw this is click a photo of the man killed in berlin fills the projection screen behind her click crime scene photo of dead body click pecos oil logo pamela contd ivan mevedev senior financial manager worked for one of the new russian petroleum companies pecos oil he claimed to know where the money landed we believe this could have only happened with help from someone inside the agency this click conklins photo pamela contd placing it on the table this is conklins computer click a photocopy of a banking contract pamela contd at the time of his death conklin was sitting on a personal account in the amount of sevenhundred and sixty thousand dollars abbott do you know what his budget was pamela excuse me abbott we were throwing money at him throwing it at him and asking him to keep it dark pamela may i finish abbott conklin mightve been a nut but he wasnt a mole you have me his calendar for a couple of days ill prove he killed lincoln appealing to marshall this is supposed to be definitive pamela whats definitive is that i just lost two people in berlin abbott so whats your theory mocking her conklins reaching out from the grave to protect his good name incredulous the man is dead marshall hes heard enough no ones disputing that ward abbott for crissake marty you knew conklin does this scan i mean at all marshall signals for quiet marshall okay cut to the chase pam what are you selling pamela i think that bourne and conklin were in business that bourne is still involved more pamela contd and that whatever information i was going to buy in berlin it was big enough to make bourne come out from wherever hes been hiding to kill again to abbott hows that scan as the mandarins all start talking at once zorn enters stands at the head of the table tries to get their attention zorn hey they look up look youre not gonna believe this but jason bournes passport just came on the grid in naples abbott blinks what ', ' nevins american a junior cia field officer walking from the parking lot talking on his cellphone nevins what can i do i cant ill call you when i know what im into a hassled pause i dont know some guys name came up on the computer starting toward the building so start without me if i can get there i will later nevins hangs up and pockets the phone he hustles towards the building ', ' the room is jumping agents tracking working the phones and computers pamela giving orders abbott watches cronin looks up from computer screen looks like hes been detained pamela whos going us cronin theres only a consulate they sent a field officer out half an hour ago pamela cuts him off then get a number they need to know who theyre dealing with cronin already on it ', ' as nevins flashes his credentials to carabinieri at door who gives an unimpressed shrug and lets him in nevins takes his overcoat off tosses it on the empty chair we see a big ass for just a second under his suit jacket nevins alright mr bourne is that your name bourne nods names nevins im with the us consulate could i see your passport bourne silent hands over his passport nevins contd so mr bourne nevins studies bournes passport nevins contd what are you doing in tangiers silence nevins contd faux friendly are you travelling alone bourne stares straight ahead nevins comes around the table and sits in front of bourne nevins contd in his face look i dont know what youve done but youre gonna need to play ball here nevins cell starts to ring he shrugs an apology turns away and answers nevins contd contd nevins pamelaphone this is pamela landy a ci supervisor calling from langley virginia are you with a jason bourne now nevins listens looks at bourne yes ', ' a pamela on the phone pamela then use extreme caution he can be very unpredictable and violent use whatever means necessary to ', ' whatever nevins is being told its concerning bourne watching him knows exactly what this is close on nevins as he steps away listening intently his hand just starting to move toward his shoulder holster nevins contd okay ill call you right back nevins flips shut his phone he reaches for his gun even as he turns and bourne is right there in his face whump momentum and gravity reaching mutual agreement as nevins hits the deck carabinieri barely clears his holster before chop chop bourne has him down in a heap bourne is back silent and effective finding nevins cellphone bourne reaches into his bag he holds the phone next to a larger diagnostic mobile unit the confirm light blinks nevins phone has been cloned bourne puts the phone back in nevins coat takes his gun and carabinieris gun and radio and puts them in his duffle were starting to realize theres a plan at work here finally bourne exits the door wedging a desk under the handle so it cannot be opened from the inside and calmly walks away like nothing ever happened ', ' and now we see the old bourne in his long black coat purposely striding out of the building he pauses long enough for the security camera to get a good look at him the ronin returns ', ' bourne crosses the street and approaches a man putting his suitcase in the trunk of a green peugeot bourne reaches into his bag pulls out some cash ', ' nevins stirring the carabinieri still out a phone starts to ring nevins phone finally sitting up he answers nevins hello ', ' pamela at the other end of the line pamelaphone mr nevins nevinsphone whos this pamelaphone pamela landy again where do we stand ', ' a nevins barely knows where he is ', ' bourne sits in the dark car headphones a nest of cool gadgetry on the passenger seat listening in recording he writes pamela landy circles it nevinsphone i think i think he got away pamela looks at the faces waiting around the table shakes her head no pamela have you locked down the area nevinsphone ah were in italy they dont exactly lock down real quick intercut bourne nevins pamela pamelaphone how long have you worked for the agency nevinsphone me four years pamelaphone if you ever want to make it to five youre gonna listen to me real close jason bourne is armed and extremely dangerous a week ago he assassinated two men in berlin one of whom was a highlyexperienced field officer continuing as were totally on bourne at this point sitting there in the dark car struggling to make sense of this what the fuck is she talking about berlin he writes it circles it pamelaphone contd i want that area secured i want any evidence secured and i want it done now is that clear nevinsphone yes sir maam pamelaphone im getting on a plane to berlin in minutes which means you are going to call me back in and when i ask you where we stand i had better be impressed my mobile number is bourne already turning the key in the ignition the peugeot roaring to life as he writes the number dropping the car into gear bourne pulls briskly away from the curb ', ' a pamela finishes hangs up abbott berlin pamela ive already got a team there i doubt bournes in naples to settle down and raise a family abbott you dont know what youre getting into here pamela and you do from the moment he left treadstone he has killed and eluded every person that you sent to find him before it can come to blows marshall riot act enough i want both of you on that plane more marshall contd and we are all of us going to do what we were either too lazy or inept to do the last time around youre going to find this sonofabitch and take him down before he destroys any more of this agency beat is that definitive enough for you abbott nods sharing a look with pamela as we ', ' aa pamela and cronin come screaming around a corner and down a long corridor abbott and zorn trying to keep up cronin kurts reopening all the wyfi and sat links pamela uplink all relevant files to kim a look back at zorn and i want them to contact anyone who had anything to do with treadstone zorn looks to abbott as they disappear around a corner ', ' b the peugeot speeding north north towards germany and ', ' bourne driving listening to playback of pamelas conversation with nevins pamelatape jason bourne is armed and extremely dangerous bournes face eyes tight looking weird pamelatape contd contd a week ago he assassinated two men in berlin one a highly a suddenly a flashback a shard pieces lightning flash of images getting in the back seat of the car rolling brandenburg berlin a mirror the television tower the driver looks back we see him well know him later as jarda then a steel case on the backseat inside a syringe a dark vial pistol as we lay hands on them b back to b bourne out of it jolted almost losing control of the car for a second jerking back into his lane recognition toughing it out steady as she goes catching his rhythm again accelerating and ', ' a bakery on the corner nicky emerging nicky from the old days suddenly she stops abbott stands there beside a parked car the passenger door open message clear get the fuck in ', ' inside a hanger inside an office abbott watching as cronin questions nicky pamela sits on a window sill cronin so your cover at the time was what nicky that i was an american student in paris cronin what exactly did your job with treadstone in paris consist of nicky looks to abbott he nods that its okay to answer pamela bristles at the checkoff nicky i had two responsibilities one was to coordinate logistical operations the other was to monitor the health of the agents to make sure they were up to date with their medications cronin health meaning what nicky their mental health because of what theyd been through they were prone to a variety of problems pamela losing patience what kind of problems nicky depression anger compulsive behaviors they had physical symptoms headaches sensitivity to light pamela amnesia nicky before this before bourne no nicky gets agitated abbott steps in fatherly good cop abbott were you familiar with the training program nicky the details no i mean i was told it was voluntary i dont know if thats true or not but thats what i was told a bit defensive look they took vulnerable subjects okay you mix that with the right pharmacology and some serious behavior modification and i dont know i mean i guess anythings possible zorn arrives from outside zorn the jets ready points to nicky theres a car for you everybody moving nicky relieved shes off the hook she thinks she becomes aware of pamela considering her nicky good luck pamela you were his local contact you were with him the night conklin died youre coming with us ', ' streaks across the sky ', ' quiet in the cabin abbott gets up to use the bathroom pamela sits across from nicky who stares out the window as the bathroom door clicks shut pamela seizes the privacy pamela im curious about bourne your interpretation of his condition you have specific training in the identification and diagnosis of psychological conditions nicky am i a doctor no but pamela are you an expert in amnesia nicky look what do you want me to say i was there i believed him pamela believed what nicky i believed jason bourne had suffered a severe traumatic breakdown pamela so he fooled you nicky frustration building if you say so pamela leans in still low not good enough youre the person who floated this amnesia story shifts gears ever feel sorry for him for what hed been through nicky youre making it out like were friends here or something i met him alone twice pamela you felt nothing no spark two young people in paris dangerous missions life and death nicky incredulous you mean did i want a date pamela did you nicky these were killers conklin had them all jacked up they were dobermans pamela some women like dobermans nicky what do you want from me i was reassigned im out pamela see thats a problem for me nicky whatever hes doing we need to end it this isnt the kind of mess you walk away from pamela leans away nicky looks back out the window ', ' three in the morning as the gulf stream lurches to a stop two black sedans here for the pickup teddy the greeting party as pamela cronin abbott zorn and nicky disembark ', ' a the sedans making their way stopping at a nondescript office building ', ' b elevator opens into their th floor world emergency activity kim ready to debrief kurt work the computers energy up pamela abbott and cronin bring nicky into the room kim so far bournes had no contact with anyone on the list langley pulled an image out of naples its uploading right now kurt coming in now everything stops as the photo blurry oblique begins materializing on halfadozen monitors around the room suddenly theyre surrounded by bourne pamela to nicky is it him looking closer she nods cronin hes not hiding thats for sure zorn why naples why now pamela has gone quiet just staring at the picture as kurt could be random cronin maybe hes running abbott looks skeptical abbott on his own passport kim the image whats he actually doing cronin whats he doing hes making his first mistake and then from behind them nicky its not a mistake everyone looks over they dont make mistakes and they dont do random theres always an objective always a target beat if hes in naples on his own passport theres a reason pamela turns to abbott a silent moment between them theyre in it now and they know it ', ' c the peugeot streaking through the alps passing a sign for the german border moonlit glacial peaks whipping past as club music starts pulsing louder and louder and ', ' d bourne driving hard pushing the car through the night mission bourne as the music keeps just building and building taking us into ', ' packed and loud skin and smoke a doorman on the move taking us with him through the crowd faces voices all the moscow party people and at the back a vip booth kirill simply shitfaced but in a really creepy numb kind of way three women absolutely gorgeous are sitting around him chatting away as if he werent even there the girls looking up to see the doorman standing there can he walk kirill stirs his stupor a futile attempt to escape eyes still those of an exceptionally hard man a minute later kirill can walk the most graceful drunk youve ever seen making his way through the club tuning out everything but the need to get to the door and ', ' yes day its nine am kirill suddenly in the sunlight people going to work kids off to school and gretkov sitting in his mercedes not happy follow car and security and assistant equally unhappy gretkov you told me jason bourne was dead kirill blinking against the sunlight trying to process ', ' discreet and chilly a car pulls up a man gets out munich we dont see his face as he heads in ', ' the man enters his alarm system beep beep starts once he comes through the door theres a keypad on the wall he enters his code and the beeping stops just like everyday its a sad house he hangs his coat on the rack moving now into the kitchen he drops his briefcase on the table opens the fridge for a drink except what he comes out with is a gun wheeling around the salaryman is jarda jarda from bournes dream but as he turns bourne behind him bigger gun waiting so ready bourne i emptied it jarda a total pro felt a little light bourne drop it jarda lets the gun fall looks his old comrade over a beat but bournes not interested in a reunion bourne contd contd here bourne tosses him flexcuffs jarda puts his hands behind his back turns to let bourne cinch them bourne contd contd front use your teeth jarda caught scamming sorry old habits bourne kicks over a chair sit jarda contd word in the ether was youd lost your memory bourne checking jardas briefcase tearing through it bourne you still shouldve moved jarda i like it here a beat more jarda contd last time i saw you was greece you had a good spot bourne reacts doesnt look over but realizes jarda contd i had the girl i had her lined up that whole afternoon waiting for you that was the problem defensive you ever do two targets its tough bourne turns cold jarda contd his real question so why didnt you kill me then bourne she wouldnt let me beat shes the only reason youre alive silence jarda down a peg or two jarda what do you want bourne conklin jarda hes dead bourne the gun right to jardas face bourne try again jarda shot dead in paris dead the night you walked out bournephone then who runs treadstone jarda nobody they shut it down were the last two its over not finishing because hes falling landing hard bourne just kicked the chair out from under him bourne youre lying if its over why are they after me jarda i dont know bourne who sent you to greece jarda a voice a voice from the states someone new bourne pamela landy jarda i dont know who that is bourne whats going on in berlin jarda i dont know why would i lie silence bourne pulls back unsure jarda makes it to his feet jarda contd what the hell did you do you must have really screwed up bourne doesnt know he backs off jarda contd she really did that told you not to kill me beat i had a woman once but after a while what do you talk about i mean for us the work you cant tell them who you are bourne i did jarda hesitates its really like bourne just told him how much he loved her jarda i thought you were here to kill me something in the way he said it plus jarda just glanced at his watch bourne what did you do jarda shrugs almost embarrassed bourne looks across to the alarm pad jarda hit on the way in voltage like a switch bourne contd contd you called it in jarda im sorry bourne how long how long do i have stopping because the phone just started ringing loud insistent bourne contd contd how long ', ' jamming right the fuck into it three guys jarheads dod special force dudes speeding through munich jar is the driver jar is prepping weapons like a maniac in the backseat and jar on the phone its a red flag file so fix it call them back asap jar the call what whatd they do jar bad news she called munich local jar slamming home another clip its probably just a drill anyway ', ' phone ringing jarda in cuffs bourne scanning out the windows everything fast bourne car keys jarda my coat but we should bourne what jarda take the back get another car bourne hesitates just a moment wrong slam out of nowhere jarda swings twohands still cuffed like a mace catching bourne hard and bourne stunned jarda smashing the coffee table slices the flexcuffs through on a shard of glass free jarda follows up knee up in the ribs the gun knocked free from bournes hand skittering across the floor bourne as jarda starts to move backhanding him and ', ' two munich patrol cars rolling and ', ' seen from inside glimpsed through the glass outside its war a flatout closequarter death match jarda older and cuffed but strong and determined bourne still hammered from that opening suckerpunch the two of them braced there grappling falling jarda the cuffs hes got bourne in a chokehold but bourne driving his head back into jardas face and ', ' jamming along through munich ', ' jarda bourne the gun on the floor struggling for it jarda there first bourne on him pinned there four hands one gun and blamm wild shot into the refrigerator still wrestling breaking jardas nose until the gun knocked away again finally their hands locked into each others throats this is as real and up close as it gets until bourne finally holds dead weight eyes fixed staring bourne jumping back blood all over his shirt bournes first kill in a long time a messy one revulsion ', ' jarheads getting close but up ahead another munich patrol car in motion the jarheads react dont need or want the company ', ' bourne all business now pulling the stove away from the wall there the gas line hose bourne ripping it free gas running wide open into the room next a fork grabbing it jamming it down into the mechanism on a toaster wedging it there and now hes grabbing papers jardas stuff on the table jamming a roll of sales projections into the toaster beside the fork bourne coughing from the gas turning the toaster on checking his watch taking one last look at jarda dead on the floor and ', ' theyre just turning into the street ', ' the dod car three dods approaching the house when booooomm jardas kitchen blown out gone ', ' bourne same moment flying out the rear as planned urban backyard exfil hes flying and gone ', ' fire smoke its all burning now munich cops blown back theyll have a story to tell tonight ', ' drives away past arriving police ', ' the bullpen is cranking phones to munich lines to langley abbott watching from the sidelines kurt and kim at their work stations pamela on mobile turns to abbott pamela so he beats a man within an inch of his life strangles him then blows the place up at nicky for someone with amnesia he certainly hasnt forgotten how to kill has he across the room cronin and teddy suddenly excited about what theyre seeing on their screen cronin hey theyve got him boxed in new data coming up on the monitor everyone rushing to look excited except zorn forget it they lost him teddy whatre you talking about theyve got a three block perimeter zorn you cant see him hes not in front of you forget it hes gone cronin fuck you buzzkill its not gonna be like last time zorn you better start listening to someone cause weve been there abbott okay enough stepping in take a walk danny get some air zorn nods happy to nicky piping in i dont think we need to keep looking for him anyway pamela and why is that nicky because hes doing just what he said hed do hes coming for us and for the first time theyre all thinking the same thing ', ' it is pouring rain seen from that hellish car a huge distinctive needlelike tower dominates the skyline lights flashing through the dark and wet ', ' bournes eyes opening heart pounding springing up alone damn his side hurts recoiling from that where is he hes in the car looking around and his windshield pov an autobahn reststop gas station sleeping trucks back to bourne catching his breath shifting away from the pain in his rib checking his watch but what the hell is that on his sleeve fuck its blood jardas blood ', ' bourne out of the car fast careless wrong not even checking whos watching pulling off the shirt tearing it off throwing it down and standing there in the weird light a big bruise ripening on his side looking around its okay nobodys watching but shit man get it together ', ' a streaking along bourne back to his mission ', ' b roaring by a sign berlin km ', ' kirill striding through the terminal moving quickly toward a departure gate and gretkov above watching him go ', ' bourne drives up ', ' quiet and forlorn this early just like bourne whos taking a locker stashing a backpack prepping the evac always ready he heads outside we hear hotel operator vo front desk german berlin hilton how can i help you bournephone vo im trying to reach a guest pamela landy please hotel operator vo im sorry but im not showing that we have a guest by that name continuing as ', ' a bourne tucked in with a berlin guide book a felt tip pen and a fiftyeuro phonecard working it bournephone pamela landy please hotel operator sorry i dont see it here crossing out another hotel off the list four down forty to go as we start time cutting and hotel voices vo overlapping no one here by that name no sir theres no landy here how are you spelling that sir sorry but no i have no landy registered sir continuing until ', ' b clean and plain a bed nobodys slept in the phone begins ringing pamela fresh from the shower rushing out from the bathroom to answer it pamelaphone hello dial tone pamela hangs up that was strange ', ' c a taxi driving through the empty early streets and ', ' d bourne in the backseat staring out the window and his pov the fernsehturm looming as they pass the berlin tv tower that needle in the sky from the flashback and then e suddenly e flashback its raining were still moving still in a car still near alexanderplatz but suddenly its pouring outside turning back we realize were not in the cab anymore theres a driver up front and beside him conklin yes conklin hes in the passenger seat turning back to us handing us something a photograph a face some guy conklin neski vladimir neski the photo hes at the hotel brecker get the papers beat say it bourne treadstone bourne alone in the back staring at the photo bourne neski hotel brecker papers conklin this is not a drill soldier were clear on that this is a live project and you are go training is over bourne yes sir conklin good then gimme the damn picture back taking it see you on the other side to the driver pull over hes getting out f back to f bourne sitting in the back seat of the cab frozen there rocked whats happening to him no chance to work it out because the taxis stopped and taxi driver waiting irritated the hotel brecker or the grand make up your mind bourne what taxi driver this is the westin grand you just said brecker bourne fishing for money yeah sorry this is good ', ' g concentric rings looking down on each other bourne slipping in unnoticed taking a quick look up before moving along ', ' h bourne stepping up to the guy behind the desk the gym mostly empty bourne hi i think i left my backpack here yesterday black nike the guy disappears in back to check bourne leans across the counter scrolling the computer the guest list his finger stabbing down on screen landy pamela bourne clears the screen walks away ', ' j because of the setup bourne pretending to talk on a house phone has a view of room across the way the door opens pamela exits carrying an overnight bag bourne watches ', ' k elevator doors opening pamela coming out into the lobby heading toward the exit and ', ' l a black suburban at the curb cronin standing there waiting as she emerges pamela anything teddy no munichs a bust hes loose pamela are we locked up cronin i told everyone they had an hour eat sleep shave whatever they want but once were back were back for good as they pile in and bourne walking right past them hes got the whole thing scoped heading quickly across the street and ', ' m bourne jumps into the first cab in the rank and ', ' n the driver starting up the car as bourne that black suv fifty euros if you keep me close the driver smiles and ', ' i pt kirill walks down the same hallway gretkov came to meet him last time a guy carrying a briefcase toward him stopping for a moment to light a smoke letting kirill take charge of the briefcase smooth like it never happened ', ' the suv rolling up the cab continuing past and stopping at the corner ', ' a bourne looking back out the rear window his pov as they pile out of the van start inside acknowledged by a security detail pretending to loiter outside as we hear pamela vo munich to berlin check everything flights trains police reports thatll be box teddy thats yours continuing as ', ' i pt kirill opening the briefcase two automatic pistols silencers ammo care package ', ' a bulkhead opening bourne stepping out among the satellite dishes unpacks a bag telescope water food and we hear pamela vo box call it prior german connections nicky i want to rerun all bournes treadstone material every footstep kim box lets call it munich outbound continuing as ', ' weve been hearing it now were seeing it pamela at the chalkboard abbott backing her up everyone else spread around theyre regrouping urgently behind them cots are being set up food water stacked up pamela lets stay on the local cops we need a vehicle parking ticket something langleys offered to upload any satellite imaging we need so lets find a target to look for to zorn danny box i need fresh eyes review the buy where we lost the three million timeline it with what we know about bournes movements turn it upside down and see how it looks continuing as ', ' a decent view into the berlin hq two windows one offers a look at an empty kitchenette the other a nice shot of the bullpen area it looks like they are in for the long haul theres teddy pacing pasta glimpse of zorn conferring with abbottnow kim talking on the phone ', ' bourne eyes locked on the target scanning waiting and then something changes suddenly theres something down there thats clearly a great deal more electric than what hes seen so far a telescopic pov a nicky shes just come into the kitchenette pouring herself a cup of coffee nicky who he knows and bourne lowering the telescope yes now hes getting somewhere thinking it through as ', ' nicky is joined by pamela who goes for the coffee pamela is it fresh nicky its got caffeine in it thats all i know before pamela can pour her cell phone rings she answers pamela pamela landy bournephone i was at the westin this morning i could have killed you pamela who is this intercut with rooftop bourne its me pamela holy christ bourne nicky reacts to the name runs to the other room to try and start a trace pamela contd contd what do you want bourne i want to come in he wants to come in its like a bomb going off nicky back in with conklin pamela waving for a pencil pamela okay how do you want to do it bourne i want someone i know to take me in pamela who bourne there was a girl in paris part of the program she used to handle the medication and now we stay with pamela her eyes flicker over to nicky pamela what if we cant find her bournephone its easy shes standing right in front of you busted pamela okay jason your move bourne alexanderplatz minutes under the world clock alone give her your phone click the line goes dead pamela steps away from the window realizing hes on one of the roofs out there ', ' a as the bulkhead door swings in the wind bourne is gone ', ' b everyone gathered a big detailed map of alexanderplatz spread on the table zorn heres the clock shit hes put her in the middle of everything cronin its a nightmare well never get her covered abbott call a mayday into berlin station we need snipers dod whatever they got pamela snipers hold on he said he wants to come in abbott my ass he does youre playing with fire pamela marshall said nail him to the wall i dont know how you interpreted that but i dont think he meant repatriate him pamela dont you want answers abbott there are no answers theres either jason bourne alive or jason bourne dead and i for one would prefer the latter and what about her points to nicky you just send her out to this lunatic with no protection pamela looks to nicky pamela what do you think is he coming in nicky i dont know he was sick he wanted out i believed him pamela alright pamela gestures to abbott cronin teddy pamela contd make the call get a wire on her if it starts to go wrong take him out ', ' a the rear of the official berlin cia hq and here they come ten delta dudes in civvies sprinting to a couple vehicles with drivers ready and engines running and bc ', ' d nicky her hands overhead as zorn tapes a transmitter and battery between her shoulder blades teddy and cronin plot the area with two men plainclothed delta team kim and kurt on their own lines kim this just in they got the number bournes calls came from nevins phone the field agent in genoa teddy nevins is bourne abbott losing it are you an idiot bourne mustve cloned his phone an embarrassed silence abbott mad at himself for losing his temper looking up to find pamelas eyes on his abbott contd contd i hope you know what youre doing ef ', ' g in all its vastness alone theres the world clock nicky waiting on the periphery two plainclothed deltas nearby in quick succession nicky binocular pov sniper scope pov on a video monitor ', ' h everyone waiting holding their breath watching nicky standing as ', ' j nickys pamelas phone rings she answers as a yellow tram approaches bourne see that tram coming around the corner nicky yes bourne get on it she turns and walks as the tram arrives the delta dudes start moving ', ' k the yellow tram arrives nicky enters one of the delta dudes just barely joining her the tram begins moving nicky looks around nervously nothing happens the tram moves about yards across the platz stops at the next stop people get on and off nicky and delta dude relax a bit doors begin to close and just like that bourne swoops in beside nicky flashes a gun bourne walk bourne takes her arm and they just get off as the doors close leaving the delta dude behind they disappear down into the pedestrian subway lm ', ' n a madhouse a video feed on a monitor pamela wheres nicky as they realize shes gone abbott goddamn it i told you cronin listen listen he cranks the speaker bournes voice what did i say what did i tell you in paris o ', ' p bourne what were my words but she cant speak leave me alone leave me out of it but you couldnt do that could you nicky i didjason i swear i didi told them i told them i believed you bourne who is pamela landy nicky you hear me i believed you bourne is she running treadstone ', ' q pamela all ears nickys voice shes ci counterintelligence shes a deputy director bournes voice what the hell is she doing ', ' r nicky whats she doing nicky looks at him like hes crazy bourne why is she trying to kill me nicky they know defiant reckless they know you were here they know you killed these two guys they know you and conklin had something on the side they dont know what it is but they know as bourne tries to process ', ' s radio chatter going wild panic delta vo into radio where are they anyone ', ' t still walking bourne knowing he must be driving them nuts bourne how do they know that how can they know any of that nicky what is this a game bourne i want to hear it from you she looks at him is he crazy what bourne contd contd say it nicky last week an agency field officer went to make a buy from a russian national bourne a russian nicky it was pamela landys op the guy was going to sellout a mole or something i havent been debriefed on exactly what it was bourne last week when is she supposed to answer nicky shrugs on quicksand nicky and you got to him before we could bourne i killed him nicky you left a print there was kel that didnt go off there was a partial print they tracked it back to treadstone they know its you bourne i left a fingerprint you fucking people suddenly bournes jerking her down to a lower level ', ' u big static on the speakers delta co cooly checks the map delta co she must be in one of the pedestrian tunnels ', ' v as delta dudes fan out head for the subway entrances ', ' w an intersection of three tunnels bourne leads nicky far left she looks really scared ', ' gretkov has landed just coming off the flight a ', ' bourne what was landy buying what kind of files when she doesnt answer instantly what was she buying nicky conklin stuff on conklin trying not to lose it suddenly he rips the microphone out from under her shirt he knew of course dropping it as he yanks her along ', ' as the transmission goes dead christ aboott drills a look at pamela your fault pamela ignoring abbott that phone has a locator on it kurt and kim work their stuff ', ' gloomy deserted a mausoleum here come nicky and bourne she knows shes on her own now bourne dead serious looks at his watch bourne why are you here then nicky please im only here because of paris because they cant figure out what youre doing im here because of abbott bourne abbott nicky he closed down treadstone he took care of me after paris bourne so when was i here nicky what do you mean bourne for treadstone in berlin you know my file i did a job here when nicky no you never worked berlin bourne my first job nicky your first assignment was geneva bourne thats a lie nicky emphatic you never worked berlin bourne raising the gun eyes gone dead oh shit nicky contd nojasonplease bourne i was here nicky its not in the filei sweari know your fileyour first job was genevai swear to god you never worked here hes so ready to kill her nicky starting to cry hands over her face covering up bracing for the bullet she knows is coming bourne about to pull the trigger suddenly a flashback a moment a shard a womans face a backing away begging begging us begging the camera pleading for her life in russian this awful blur of desperation and panic fear too fast too panicked b jam back to b bourne swamped thrown hesitating close on nicky sobbing now when finally looking out and bourne is gone ', ' c an hour later whole new vibe siege mode curtains drawn three delta dudes parked around the room kurt and kim working the phones and screens the mood is dark pamela abbott cronin all in here the safe zone away from the windows cronin on a cell phone got it yeah hang on to the room okay theyve got three guys out front and another two taking the back stairs no word on nicky kurt looks up from screen even if shes still got your phone it might take awhile signals hard to trace down there pamela turns looking at the photo of bourne in naples introspective pamela so whats he doing you believe him abbott its hard to swallow beat the confusion the amnesia but he keeps on killing its more calculated than sick real soft sell what about nicky shes the last one to see bourne in paris shes the one he asks for they disappear pamela well whatever hes doing ive had enough this is now a search and destroy mission turns to the room i want the berlin police fully briefed and handing the photo to cronin get this out to all the agencies abbott agrees ', ' a bmw parked in the shadows ', ' kirill wearing headphones listening to a berlin police frequency theres an interpol wanted picture of jason bourne there on the seat hes in play ', ' d quiet intense activity military radios chirping here and there zorn moving through the bullpen carrying a cup of coffee heading back toward pamelas office where abbott is leaning in the doorway past him inside we can see pamela in the midst of a tough phone conversation cronin and the delta boss sitting there with her zorn the coffee sir abbott thanks abbott nods takes a sip looking beat zorn contd i have that number you wanted abbott hesitates but only a moment he never asked for a number but hes playing along looking satisfied as zorn hands him a slip of paper abbott glancing at it she say what time i should call zorn the sooner the better abbott nods pockets the paper turning back as if it were nothing and ', ' e massive modern busy bourne in the back in a corner doing a search hotel brecker scrolling and then stopping freezing because on the monitor a berlin newspaper archive there it is written large in loud tabloid german oil reformer murdered theres a photograph of the berlin police carrying two body bags out of the hotel brecker theres a caption identifying the dead as vladimir and sonya neski theres even a long article accompanying all this but its in german and we dont need to read it anyway because bourne is reading it and were reading in his face that he is rocked that he has found another bottom to the abyss ', ' f remember the building where vic was killed were back zorn and abbott making their way in zorn steering them away toward a stairwell at the back ', ' zorn and abbott have snuck in here work light signs of repair on the wall zorn nervous i did my box work but i wanted to show you before i showed landy i came out here last night because none of this was making any sense i mean im with you on this conklin was a nut but a traitor i just cant get there abbott what do you have danny zorn the electrical riser you put a fourgam kel on here and its gonna take out power to the building you know that what you cant know is if its gonna blow the room with it abbott and zorn there were two charges they were supposed to go off simultaneously the second one the one that didnt go off was down here pointing it out first of all this is nothing its a sub line for the breaker above second why put the charge all the way down here if youre good enough to get in here and handle the gear youre good enough to know you dont need this beat bourne would know abbott it was staged zorn is it a slam dunk no but abbott jesus zorn spitballing okay what if someone decided to cover their tracks by blaming conklin and bourne what if bourne didnt have anything to do with this abbott keep going zorn somethings been going on here in europe and its still going on post conklin whos been in berlin abbott lots of people zorn including landy jumping off the cliff she had access to the archives zorn hesitates but its out its in the room abbott who else knows about this zorn nobody you hes scared i had to tell you right abbott show me again zorn okay turning away when abbott out of nowhere his hand jamming up into zorns ribcage more than his hand because zorns eyes barely have a moment to register shock before they bulge clenching the younger mans body pulling him close as he turns the knife and zorn is dead abbott without hesitation shifting away from the blood letting the body fall abbott standing there listening checking himself for blood hes clean looking for a place to stash the body as ', ' a bourne across the street staring at the hotel haunted as a police siren edges closer through the empty streets aa flashback aa we are a pov a stakeout watching the hotel across the way the pov checks its watch checks the perimeter the street deserted foreboding the hotel our destiny waiting up there somehow and suddenly a light comes on a terrible signal and as the car suddenly lurches forward and around the corner ab back to ab bourne muscling up his backpack heading toward the hotel ', ' b and hotel fusty but comfortable and busy guests and staff doing their thing a clerk behind the reception desk clerk guten abend bourne playing it american guten abend clerk switching to english can i help you suddenly ba flashback the lobby but seven years ago ba across the room a man buttoning a raincoat as he passes neski bb jamming back to bb bourne stalled coming back as clerk contd contd sir smiling do you have a reservation bourne no sorry i just got in rallying back i is room available off the clerks look i stayed there before my wife and i the clerk nods checking the register the concierge just down the desk glancing over at bourne nodding hello and clerk im sorry that room is occupied would room be okay its just across the hall bourne sure thats fine danka cd shot ', ' a bourne riding up alone dread mounting and ', ' the concierge coming out of the office with a sheet of fax paper placing it quietly down beside the clerk and the fax bournes face the same wanted picture and ', ' bourne off the elevator he makes his way down his pov the sixth floor hallway suddenly scary ', ' a kirill sitting up as the police radio starts broadcoasting an allpoints bulletin the words hotel brecker in there kirill dropping the car into gear and ', ' b bourne walking theres his room but across the hall and down one room bourne steps up listening a moment then he knocks nothing he pulls a knife from his pocket checks the hallway hes clear wedges the blade in there and onetwo pop ', ' bourne enters a suite closing the door behind him and treadstone bourne seven years ago does the same bourne shakes off the flash looks around the lights are on an open suitcase on the bed ', ' the clerk the concierge and the manager are huddled in conversation with three berlin cops whove just arrived and trying to be discreet but this is clearly serious ', ' bourne just standing there breathing it in treadstone bourne doing the same ', ' bourne with his hand on the wall as if he can feel it like its all still here heart pounding and ', ' chaos bournes been found everybody rushing out cronin to teddy go take the van pamela the hotel how far teddy five six minutes cronin kurt youre here keep the comm line open ', ' bourne standing there looking out the window the images the television tower over the city everything but the rain ', ' the berlin police swat team truck arrives discreetly by the back loading area ', ' bourne flat against the wall just as he was leaning forward to see in the mirror just so and there ', ' a a man in the mirror pacing into view neski on the phone a talking in russian its raining bourne standing there treadstone bourne still wet from the rain one eye on that mirror and the other on a syringe that he prepped a predator the mirror the doorbell rings neski gets off the phone bourne tensing new element factoring and the mirror as neski opens the door a new flood of russian happy its mrs neski a surprise but hes very happy to see her bourne pocketing the syringe new weapon pistol quiet methodical watching the lovers bill and coo and the mirror mr neski kisses her takes her bag shes hanging up her coat and moving now toward the bathroom and bourne checking the window the weapon his balance and the mirror mrs neskis face right there seeing him so freaked she cant even register it yet bourne with the pistol in her face finger to his lips shhh but she knows backing away begging for her life in russian this awful blur of desperation and fear mr neski turning back to see his wife backing out of the bathroom and bourne with the pistol with no hesitation snap one shot into neskis heart hes down mrs neski whats just happened bourne has her wrist in his hand raising it to her head to where he holds the pistol her fingers his trigger snap letting the gun fall with her as she drops and bourne starts to move starts to prep his evac but theres something on the dresser a photograph the neski family father mother and a twelveyearold girl arms around each other happy and bourne staring at the picture undone for a moment hard out flashback to ', ' bourne our bourne standing where they fell frozen there paralyzed by the shame of original sin pt ', ' a swat captain conferring discreetly with the manager manager hes in swat captain call all the guests on the th floor tell them to remain in their rooms tell them its a police order then start on the th and th floors ', ' a bourne trying to stabilize to breathe ', ' the swat team on their way up ', ' a ring ring bourne snaps back as the phone in his room starts to ring four times and it stops bourne freezes footsteps shadows under the door he leans into the peephole bournes pov room german swat team taking position ', ' b bourne backs away surveys the room his watch his balance and ', ' c quickly turning into a major event halfadozen police vehicles already parked here more arriving every minute passersby mixing with the cops and people from the hotel whove just come out and kirill jogging over from the bmw hes just parked and ', ' wham the door kicked off its hinges swat team flooding into bournes empty hotel room and ', ' a bourne in motion out the bathroom window and ', ' berlin swat leader gives order to search other rooms and ', ' bourne up the water pipe to the roof as he arrives a swat team member turns bourne pulls him over the edge fires point blank into the nd swat members vest stunning him hes moving fast scrambling along the roof and into the night ', ' wham the door caves in and the swat team moves enters rushing to the window nobody no sign of him and ', ' kirill heading for the hotel entrance blocked by the exiting guests ', ' too many cops and radios swat team boss trying to take charge listen up were clearing the building room by room ', ' pamela jumping out of a van the moment it stops seeing it all the crowd the army of cops the searchlights playing across the hotel facade its another disaster ', ' kirill wants to get upstairs he cant too many guests coming down the stairwell berlin cops trying keep it moving and ', ' kirill hears bourne is on the roof ', ' pamela and cronin listening to teddy who just got the police update teddy black coat possibly leather dark slacks dark tshirt pointing now he says theyre gonna try and corral the guests on the street over there and then check them out but pamela disgusted yeah thatll workwhat the hell was he doing here cronin maybe he just needed a place to spend the night pamela i want to look at the room to teddy as she goes check it out pamelas in charge now they enter the elevator ', ' bourne coming around the other side of the hotel stepping to the left before he spots the swat van bourne aboutfaces heads the other way a sidewalk cop looks over checks the bourne photo print out in his hand ', ' teddy huddled with the hotel manager and a group of high ranking berlin cops turning back as abbott arriving breathless they missed him teddy so far but they found nicky shes back at the westin bourne let her go abbott he let her go great wheres danny he should head over there and debrief her the hotel whats here what was he doing teddy we dont know theyre in a room upstairs i was told to wait down here abbott accepting that because he has to only we see the fear turns to leave abbott ok if you see danny tell him i went back to the hotel abbott steps out into the street as ', ' bourne striding away and following sidewalk cop blowing a whistle fumbling for his holster bourne running now slowly at first and ', ' a now faster as if he can gauge his speed and distance ', ' motion bourne tearing away and ', ' a bourne slows to a walk two patrol cars heading his way no choice there a narrow passageway between two moving trolley trains and sprinting through the patrol cars skidding into s ', ' b the river spree lit by the trolley thats rumbling past and the running lights of a double coal barge up the river bourne runs across the bridge going as fast as he can hearing the police sirens swirling behind him when a third and fourth police car ahead bourne turns hard for a stairwell jumps the walkway curb leaps up the stairs two at a time as all four cop cars skid to a stop as doors open ', ' a tram waiting as the last few passengers get on the doors seem to stay open in slow motion as bourne appears makes a mad last dash and hes on and the doors dont close its not scheduled to go yet and here come the cops bourne off the tram guns appear bourne runs to his left stops short the other cops are coming this way screaming at him not a lot of options bourne looks over the rail down below a coal barge passing the prow just emerging bourne on the rail and jumping even as the first shot is fired ', ' bourne lands hard stands voltage going up one leg and theyre shooting at him he can worry about the leg later he runs back toward them the barge moving slow bourne disappears under the bridge ', ' guns aimed police waiting for a clear shot two of them dash to watch over the other side ', ' countering the barge going one way bourne the other dodging all the superstructure on deck all the while keeping his cover overhead and leaping to the second barge and more of the same until bourne running out of barge leaping back onto the bridge footing and ', ' the police watching the barge fully emerge continuing down river shouting in german that hes either in the water or hiding on the barge off they go down the stairs leaving the passengers on the tram blinking out in shock and bourne climbing back over the rail limping back on the tram just before the doors close and off it goes ', ' police converge from both ends barge goes under as kirill arrives at the center of the bridge missed again behind kirill a train snakes off into the night ', ' pt pamela and cronin move into the living room a couple of cops in the hallway outside cronin the room he checked into was across the hall why why would he come here pamela glances around something bothering her about this space pamela he mustve had a reason thats how they were trained cronin moves around the bedroom then into the bathroom and cronin he went out the window in here ', ' pt there on the mirror scrawled in soap on the glass i killed neski cronin pam you need to see this pamela moves in behind him cronin contd whos neski both of them staring pamela thinking alrighttake it down cronin what pamela this stays between you and i sensing confusion we finally have an edge i dont want to lose it ', ' very late abbott waits on an isolated bridge a lone figure in the shadow of east berlin gretkov arrives by car walks through the darkness abbott barely glancing over abbott you told me bourne was dead gretkov there was a mistake abbott ill say you killed his goddam girlfriend instead now theyre onto neski theyre at the brecker hotel even as we speak gretkov will it track back to us abbott no the files are spotless whatever they find its just going to make conklin look worse gretkov and the landy woman abbott shes done everything i wanted she bit on conklin so fast it was laughable she even found his bogus swiss account gretkov anything else abbott shoves a piece of paper and address into gretkovs hand abbott the paper theres a body in the basement danny zorn hes got to disappear for good clean and fast ill put him in bed with conklin and bourne even the girl nicky give me twentyfour hours ill think it up but get the goddamn body out of there its getting late a taxi now and then abbott contd neski was a roadblock without me theres no company no fortune you owe me uri one last push gretkov one last push one gretkov leaves abbott watches him go ', ' seconds later gretkov getting in slowly ', ' kirill slouched in back waiting gretkov to the driver gretkov airport to kirill were done here kirill nods as they pull away abbott turns and walks into the foggy night ', ' a late abbott walks a lonely figure past someone in the shadows bourne mr abbott he turns to answer when bourne firmly guides him into a side street bourneabbott scene ', ' as pamela and cronin exit the elevator they are met by teddy teddy heres what ive got reads remember vladimir neski russian politician seven years ago he was due to speak to a group of european oil ministers here at the hotel he never did he was murdered pamela by who teddy his wife in room then she shot herself pamela and cronin share a look pamela to teddy alrighti want you kurt and kim to stay on bourne track everything thats out there teddy goes to get in the van pamela follows with cronin pamela contd confidentially to cronin and i want you to go through and cross reference our buy that went bad the neskis and treadstone as they get in pamela contd they have to be related ', ' bournes arrived limping as he continues for the station ', ' bourne retrieving the exfil bag he stashed in the locker changed his clothes ', ' bag slung limping out bourne has changed clothes a big overcoat knit cap ', ' a busy midnight departure big train bourne climbing on the train under the sign moscow express moved ', ' a a blueprint spread across a table nicky kurt kim all gathered around cronin works the treadstone files on another table teddy at center briefing pamela teddy were looking at all berlin outbound good news is every train station in berlin has thirty to forty fixed digital security cameras common feed pamela are we hacking or asking teddy yes in that order pamela and what about you anything cronin its starting to link up the hijacked money the leak pecos oil one last bit is treadstone ', ' crossing the border into poland cold desolate snow ', ' conductors moving quietly through the dark cars checking tickets and visas and bourne hands over his ticket and russian passport off the grid ', ' a am kurt kim and teddy spread around the room theyve been running laptop train station videos for hours just about ready to raise the white flag all they have so far is an isolated loop of bourne limping into the mens room cronin watches it stutter along cronin does it look like hes faking teddy on the way in forget it kurt the legs definitely hurt cronin the blueprint well theres no window in the mens room folks so lets find somebody coming out with a bad left leg kurt worn out maybe hes still in there teddy ive got a limping guy but its the right leg kim walking away or walking toward you cronin jumping on that right there over teddys shoulder cronin thats him its the coat what train is that ', ' bourne asleep in his chair rocked by the rhythm but something wakes him up looks out the window something weird about the light out there then up to see marie looking at him over the back of his chair in front of him no big deal bourne hey she smiles a beat she comes around sits beside him he looks away out the window bourne contd i wanted to kill him marie but you found another choice bourne i did marie it wouldnt have changed the way you feel bourne it might have bourne looks back at her she smiles he accepts it leans back closes his eyes bourne contd i know its a dream marie you do bourne i only dream about people who are dead marie leans over kisses his forehead whispers bourne contd god i miss you i dont know what to do without you marie softly serenely jason you know exactly what to do that is your mission now bourne opens his eyes and its morning outside and marie is gone a little girl smiles at him from over the back of the chair in front bourne cant meet her gaze for long as he looks back out the window ', ' bourne watching the birch trees rush past not quite hiding the smokestacks beyond eyes locked forging something within one final mission as we ', ' abbott coming through its empty this early but heres pamela nicky cronin and the team waiting to report pamela sorry to wake you abbott waves off apology i wasnt sleeping to nicky as he passes you ok nicky yeah thanks abbott whats up pamela bunch of stuff pamela looks to cronin him first cronin we tied the room bourne visited tonight to a murdersuicide seven years ago a russian couple the neskis abbott playing along neski the reformer i remember that cronin he championed the equal distribution of oil leases in the caspian sea when he died they were all released to one petroleum company pecos oil guess what the ceo uri gretkov is ex kgb nicky someone was using treadstone as a private cleaning service abbott conklin a beat its im sorry pamela i guess you were right all along pamela waves him off its okay but pamela theres something else abbott can see by their faces this hits closer to home abbott what pamela they found danny zorns body dead in the basement at the building where my people got hit the first time abbott oh god it must have been bourne pamela did he say anything to you abbott no it must have been bourne pamela straight pamela well know for sure when we get the security tapes cronin but we can relax we tracked him hes on a train to moscow abbott reeling hiding it abbott moscow what the hells he going to moscow for pamela shrugs dont know abbott jesus i zorn i have to call his family tell them pamela im sorry ward they watch as he goes ', ' abbott in the rising elevator imploding ', ' palatial but you cant buy taste gretkov working his computer answers his phone gretkov da abbottphone you didnt stay uri gretkov matter of fact this is not a clean phone ', ' everyone still here cronin answering his cell phone motioning to them hes got news cronin phone to his ear youre sure pamela what the tapes cronin nodding but hold on holding the phone yep and abbott just direct dialed moscow from his room now we realize shes set a trap and abbotts walked in all the same pamela shakes her head wishes it wasnt true and theyre moving ', ' abbott at his desk still on the phone pouring a vodka gretkov leaving was a business decision were both rich come enjoy it abbott what do you mean gretkov go to the airport get a plane ill have a brass band waiting for you abbott save it for bourne gretkov what theres a knocking at his door abbott simply ignores it abbott he left yesterday on the night train hes probably just getting in now he drinks youll have to hurry gretkov bourne comes here why more knocking abbott good luck ', ' a speeding east through the russian countryside the forest is gone replaced by factories and refineries a wasteland of rust and gray that seems to go on forever ', ' pamela knocking again nicky teddy and cronin behind her pamela open it cronin with a pass key teddy prepped and ', ' a pamela leading they enter stop short abbott at his desk calmly pointing a pistol at pamela abbott they go you stay she looks back cronin shakes his head no pamela yes now they reluctantly obey the door clicking shut behind them abbott sit down pamela id rather stand if its all the same to you abbott i dont exactly know what to say im sorry pamela why would be enough for me abbott im not a traitor ive served my country pamela and pocketed a fair amount of change while doing it abbott why not it was just money pamela and danny zorn what was that abbott had to be done pamela no good options left abbott shrugs in the end honestly its hubris simple hubris you reach a point in this game when the only satisfaction left is to see how clever you are pamela no you lost your way abbott well youre probably right i guess thats all that hubris is he raises the gun pamela presses her lips together closes her eyes boom she opens them and as cronin flies back through the door theres abbott dead at the desk hes shot himself also in a way with some help from bourne ', ' the train easing to a stop the platform busy with people waiting and passengers disembarking bourne among them unremarkable in the crowd and ', ' bourne on the move welcome to the whole mad moscow scene a jumble of faces and voices travellers arrivals and departures families beggars drunk war vets hawkers ', ' there in the plaza bourne hobbling across the street when suddenly a car horn he turns and look out a big black bmw speeding past followed by two more all three cars with blue lights strobing on the dashboards a convoy whipping by like they own the place and taxi driver os gangster bastards dont care what they do bourne turns a grizzled taxi driver right beside him bourne pulls a slip of paper from his pocket bourne his russian is basic you know this address the taxi driver squints finally grunts affirmative he motions to his cab as they get in and pull away ', ' lots of cars no people but someone running its kirill pulling his keys as he sprints past and ', ' bourne and the taxi driver looking over as three moscow police cars speed by sirens wailing taxi driver its always something right bourne just nods as we ', ' kirill at the wheel a guy in a hurry who knows what hes doing one more thing on the passenger seat two big automatic pistols ', ' moscow cops fanning through the crowd showing bournes interpol picture have you seen him ', ' moscow cops with the picture flashing it around until young cabby the moment he sees it he was just here they just left ', ' theyve stopped bourne flashes a fifty dollar bill bourne you wait you understand stay taxi driver happy to pocket the cash sure no problem i sit ', ' old moscow but not for long theres new construction metastasizing all around it bourne crosses the street and his pov an abandoned wooden house windows shattered and boarded up paint all but gone roof and gables all failing back to bourne crestfallen checking the address this is it ', ' more cops everything focused on another taxi driver whos making a call on a cell phone everybody waiting on it ', ' bourne off the sidewalk now peering around the side trying to see if theres anything around back and over there an old woman on the steps next door watching him bourne starts over finding the sweetest smile hes got ', ' the taxi driver still parked there his pov bourne and the old lady shes pointing like shes giving directions when suddenly the drivers cell phone rings taxi driverphone hello ', ' bourne and the old lady his russian is limited but shes charmed nonetheless bourne a pento writeone minute searching his pockets ', ' the taxi driver on the phone not so happy anymore taxi driver im looking at him american hes right here ', ' the old lady scribbling on a piece of paper bourne reacting as the taxi drops into gear pulls away bourne wait hey but the taxi only speeds up and ', ' moscow police cars tearing away and ', ' kirill driving reaching for his ringing phone and ', ' the black bmw a moment later slamming on the brakes fishtailing a uturn and ', ' bourne hustling past all the new construction glancing back as police sirens start rising behind him and ', ' kirill skidding around another corner and ', ' two police cars just stopped there cops the old lady pointing everyone turning as the red lexus speeds past them and ', ' bourne coming down as fast as he can just ahead theres a footpath beneath a four lane overpass a neighborhood on the other side he could disappear there ', ' kirill driving and scanning there as he passes it the overpass slamming on the brakes and ', ' bourne hobbling out in the open twenty yards to go ', ' kirill jumping out of the lexus with a pistol in hand and ', ' bourne no clue bang his shoulder hes hit he throws himself forward and ', ' kirill shifting for a better second shot and ', ' bourne hes diving rolling pure instinct back under the embankment and ', ' kirill with no shot suddenly leaning over the rail just as the two moscow police cars come screaming up moscow cops jumping out with guns drawn and ', ' bourne hes up hes bleeding hes moving and ', ' chaos kirill with his hands in the air moscow cops coming toward him everyone screaming moscow cops mockbourne up hands up keep im kgb assholes them up drop the gun were chasing the same guy drop it hes getting away they let kirill go he looks back at the footpath bourne is gone as ', ' a gretkov strolls along suddenly two black sedans pull up and he is arrested ', ' a bourne hurriedly makes his way to the other end a few beats later kirill on the hunt ', ' a labyrinth of stalls food hardware clothes and crowded even this hardtoimpress crowd noticing bourne hobbling through nothing like a limping madman with a fresh gunshot wound to get attention people back off pull their kids out of the way some woman starts screaming and ', ' a security guard hears the commotion jogs out and ', ' kirill running toward the market five moscow cops behind him cant keep up and ', ' the security guard coming up fast behind bourne security guard hey hey you stop bourne turns the security guard right behind him and bourne no warning his good arm smash right into the security guards face and bourne takes his pistol and the crowd they jump holy shit ', ' crazy kirill sprinting through where did bourne go ', ' bourne back on the march except now hes shopping grabbing a bundle of tube socks and ', ' kirill sprinting out toward the stalls and ', ' bourne there a roll of duct tape and a bottle of vodka and ', ' kirill fighting his way through the fleeing crowd ', ' pt bourne leaving the market taking a swig of vodka and continues knows there are two new cops on his ass ', ' pt another cab stand cabbie by a yellow cab looks up to see bourne coming toward him and also the two cops as bourne nears the cabbie shakes his head bourne pivots casually like he doesnt know theyre coming until he spits vodka into one of the cops face blinded as bourne takes him and his partner out the cabbie raises his hands in surrender steps aside as bourne takes his car ', ' pt bourne in the yellow cab starting the engine peeling away careening into the street and kirill sprinting into the parking lot just in time to see ', ' pt bourne concentrating away the pain trying to drive ', ' two ladies ducked behind a big black gwagon freaked out as kirill grabs their keys and ', ' the cab speeding across a boulevard into an older neighborhood of rising narrow streets and two moscow police cars pulling uturns on the boulevard whipping around to give chase and the gwagon in full pursuit now and bourne driving up this curving little hill and the two moscow police cars starting to climb and kirill driving and hes on the hill now bourne bad hand on the wheel holding on trying to find something in passenger seat tube socks the two moscow police cars splitting up one on bournes ass the other cutting hard into a side street flanking him and bourne topping the hill two choices right or left right no wrong because down the hill theres a police car just about to angle in from the sidestreet and bourne no choice flooring it the cab its a whale slam knifing the front end of the police car and the police car spun back crashing against a building on the corner and kirill right behind that guy swerving onto the sidewalk sparks from the wall as he scrapes hanging in skidding into a turn down the hill and just missing the first police car bombing right past him bourne in pain as he packs his shoulder wound with the socks ahead the street banks downhill to left and there a boulevard wide ride lots of traffic and the cab rocketing into the flow and behind him police car with the gwagon right on his ass and bourne wrists flicking the wheel the cab screaming through the slower traffic and kirill totally on it pedal down passenger window open wind blowing hes got the pistol in his hand closing the gap and the black gwagon blowing past police car and bourne steering barely as he tears a few strips of duct tape to finish his triage blam blam the gwagon right beside him bourne reacting what the fuck thats not a cop but no time to clock kirill because kirill shit cant keep shooting into the oncoming lanes swinging wide a truck swerving again and the cab wavering again rallying and up ahead the boulevard opens into the river beltway big wide fast kremlin in the bg and four new police cars screaming down from red square and bourne skidding onto the beltway looking for room finding it open road kirill back in the hunt and the river beltway cab screaming past then one two three four police cars now the black gwagon and bourne both hands on the wheel hes already forgotten about his shoulder the beltway up ahead another choice right takes you up to the city left is a transit tunnel and bourne checking his rearview starting right and the two lead police cars right on his ass and bourne fake out veering left last second into the tunnel and the two lead police cars wrong and worse trying to change crash spinning and its not just them a third police car caught in the clutter not to mention the commuters crash the police are out of the race kirill not fooled threading the needle through the carnage and into ', ' four lanes two way and long theres the cab squibbing past slower cars and kirill on him move for move follow the leader and bourne checks the rearview hes lost them all but the gwagon who the hell is that the heavyweights world championship belt up for grabs kirill gaining nearly pulling level bourne nowhere to go thats never stopped him before he carves a path turns two lanes into three as sparks his way through a lane split the gwagon roaring after him bourne checks the mirror closer who the hell is that guy kirill gaining firing through his passenger window bourne brakes tunnel as the two vehicles scrape along each other kirill firing back odd angle bourne ducking for meager cover as bullets stitch through the roof tunnel the gwagon crushes the cab against the wall sparks showering the windshield finally the cab shoots ahead kirill in a controlled fury the suv jerking hard and right into the rear of the cab bourne trying to keep control spots a maintenance truck up ahead kirill banging away as his quarry straightens maintenance truck looming bourne a hard left tunnel the cab wrapping around the front of the suv wham pushing it to the right the cab continues spinning around the gwagon details front bumpers locking on rear fenders as tunnel the gwagon hurtling forward the cab ass end first locked together kirill firing into the cab really unloading now bourne down on the floor a tornado overhead kirill slaps in a new clip intense bourne gun against his door just below the window knob whumpwhumpwhump suv tire shredding kirill fights the wheel another truck looming large bourne looking between the seats out the rear window a lane dividing pillar ahead cab as bourne sits up jerks the wheel to the right tunnel the cars unlock spin away from each other kirill focused taking deadly aim bourne staring back at him calm i know something you dont know kirill frowns the truck swerves to reveal the pillar to kirills pov kirill eyes go wide whallop steel vs concrete concrete victorious a bone compressing truly horrendous impact bourne whipping the wheel cab spinning to a stop out of harms way door opening ', ' gun ready bourne heads over ahead spam in a can bourne crouches down looks in kirill bloody beattocrap barely alive but trapped entombed alive by the metal crushed around him bourne watches not here to help kirill looks over calms a moment as the two men consider each other bourne looks at him long and hard kirill dies and bourne stands and just walks away ', ' a snow swirls pamela disembarks from the g or us military plane she is met by russian officials ', ' huge awful sovietera housing towers fill the horizon a city bus grinds to a stop people trundle off working people at the end of their day tired cold a girl trudging a manmade wasteland twenty a proud little waif sad eyes home from some job irena ', ' grimmer up close rusted steel mesh over the windows drunk teenagers a haze of cigarette smoke irena pushing through doesnt want to talk to anyone ', ' irena climbing a junkie here flickering light there ', ' irena her key at the door domestic disturbance playing across the hall she opens up and ', ' its dark and shes barely through the door when irena jumps chokes back a cry bourne is standing there propped there actually behind her gun in hand motioning for her to be quiet bourne his shabby russian quiet silence okay irena nods scared gun in hand bourne pushes the door the last few inches so its fully closed irena i have no money no drugs is that what you want and now she can really see him hes a disaster shivering bloody eyes more hollow than hers are bourne sit can you trying to conjure the russian the chair have the chair irena accented i speak english bourne staring at her nods gestures for her to sit bourne please so she does and here they are bourne contd contd of all the people in the world youre the only one i have anything to offer hesitating thats why i came here irena shes terrified okay hes got something beside him something hes taken off the wall its the photograph the neski family same as the one that was in the hotel brecker mom dad and irena arms around each other in front of the house before it was abandoned happy smiling perfect bourne its nice a beat does this picture mean anything to you no answer hmm irena its nothing its just a picture bourne no its because you dont know how they died irena he couldnt understand no i do a change in bourne as he studies her measures her some moment of truth is here irena braces unsure bourne i would want to know beat i would want to know that my mother didnt kill my father i would want to know that she didnt kill herself irena what she really looks at him now fear overwhelmed by curiosity bourne i would grow up thinking that they didnt love me if they just left me like that irena making sure her eyes dont leave his they dont bourne contd contd it changes things that knowledge doesnt it irena wary yes bourne thats not what happened to your parents irena then what bourne i killed them body blows but he has her attention she wipes a tear bourne contd it was my job my first time your father was supposed to be alone but then your mother she came out of nowhere a little shrug i had to change my plan beat you understand me does she you dont have to live like that anymore thinking that irena you killed them bourne nods thats right bourne they loved you beat and i killed them irena howhow canhow can you be here and say this bourne i dont want you to forgive me she stands suddenly stands because if she doesnt shell burst into tears because she knows if she starts crying she wont be able to make sense of this irena for who he doesnt answer killed for who bourne pushes himself to his feet a real effort bourne it doesnt matter your life is hard enough irena youre a liar bourne you know im not irena youre a liar bourne look at me there they are two people standing in a room squared off and now she starts crying really crying and hes taking it irena i should kill youif its true you should diei should kill you now bourne i cant let you do that either irena because youre afraid bourne no starting for the door because you dont want to know how it feels she hesitates stunned hes leaving hes opening the door bourne contd i have to go now irena is this really happening bourne empty im sorry and she sags back into the chair as the photograph on the table the sound of the door closing and irena crying as ', ' bourne trudging along across the snow hes done it and he really cant take another step theres a bench he sits down out of gas he just might die here we slowly tilt up to the multi colored moscow tenements fade out ', ' bourne waking up sitting up where is he trying to get his bearings but its so bright white walls sheets sunshine through clean windows and pamela os hello david there she is standing at the foot of his bed bourne where am i pamela ramstein air base germany smiles before the wall fell you would have woken up in a russian prison hospital he looks around tries to move hammered by pain bourne oh shit pamela careful long moment hes taking it in trying to bourne why am i alive pamela are you disappointed they study each other a beat bourne i know who you are pamela nods very calm here no sudden movements pamela thank you for your gift im sorry about marie bourne whats that pamela do you think you can read are you well enough she has a folder a photograph bournes face stapled to the cover pamela contd its all in here treadstone a summary of your life all of it he waves it off bourne dont need it i remember everything pamela smiles again sounds like a threat bourne you didnt answer my question pamela why youre alive beat youre alive because youre special because she kept you alive she smiles because we want you back on our side bourne silent but hearing it pamela leaves the file pamela contd contd take a look at it well talk later bourne watching her back away as she exits into ', ' long sterile hallway cronin and nicky standing there with an air force sentry assigned to guard the room cronin and nicky trying to play it cool but now as they get some distance down the hallway pamela to the sentry lets give him half an hour nicky quietly so pamela felt promising its a start a chill in the air both of them going quiet because theres a nurse carrying a tray of food shes coming toward us theyre walking away staying with the nurse now coming up the hall the sentry smiles opens the door and she enters ', ' empty bed open window bourne is gone as the music starts pumping and we ', ' off he goes disappearing into thin air fade out the end '] (2) BoW (Bag of Words) 벡터 생성 123456789from sklearn.feature_extraction.text import CountVectorizer# filter stop wordsvect = CountVectorizer(tokenizer=None, stop_words='english', analyzer='word').fit(corpus)# tokenize: 문장을 단어로 나누는 기준; stop_words: 불용어 설정bow_vect = vect.fit_transform(corpus) # BoW 벡터 생성word_list = vect.get_feature_names()count_list = bow_vect.toarray().sum(axis=0) 123# 등장한 단어 listword_list ['aa', 'ab', 'abandoned', 'abandons', 'abbott', 'abbottnow', 'abbottphone', 'abbotts', 'abend', 'able', 'aboott', 'aboutfaces', 'absolutely', 'abyss', 'accelerating', 'accented', 'accepting', 'accepts', 'access', 'accompanying', 'accomplished', 'account', 'acknowledged', 'act', 'activity', 'actually', 'address', 'adjust', 'adrenaline', 'affirmative', 'afford', 'afraid', 'afternoon', 'againi', 'agencies', 'agency', 'agent', 'agents', 'agitated', 'ago', 'agreement', 'agrees', 'ah', 'ahead', 'aim', 'aimed', 'air', 'airport', 'alarm', 'alert', 'alexanderplatz', 'alive', 'alley', 'alleys', 'allocation', 'allpoints', 'alongside', 'alps', 'alright', 'alrighti', 'alrighttake', 'american', 'ammo', 'amnesia', 'amused', 'anger', 'angle', 'ankle', 'anonymous', 'answer', 'answering', 'answers', 'anymore', 'anythings', 'apology', 'appealing', 'appear', 'appearing', 'appears', 'approached', 'approaches', 'approaching', 'archive', 'archives', 'area', 'arm', 'armed', 'arms', 'army', 'arrested', 'arrivals', 'arrived', 'arrives', 'arriving', 'article', 'asap', 'ashes', 'aside', 'ask', 'asked', 'asking', 'asks', 'asleep', 'ass', 'assassinated', 'assholes', 'assigned', 'assignment', 'assistant', 'associated', 'attempt', 'attention', 'autobahn', 'automatic', 'available', 'aware', 'away', 'awful', 'awhile', 'ba', 'backhanding', 'backing', 'backpack', 'backpacks', 'backs', 'backseat', 'backyard', 'bad', 'bag', 'bags', 'bail', 'bailing', 'bakery', 'balance', 'ball', 'balling', 'band', 'bang', 'banging', 'bank', 'banking', 'banks', 'bar', 'barely', 'bargain', 'barge', 'barn', 'base', 'basement', 'basic', 'basically', 'bastards', 'bathroom', 'battery', 'bb', 'bc', 'beach', 'bearing', 'bearings', 'beat', 'beats', 'beattocrap', 'bed', 'bedroom', 'beep', 'beeping', 'beggars', 'begging', 'begin', 'begins', 'behavior', 'behaviors', 'believe', 'believed', 'belongings', 'belt', 'beltway', 'bench', 'bends', 'beneath', 'berlin', 'better', 'bg', 'big', 'bigger', 'binders', 'binocular', 'birch', 'bit', 'bits', 'black', 'blade', 'blades', 'blam', 'blaming', 'blamm', 'blank', 'bleeding', 'blending', 'blinded', 'blindsided', 'blinking', 'blinks', 'block', 'blocked', 'blocking', 'blocks', 'blonde', 'blood', 'bloody', 'blow', 'blowing', 'blown', 'blows', 'blue', 'blueprint', 'blur', 'blurry', 'bmw', 'board', 'boarded', 'body', 'bogus', 'bomb', 'bombing', 'bone', 'book', 'boom', 'booooomm', 'booth', 'border', 'boss', 'bothering', 'bottle', 'boulevard', 'bouncing', 'bound', 'bourne', 'bourneabbott', 'bournephone', 'bournes', 'box', 'boxed', 'boxes', 'braced', 'braces', 'bracing', 'brakes', 'brandenburg', 'brass', 'breakdown', 'breaker', 'breaking', 'breaks', 'breath', 'breathe', 'breathing', 'breathless', 'brecker', 'bridge', 'briefcase', 'briefed', 'briefing', 'bright', 'bring', 'briskly', 'bristles', 'broadcoasting', 'bruise', 'budget', 'building', 'bulge', 'bulging', 'bulkhead', 'bullet', 'bulletin', 'bullets', 'bullpen', 'bumpers', 'bunch', 'bundle', 'burly', 'burn', 'burning', 'burst', 'bus', 'business', 'bust', 'busted', 'busy', 'button', 'buttoning', 'buy', 'buying', 'buzzkill', 'cab', 'cabbie', 'cabby', 'cabin', 'cabinet', 'cable', 'cabled', 'caffeine', 'calculated', 'calendar', 'caliber', 'called', 'calling', 'calls', 'calm', 'calmly', 'calms', 'came', 'camera', 'cameras', 'campground', 'canhow', 'canvas', 'cap', 'captain', 'caption', 'car', 'carabinieri', 'carabinieris', 'card', 'cards', 'care', 'careening', 'careful', 'carefully', 'careless', 'carnage', 'carries', 'carrying', 'cars', 'carves', 'cascading', 'case', 'cash', 'caspian', 'caspiexpetroleum', 'cast', 'casual', 'casually', 'catches', 'catching', 'caught', 'cause', 'caution', 'caves', 'cd', 'cell', 'cellphone', 'cement', 'center', 'ceo', 'certainly', 'chair', 'chairs', 'chalkboard', 'championed', 'championship', 'chance', 'change', 'changed', 'changes', 'chaos', 'charge', 'charges', 'charmed', 'chase', 'chasing', 'chatter', 'chatting', 'check', 'checked', 'checking', 'checkoff', 'checks', 'cherbourg', 'childlike', 'chill', 'chilly', 'chinese', 'chirping', 'choice', 'choices', 'chokehold', 'chokes', 'chop', 'choreographed', 'christ', 'chucked', 'chugging', 'ci', 'cia', 'cigarette', 'cigarettes', 'cinch', 'circles', 'city', 'civvies', 'claimed', 'clean', 'cleaning', 'clear', 'clearance', 'clearing', 'clearly', 'clears', 'clenching', 'clerk', 'clerks', 'clever', 'click', 'clicking', 'clicks', 'cliff', 'climb', 'climbing', 'clip', 'clipping', 'clock', 'clogging', 'cloned', 'close', 'closed', 'closequarter', 'closer', 'closes', 'closing', 'clothes', 'club', 'clubhouse', 'clue', 'cluster', 'clutter', 'cluttered', 'coal', 'coat', 'code', 'coding', 'coffee', 'cold', 'colonial', 'colored', 'come', 'comes', 'comfortable', 'coming', 'comm', 'command', 'commanders', 'common', 'commotion', 'communications', 'commuters', 'companies', 'company', 'comparison', 'complaining', 'compressing', 'compulsive', 'computer', 'computers', 'comrade', 'concentrating', 'concentric', 'concerned', 'concerning', 'concierge', 'concrete', 'condition', 'conditions', 'conductors', 'conferring', 'confidentially', 'confirm', 'confusion', 'conjunction', 'conjure', 'conklin', 'conklins', 'connections', 'consider', 'considering', 'consist', 'console', 'construction', 'consulate', 'contact', 'contd', 'continents', 'continues', 'continuing', 'contract', 'control', 'controlled', 'converge', 'conversation', 'convinced', 'convoy', 'coo', 'cool', 'cooly', 'coordinate', 'cop', 'cops', 'corner', 'corral', 'corridor', 'cots', 'cottage', 'coughing', 'counter', 'countering', 'counterintelligence', 'counting', 'country', 'countryside', 'couple', 'course', 'courtesy', 'cover', 'covered', 'covering', 'coworkers', 'cranking', 'cranks', 'crap', 'crash', 'crashes', 'crashing', 'crazy', 'credentials', 'credit', 'creepy', 'crestfallen', 'crewcut', 'crime', 'crinkles', 'crisp', 'crissake', 'cronin', 'croninradio', 'cross', 'crosses', 'crossing', 'crouches', 'crowd', 'crowded', 'cruising', 'crush', 'crushed', 'crushes', 'crying', 'cuffed', 'cuffs', 'cup', 'curb', 'curiosity', 'curious', 'curtains', 'curving', 'customs', 'cut', 'cuts', 'cutting', 'cyrillic', 'da', 'dad', 'damn', 'dangerous', 'dangle', 'daniel', 'danka', 'danny', 'dark', 'darkened', 'darkness', 'dash', 'dashboards', 'data', 'database', 'date', 'david', 'day', 'days', 'dead', 'deadly', 'deal', 'dealing', 'death', 'debrief', 'debriefed', 'decent', 'decide', 'decided', 'decision', 'decives', 'deck', 'deep', 'defensive', 'defiant', 'definitely', 'definitive', 'delta', 'deltas', 'departure', 'departures', 'depression', 'deputy', 'descend', 'deserted', 'desk', 'desolate', 'desperation', 'destiny', 'destroy', 'destroys', 'detailed', 'details', 'detained', 'determined', 'detonation', 'device', 'diagnosis', 'diagnostic', 'dial', 'dialed', 'did', 'didi', 'didjason', 'didnt', 'die', 'died', 'diei', 'dies', 'different', 'digital', 'digs', 'direct', 'directions', 'directly', 'director', 'disappear', 'disappearing', 'disappears', 'disappointed', 'disaster', 'discreet', 'discreetly', 'disembark', 'disembarking', 'disembarks', 'disgusted', 'dishes', 'disputing', 'distance', 'distinctive', 'distribution', 'disturbance', 'ditch', 'dividing', 'diving', 'dobermans', 'doctor', 'document', 'dod', 'dodging', 'dods', 'does', 'doesnt', 'doing', 'dollar', 'dollars', 'domestic', 'dominant', 'dominates', 'donnie', 'dont', 'door', 'doorbell', 'doorman', 'doors', 'doorway', 'double', 'doublecrossed', 'doubt', 'downhill', 'downs', 'dozens', 'drab', 'drawn', 'dread', 'dream', 'dresser', 'drifting', 'drill', 'drills', 'drink', 'drinks', 'drive', 'driver', 'driverphone', 'drivers', 'drives', 'driving', 'drone', 'drop', 'dropping', 'drops', 'drugs', 'drunk', 'ducked', 'ducking', 'duct', 'dude', 'dudes', 'duffel', 'duffle', 'dumping', 'dunk', 'ear', 'earlier', 'early', 'earpiece', 'ears', 'easing', 'east', 'easy', 'eat', 'edge', 'edges', 'ef', 'effective', 'effort', 'effortless', 'electric', 'electrical', 'element', 'elevator', 'eluded', 'embankment', 'embarrassed', 'emerge', 'emergency', 'emerges', 'emerging', 'emphatic', 'emptied', 'end', 'ends', 'energy', 'engine', 'engines', 'english', 'enjoy', 'enter', 'entering', 'enters', 'entombed', 'entrance', 'entrances', 'equal', 'equally', 'escape', 'escort', 'ether', 'europe', 'european', 'euros', 'evac', 'event', 'everybody', 'everyday', 'evidence', 'ex', 'exactly', 'exceptionally', 'excited', 'excuse', 'exfil', 'exit', 'exiting', 'exits', 'exnavyseal', 'expensive', 'expert', 'explosion', 'explosive', 'express', 'extreme', 'extremely', 'eye', 'eyes', 'facade', 'facades', 'face', 'faces', 'fact', 'factories', 'factoring', 'fade', 'fading', 'fail', 'failed', 'failing', 'fair', 'fake', 'fakes', 'faking', 'fall', 'fallen', 'falling', 'falls', 'familiar', 'families', 'family', 'fan', 'fanning', 'far', 'fast', 'faster', 'father', 'fatherly', 'fault', 'faux', 'favorite', 'fax', 'fear', 'feed', 'feeding', 'feel', 'feels', 'feet', 'fell', 'felt', 'fender', 'fenders', 'fernsehturm', 'ferry', 'field', 'fiftyeuro', 'fighting', 'fights', 'figure', 'file', 'filei', 'files', 'fileyour', 'filing', 'filling', 'fills', 'final', 'finally', 'financial', 'finding', 'finds', 'fine', 'finger', 'fingerprint', 'fingers', 'finish', 'finished', 'finishes', 'finishing', 'fired', 'fires', 'firing', 'firmly', 'fishing', 'fishtailing', 'fix', 'fixed', 'flag', 'flames', 'flanking', 'flash', 'flashback', 'flashes', 'flashing', 'flat', 'flatout', 'fleeing', 'flexcuffs', 'flicker', 'flickering', 'flicking', 'flies', 'flight', 'flights', 'flimsy', 'flips', 'floated', 'flood', 'flooding', 'floor', 'flooring', 'floors', 'flow', 'fly', 'flying', 'focused', 'focusing', 'foggy', 'folder', 'folding', 'folks', 'follow', 'followed', 'following', 'follows', 'food', 'fooled', 'foot', 'footing', 'footlocker', 'footpath', 'footstep', 'footsteps', 'force', 'foreboding', 'forehead', 'forest', 'forever', 'forget', 'forging', 'forgive', 'forgotten', 'fork', 'forlorn', 'forties', 'fortune', 'forward', 'fourgam', 'fourth', 'frantic', 'freaked', 'free', 'freezes', 'freezing', 'frequency', 'fresh', 'fridge', 'friendly', 'friends', 'frowns', 'frozen', 'frustration', 'fry', 'fuck', 'fucking', 'fully', 'fumbling', 'funky', 'furious', 'fury', 'fusty', 'futile', 'gables', 'gadgetry', 'gaining', 'game', 'gangster', 'gap', 'gas', 'gasolinestoked', 'gasping', 'gassoaked', 'gate', 'gathered', 'gathering', 'gauge', 'gaze', 'gear', 'gears', 'geneva', 'genevai', 'genoa', 'gentlemen', 'german', 'germans', 'germany', 'gestures', 'gets', 'getting', 'gift', 'gimme', 'girl', 'girlfriend', 'girls', 'given', 'gives', 'giving', 'glacial', 'glance', 'glanced', 'glances', 'glancing', ...] 123# 각 단어의 씬별 등장 횟수bow_vect.toarray() array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=int64) 1bow_vect.shape (320, 2850) 123# 각 단어의 총 등장 횟수 (모든 씬에서의 등장 횟수의 합)count_list # BoW array의 각 column에 대해서 모든 row의 합을 구하기 array([ 3, 3, 2, ..., 1, 42, 3], dtype=int64) 1234# \"단어\" - \"총 등장 횟수\" Matchingword_count_dict = dict(zip(word_list, count_list))word_count_dict {'aa': 3, 'ab': 3, 'abandoned': 2, 'abandons': 1, 'abbott': 128, 'abbottnow': 1, 'abbottphone': 4, 'abbotts': 3, 'abend': 2, 'able': 1, 'aboott': 1, 'aboutfaces': 1, 'absolutely': 1, 'abyss': 1, 'accelerating': 1, 'accented': 1, 'accepting': 1, 'accepts': 1, 'access': 3, 'accompanying': 1, 'accomplished': 1, 'account': 3, 'acknowledged': 1, 'act': 1, 'activity': 2, 'actually': 3, 'address': 3, 'adjust': 1, 'adrenaline': 1, 'affirmative': 1, 'afford': 1, 'afraid': 1, 'afternoon': 1, 'againi': 1, 'agencies': 1, 'agency': 5, 'agent': 2, 'agents': 2, 'agitated': 1, 'ago': 10, 'agreement': 2, 'agrees': 1, 'ah': 1, 'ahead': 17, 'aim': 1, 'aimed': 1, 'air': 9, 'airport': 2, 'alarm': 3, 'alert': 1, 'alexanderplatz': 3, 'alive': 9, 'alley': 2, 'alleys': 1, 'allocation': 1, 'allpoints': 1, 'alongside': 1, 'alps': 1, 'alright': 2, 'alrighti': 1, 'alrighttake': 1, 'american': 4, 'ammo': 1, 'amnesia': 5, 'amused': 1, 'anger': 1, 'angle': 2, 'ankle': 1, 'anonymous': 3, 'answer': 8, 'answering': 1, 'answers': 7, 'anymore': 3, 'anythings': 1, 'apology': 2, 'appealing': 1, 'appear': 1, 'appearing': 1, 'appears': 1, 'approached': 1, 'approaches': 2, 'approaching': 1, 'archive': 1, 'archives': 2, 'area': 5, 'arm': 2, 'armed': 2, 'arms': 3, 'army': 1, 'arrested': 1, 'arrivals': 1, 'arrived': 3, 'arrives': 8, 'arriving': 3, 'article': 1, 'asap': 1, 'ashes': 1, 'aside': 2, 'ask': 1, 'asked': 1, 'asking': 2, 'asks': 1, 'asleep': 1, 'ass': 7, 'assassinated': 2, 'assholes': 1, 'assigned': 1, 'assignment': 1, 'assistant': 1, 'associated': 1, 'attempt': 1, 'attention': 3, 'autobahn': 1, 'automatic': 3, 'available': 1, 'aware': 1, 'away': 48, 'awful': 3, 'awhile': 2, 'ba': 2, 'backhanding': 1, 'backing': 5, 'backpack': 4, 'backpacks': 2, 'backs': 2, 'backseat': 4, 'backyard': 1, 'bad': 5, 'bag': 18, 'bags': 2, 'bail': 1, 'bailing': 2, 'bakery': 1, 'balance': 2, 'ball': 1, 'balling': 1, 'band': 1, 'bang': 1, 'banging': 1, 'bank': 3, 'banking': 1, 'banks': 1, 'bar': 1, 'barely': 8, 'bargain': 1, 'barge': 9, 'barn': 1, 'base': 2, 'basement': 2, 'basic': 1, 'basically': 1, 'bastards': 1, 'bathroom': 7, 'battery': 1, 'bb': 2, 'bc': 1, 'beach': 10, 'bearing': 1, 'bearings': 1, 'beat': 22, 'beats': 2, 'beattocrap': 1, 'bed': 9, 'bedroom': 2, 'beep': 2, 'beeping': 2, 'beggars': 1, 'begging': 4, 'begin': 2, 'begins': 9, 'behavior': 1, 'behaviors': 1, 'believe': 4, 'believed': 6, 'belongings': 1, 'belt': 1, 'beltway': 4, 'bench': 1, 'bends': 1, 'beneath': 1, 'berlin': 37, 'better': 7, 'bg': 1, 'big': 17, 'bigger': 3, 'binders': 1, 'binocular': 1, 'birch': 1, 'bit': 6, 'bits': 1, 'black': 15, 'blade': 1, 'blades': 1, 'blam': 2, 'blaming': 1, 'blamm': 1, 'blank': 2, 'bleeding': 1, 'blending': 1, 'blinded': 1, 'blindsided': 1, 'blinking': 2, 'blinks': 2, 'block': 3, 'blocked': 2, 'blocking': 2, 'blocks': 1, 'blonde': 1, 'blood': 6, 'bloody': 2, 'blow': 1, 'blowing': 4, 'blown': 3, 'blows': 4, 'blue': 3, 'blueprint': 2, 'blur': 3, 'blurry': 1, 'bmw': 4, 'board': 2, 'boarded': 1, 'body': 9, 'bogus': 1, 'bomb': 1, 'bombing': 1, 'bone': 1, 'book': 2, 'boom': 1, 'booooomm': 1, 'booth': 3, 'border': 2, 'boss': 2, 'bothering': 1, 'bottle': 3, 'boulevard': 4, 'bouncing': 1, 'bound': 1, 'bourne': 455, 'bourneabbott': 1, 'bournephone': 5, 'bournes': 29, 'box': 5, 'boxed': 1, 'boxes': 1, 'braced': 1, 'braces': 1, 'bracing': 2, 'brakes': 3, 'brandenburg': 1, 'brass': 1, 'breakdown': 1, 'breaker': 1, 'breaking': 1, 'breaks': 1, 'breath': 2, 'breathe': 1, 'breathing': 1, 'breathless': 1, 'brecker': 9, 'bridge': 13, 'briefcase': 8, 'briefed': 1, 'briefing': 1, 'bright': 2, 'bring': 2, 'briskly': 2, 'bristles': 1, 'broadcoasting': 1, 'bruise': 1, 'budget': 1, 'building': 20, 'bulge': 1, 'bulging': 1, 'bulkhead': 2, 'bullet': 1, 'bulletin': 1, 'bullets': 2, 'bullpen': 5, 'bumpers': 1, 'bunch': 1, 'bundle': 1, 'burly': 1, 'burn': 1, 'burning': 2, 'burst': 1, 'bus': 3, 'business': 3, 'bust': 1, 'busted': 1, 'busy': 7, 'button': 1, 'buttoning': 1, 'buy': 7, 'buying': 2, 'buzzkill': 1, 'cab': 24, 'cabbie': 3, 'cabby': 1, 'cabin': 1, 'cabinet': 3, 'cable': 1, 'cabled': 1, 'caffeine': 1, 'calculated': 1, 'calendar': 1, 'caliber': 1, 'called': 3, 'calling': 1, 'calls': 1, 'calm': 4, 'calmly': 2, 'calms': 1, 'came': 10, 'camera': 3, 'cameras': 1, 'campground': 1, 'canhow': 1, 'canvas': 1, 'cap': 1, 'captain': 2, 'caption': 1, 'car': 51, 'carabinieri': 5, 'carabinieris': 1, 'card': 1, 'cards': 2, 'care': 3, 'careening': 1, 'careful': 2, 'carefully': 2, 'careless': 1, 'carnage': 1, 'carries': 1, 'carrying': 6, 'cars': 20, 'carves': 1, 'cascading': 1, 'case': 7, 'cash': 6, 'caspian': 1, 'caspiexpetroleum': 1, 'cast': 1, 'casual': 2, 'casually': 1, 'catches': 1, 'catching': 3, 'caught': 2, 'cause': 1, 'caution': 1, 'caves': 1, 'cd': 1, 'cell': 7, 'cellphone': 2, 'cement': 1, 'center': 3, 'ceo': 1, 'certainly': 1, 'chair': 9, 'chairs': 1, 'chalkboard': 1, 'championed': 1, 'championship': 1, 'chance': 1, 'change': 4, 'changed': 3, 'changes': 2, 'chaos': 2, 'charge': 7, 'charges': 2, 'charmed': 1, 'chase': 2, 'chasing': 1, 'chatter': 1, 'chatting': 1, 'check': 6, 'checked': 2, 'checking': 13, 'checkoff': 1, 'checks': 12, 'cherbourg': 1, 'childlike': 1, 'chill': 1, 'chilly': 1, 'chinese': 2, 'chirping': 1, 'choice': 5, 'choices': 1, 'chokehold': 1, 'chokes': 1, 'chop': 2, 'choreographed': 1, 'christ': 2, 'chucked': 1, 'chugging': 2, 'ci': 2, 'cia': 7, 'cigarette': 1, 'cigarettes': 1, 'cinch': 1, 'circles': 2, 'city': 4, 'civvies': 1, 'claimed': 1, 'clean': 7, 'cleaning': 1, 'clear': 9, 'clearance': 1, 'clearing': 1, 'clearly': 5, 'clears': 2, 'clenching': 1, 'clerk': 8, 'clerks': 1, 'clever': 1, 'click': 6, 'clicking': 1, 'clicks': 1, 'cliff': 1, 'climb': 1, 'climbing': 4, 'clip': 3, 'clipping': 1, 'clock': 4, 'clogging': 1, 'cloned': 2, 'close': 13, 'closed': 3, 'closequarter': 1, 'closer': 4, 'closes': 2, 'closing': 3, 'clothes': 7, 'club': 3, 'clubhouse': 1, 'clue': 1, 'cluster': 1, 'clutter': 1, 'cluttered': 1, 'coal': 2, 'coat': 7, 'code': 2, 'coding': 1, 'coffee': 5, 'cold': 4, 'colonial': 1, 'colored': 1, 'come': 18, 'comes': 10, 'comfortable': 1, 'coming': 30, 'comm': 2, 'command': 1, 'commanders': 1, 'common': 1, 'commotion': 1, 'communications': 2, 'commuters': 1, 'companies': 1, 'company': 3, 'comparison': 1, 'complaining': 1, 'compressing': 1, 'compulsive': 1, 'computer': 7, 'computers': 2, 'comrade': 1, 'concentrating': 1, 'concentric': 1, 'concerned': 2, 'concerning': 1, 'concierge': 3, 'concrete': 3, 'condition': 1, 'conditions': 1, 'conductors': 1, 'conferring': 2, 'confidentially': 1, 'confirm': 1, 'confusion': 3, 'conjunction': 1, 'conjure': 1, 'conklin': 29, 'conklins': 4, 'connections': 1, 'consider': 1, 'considering': 1, 'consist': 1, 'console': 1, 'construction': 2, 'consulate': 2, 'contact': 3, 'contd': 63, 'continents': 1, 'continues': 3, 'continuing': 8, 'contract': 1, 'control': 2, 'controlled': 1, 'converge': 1, 'conversation': 3, 'convinced': 1, 'convoy': 1, 'coo': 1, 'cool': 3, 'cooly': 1, 'coordinate': 1, 'cop': 5, 'cops': 22, 'corner': 13, 'corral': 1, 'corridor': 1, 'cots': 1, 'cottage': 1, 'coughing': 1, 'counter': 2, 'countering': 1, 'counterintelligence': 2, 'counting': 1, 'country': 1, 'countryside': 1, 'couple': 4, 'course': 1, 'courtesy': 1, 'cover': 9, 'covered': 2, 'covering': 1, 'coworkers': 1, 'cranking': 1, 'cranks': 1, 'crap': 2, 'crash': 2, 'crashes': 1, 'crashing': 2, 'crazy': 4, 'credentials': 1, 'credit': 1, 'creepy': 1, 'crestfallen': 1, 'crewcut': 1, 'crime': 2, 'crinkles': 1, 'crisp': 1, 'crissake': 1, 'cronin': 81, 'croninradio': 1, 'cross': 2, 'crosses': 3, 'crossing': 2, 'crouches': 1, 'crowd': 7, 'crowded': 2, 'cruising': 1, 'crush': 1, 'crushed': 1, 'crushes': 1, 'crying': 4, 'cuffed': 2, 'cuffs': 2, 'cup': 2, 'curb': 3, 'curiosity': 1, 'curious': 1, 'curtains': 2, 'curving': 1, 'customs': 1, 'cut': 2, 'cuts': 2, 'cutting': 3, 'cyrillic': 2, 'da': 1, 'dad': 1, 'damn': 2, 'dangerous': 3, 'dangle': 1, 'daniel': 1, 'danka': 1, 'danny': 8, 'dark': 16, 'darkened': 1, 'darkness': 2, 'dash': 2, 'dashboards': 1, 'data': 3, 'database': 1, 'date': 2, 'david': 1, 'day': 4, 'days': 2, 'dead': 22, 'deadly': 1, 'deal': 3, 'dealing': 1, 'death': 4, 'debrief': 2, 'debriefed': 1, 'decent': 1, 'decide': 2, 'decided': 1, 'decision': 1, 'decives': 1, 'deck': 2, 'deep': 2, 'defensive': 2, 'defiant': 1, 'definitely': 1, 'definitive': 3, 'delta': 12, 'deltas': 1, 'departure': 2, 'departures': 1, 'depression': 1, 'deputy': 2, 'descend': 1, 'deserted': 2, 'desk': 17, 'desolate': 2, 'desperation': 2, 'destiny': 1, 'destroy': 1, 'destroys': 1, 'detailed': 1, 'details': 2, 'detained': 1, 'determined': 1, 'detonation': 1, 'device': 2, 'diagnosis': 1, 'diagnostic': 1, 'dial': 1, 'dialed': 1, 'did': 15, 'didi': 1, 'didjason': 1, 'didnt': 11, 'die': 1, 'died': 3, 'diei': 1, 'dies': 1, 'different': 2, 'digital': 1, 'digs': 1, 'direct': 1, 'directions': 1, 'directly': 1, 'director': 2, 'disappear': 6, 'disappearing': 1, 'disappears': 3, 'disappointed': 1, 'disaster': 3, 'discreet': 2, 'discreetly': 2, 'disembark': 1, 'disembarking': 2, 'disembarks': 1, 'disgusted': 1, 'dishes': 1, 'disputing': 1, 'distance': 4, 'distinctive': 1, 'distribution': 1, 'disturbance': 1, 'ditch': 1, 'dividing': 1, 'diving': 1, 'dobermans': 2, 'doctor': 1, 'document': 1, 'dod': 3, 'dodging': 1, 'dods': 1, 'does': 9, 'doesnt': 11, 'doing': 20, 'dollar': 2, 'dollars': 4, 'domestic': 1, 'dominant': 1, 'dominates': 1, 'donnie': 1, 'dont': 42, 'door': 31, 'doorbell': 1, 'doorman': 2, 'doors': 9, 'doorway': 3, 'double': 1, 'doublecrossed': 1, 'doubt': 1, 'downhill': 1, 'downs': 1, 'dozens': 1, 'drab': 1, 'drawn': 4, 'dread': 2, 'dream': 4, 'dresser': 1, 'drifting': 2, 'drill': 3, 'drills': 2, 'drink': 1, 'drinks': 1, 'drive': 3, 'driver': 20, 'driverphone': 1, 'drivers': 3, 'drives': 4, 'driving': 12, 'drone': 1, 'drop': 4, 'dropping': 3, 'drops': 3, 'drugs': 1, 'drunk': 4, 'ducked': 1, 'ducking': 1, 'duct': 2, 'dude': 2, 'dudes': 6, 'duffel': 1, 'duffle': 3, 'dumping': 1, 'dunk': 1, 'ear': 1, 'earlier': 1, 'early': 5, 'earpiece': 3, 'ears': 1, 'easing': 1, 'east': 2, 'easy': 2, 'eat': 1, 'edge': 2, 'edges': 1, 'ef': 1, 'effective': 1, 'effort': 1, 'effortless': 1, 'electric': 1, 'electrical': 3, 'element': 1, 'elevator': 8, 'eluded': 1, 'embankment': 1, 'embarrassed': 2, 'emerge': 1, 'emergency': 1, 'emerges': 1, 'emerging': 2, 'emphatic': 1, 'emptied': 1, 'end': 10, 'ends': 2, 'energy': 1, 'engine': 1, 'engines': 1, 'english': 2, 'enjoy': 1, 'enter': 2, 'entering': 1, 'enters': 10, 'entombed': 1, 'entrance': 2, 'entrances': 1, 'equal': 1, 'equally': 1, 'escape': 1, 'escort': 3, 'ether': 1, 'europe': 2, 'european': 1, 'euros': 1, 'evac': 2, 'event': 1, 'everybody': 3, 'everyday': 1, 'evidence': 1, 'ex': 1, 'exactly': 7, 'exceptionally': 1, 'excited': 2, 'excuse': 1, 'exfil': 6, 'exit': 2, 'exiting': 1, 'exits': 3, 'exnavyseal': 1, 'expensive': 1, 'expert': 1, 'explosion': 1, 'explosive': 3, 'express': 1, 'extreme': 1, 'extremely': 2, 'eye': 6, 'eyes': 25, 'facade': 2, 'facades': 1, 'face': 22, 'faces': 6, 'fact': 2, 'factories': 1, 'factoring': 1, 'fade': 2, 'fading': 1, 'fail': 1, 'failed': 1, 'failing': 1, 'fair': 1, 'fake': 1, 'fakes': 1, 'faking': 1, 'fall': 3, 'fallen': 1, 'falling': 2, 'falls': 1, 'familiar': 2, 'families': 1, 'family': 5, 'fan': 1, 'fanning': 1, 'far': 7, 'fast': 13, 'faster': 1, 'father': 3, 'fatherly': 1, 'fault': 1, 'faux': 1, 'favorite': 1, 'fax': 2, 'fear': 4, 'feed': 2, 'feeding': 1, 'feel': 4, 'feels': 1, 'feet': 3, 'fell': 3, 'felt': 4, 'fender': 1, 'fenders': 1, 'fernsehturm': 1, 'ferry': 2, 'field': 6, 'fiftyeuro': 1, 'fighting': 1, 'fights': 1, 'figure': 3, 'file': 8, 'filei': 1, 'files': 11, 'fileyour': 1, 'filing': 1, 'filling': 1, 'fills': 1, 'final': 5, 'finally': 11, 'financial': 2, 'finding': 3, 'finds': 1, 'fine': 2, 'finger': 4, 'fingerprint': 3, 'fingers': 1, 'finish': 2, 'finished': 1, 'finishes': 1, 'finishing': 1, 'fired': 1, 'fires': 1, 'firing': 4, 'firmly': 1, 'fishing': 2, 'fishtailing': 1, 'fix': 2, 'fixed': 3, 'flag': 2, 'flames': 1, 'flanking': 1, 'flash': 2, 'flashback': 7, 'flashes': 3, 'flashing': 2, 'flat': 2, 'flatout': 1, 'fleeing': 1, 'flexcuffs': 2, 'flicker': 3, 'flickering': 1, 'flicking': 1, 'flies': 1, 'flight': 1, 'flights': 1, 'flimsy': 1, 'flips': 2, 'floated': 1, 'flood': 1, 'flooding': 1, 'floor': 10, 'flooring': 1, 'floors': 1, 'flow': 1, 'fly': 1, 'flying': 2, 'focused': 3, 'focusing': 1, 'foggy': 1, 'folder': 1, 'folding': 2, 'folks': 1, 'follow': 3, 'followed': 1, 'following': 2, 'follows': 3, 'food': 4, 'fooled': 2, 'foot': 4, 'footing': 1, 'footlocker': 2, 'footpath': 2, 'footstep': 1, 'footsteps': 1, 'force': 2, 'foreboding': 1, 'forehead': 1, 'forest': 1, 'forever': 1, 'forget': 3, 'forging': 1, 'forgive': 1, 'forgotten': 2, 'fork': 2, 'forlorn': 1, 'forties': 1, 'fortune': 1, 'forward': 6, 'fourgam': 1, 'fourth': 1, 'frantic': 1, 'freaked': 3, 'free': 4, 'freezes': 1, 'freezing': 2, 'frequency': 1, 'fresh': 4, 'fridge': 1, 'friendly': 1, 'friends': 2, 'frowns': 1, 'frozen': 2, 'frustration': 1, 'fry': 1, 'fuck': 7, 'fucking': 1, 'fully': 3, 'fumbling': 1, 'funky': 1, 'furious': 1, 'fury': 2, 'fusty': 1, 'futile': 1, 'gables': 1, 'gadgetry': 1, 'gaining': 2, 'game': 4, 'gangster': 1, 'gap': 1, 'gas': 5, 'gasolinestoked': 1, 'gasping': 1, 'gassoaked': 1, 'gate': 1, 'gathered': 2, 'gathering': 1, 'gauge': 1, 'gaze': 2, 'gear': 9, 'gears': 1, 'geneva': 1, 'genevai': 1, 'genoa': 1, 'gentlemen': 1, 'german': 7, 'germans': 1, 'germany': 2, 'gestures': 3, 'gets': 7, 'getting': 16, 'gift': 1, 'gimme': 1, 'girl': 6, 'girlfriend': 1, 'girls': 1, 'given': 1, 'gives': 4, 'giving': 3, 'glacial': 1, 'glance': 1, 'glanced': 1, 'glances': 1, 'glancing': 8, ...} 12345# 등장 횟수 (count) 순으로 정렬import operatorsorted(word_count_dict.items(), key = operator.itemgetter(1), reverse = True)[:5] [('bourne', 455), ('pamela', 199), ('abbott', 128), ('hes', 100), ('kirill', 93)] (3) 단어 분포 탐색 12plt.hist(list(word_count_dict.values()), bins=150)plt.show() 대부분의 단어가 0번~50번 사이에 등장했고, 일부 소수의 단어들이 100번 이상 등장한 것을 확인할 수 있습니다. 4. 택스트 마이닝 4-1. 단어별 빈도 분석 (+ Word Cloud) (1) 상위 빈도수 단어 출력 1234# word_count_dict중 상위 25 tags 확인해보기ranked_tags = Counter(word_count_dict).most_common(25)ranked_tags [('bourne', 455), ('pamela', 199), ('abbott', 128), ('hes', 100), ('kirill', 93), ('nicky', 90), ('cronin', 81), ('just', 80), ('marie', 67), ('contd', 63), ('know', 61), ('car', 51), ('away', 48), ('room', 44), ('jarda', 43), ('looks', 43), ('dont', 42), ('zorn', 42), ('phone', 40), ('right', 39), ('theres', 39), ('police', 38), ('want', 38), ('berlin', 37), ('teddy', 35)] (2) Word Cloud 시각화 1!pip install pytagcloud pygame simplejson Collecting pytagcloud Downloading pytagcloud-0.3.5.tar.gz (754 kB) Collecting pygame Downloading pygame-1.9.6-cp37-cp37m-win_amd64.whl (4.3 MB) Collecting simplejson Downloading simplejson-3.17.2-cp37-cp37m-win_amd64.whl (73 kB) Building wheels for collected packages: pytagcloud Building wheel for pytagcloud (setup.py): started Building wheel for pytagcloud (setup.py): finished with status 'done' Created wheel for pytagcloud: filename=pytagcloud-0.3.5-py3-none-any.whl size=759873 sha256=0c740b8c183f3dd04c6b6353e75f2307bdcc7855bb0ce66f4caa3ed352b6e8cc Stored in directory: c:\\users\\kimsu\\appdata\\local\\pip\\cache\\wheels\\fc\\fd\\aa\\86956a295a7c9205bafd518ef4b6d489e51d2d476990c18238 Successfully built pytagcloud Installing collected packages: pytagcloud, pygame, simplejson Successfully installed pygame-1.9.6 pytagcloud-0.3.5 simplejson-3.17.2 12345from collections import Counterimport randomimport pytagcloudimport webbrowser pygame 1.9.6 Hello from the pygame community. https://www.pygame.org/contribute.html 내림순으로 상위 N개를 추출하는 두가지 방법: sorted(dict .items(), key = operator.itemgetter(col_index), reverse=True) [:N] Counter(dict .most_common(N)) 1234567# Top 40 단어로 word cloud 생성하기taglist = pytagcloud.make_tags(sorted(word_count_dict.items(), key = operator.itemgetter(1), reverse=True)[:40], maxsize=60) # 빈도수(itemgetter(1)) 내림차순(reverse=True)으로 정렬, maxsize: 글자 크기# taglist = pytagcloud.make_tages(Counter(word_count_dict).most_common(40), maxsize=60)pytagcloud.create_tag_image(taglist, 'movie_wordcloud.jpg', rectangular=False)from IPython.display import ImageImage(filename='movie_wordcloud.jpg') ​ 4-2. 장면별 중요 단어 시각화 (TF-IDF) (1) TF-IDF 변환 Bag of Words 벡터에 대해서 TF-IDF변환 진행합니다. 123456from sklearn.feature_extraction.text import TfidfTransformertfidf_vectorizer = TfidfTransformer()tf_idf_vect = tfidf_vectorizer.fit_transform(bow_vect)print(tf_idf_vect.shape) # 320*2850 vector: 320 scenes, 2850 sentences (320, 2850) 변환 후 320*2850 matrix가 출력됩니다. 여기서 한 행(row)은 한 씬을 의미하고 한 열(column)은 한 단어를 의미합니다. 12# 첫번째 행 출력 (0이 아닌것 만) -- 즉 첫 씬에서 모든 단어의 TF-IDF 값print(tf_idf_vect[0]) (320, 2850) (0, 2788) 0.19578974958217082 (0, 2763) 0.27550455848587985 (0, 2412) 0.1838379942679887 (0, 2387) 0.3109660261831164 (0, 1984) 0.2902223973596984 (0, 1978) 0.3109660261831164 (0, 1898) 0.27550455848587985 (0, 1673) 0.2902223973596984 (0, 1366) 0.21520447034992146 (0, 1251) 0.19855583314180728 (0, 1001) 0.2340173008390438 (0, 974) 0.2902223973596984 (0, 874) 0.27550455848587985 (0, 798) 0.1906694714764746 (0, 237) 0.08646242181596513 (0, 125) 0.26408851574819875 123# (0을 포함한) 실제 vector의 모습 출력해보기print(tf_idf_vect[0].toarray().shape)print(tf_idf_vect[0].toarray()) (1, 2850) [[0. 0. 0. ... 0. 0. 0.]] (2) “벡터” - “단어” mapping 길이가 2850인 단어 벡터의 각 위치가 어떤 단어를 상징하는지를 알아내기 위해 단어 벡터에 대해서 “단어” - “index No.” Mapping 을 진행합니다. 1vect.vocabulary_ {'raining': 1898, 'light': 1366, 'strobes': 2387, 'wet': 2763, 'glass': 1001, 'rhythmic': 1978, 'pace': 1673, 'suddenly': 2412, 'window': 2788, 'face': 798, 'jason': 1251, 'bourne': 237, 'riding': 1984, 'backseat': 125, 'gaze': 974, 'fixed': 874, 'knee': 1297, 'syringe': 2459, 'gun': 1055, 'eyes': 795, 'driver': 703, 'jarda': 1248, 'watching': 2741, 'bournes': 240, 'pov': 1817, 'passenger': 1710, 'head': 1097, 'cell': 351, 'phone': 1747, 'rings': 1990, 'turns': 2626, 'conklin': 481, 'returns': 1971, 'stare': 2332, 'open': 1649, 'panicked': 1693, 'gasping': 968, 'trying': 2615, 'stay': 2347, 'quiet': 1886, 'marie': 1454, 'sleeps': 2221, 'moving': 1556, 'medicine': 1482, 'cabinet': 299, 'digs': 640, 'downs': 690, 'specific': 2283, 'minute': 1515, 'later': 1333, 'moves': 1555, 'veranda': 2679, 'pads': 1681, 'moment': 1537, 'concerned': 468, 'clearly': 408, 'time': 2542, 'happened': 1082, 'look': 1405, 'different': 638, 'saw': 2044, 'hair': 1065, 'longer': 1404, 'shes': 2137, 'blonde': 207, 'hippie': 1138, 'travelers': 2592, 'cottage': 512, 'humble': 1174, 'sweet': 2445, 'bedroom': 163, 'opens': 1652, 'beach': 156, 'town': 2570, 'just': 1272, 'hill': 1135, 'club': 432, 'music': 1566, 'night': 1602, 'rave': 1909, 'wafting': 2708, 'far': 821, 'distance': 658, 'car': 321, 'ill': 1193, 'book': 226, 'theres': 2513, 'new': 1595, 'youre': 2844, 'sure': 2428, 'nods': 1607, 'write': 2823, 'years': 2834, 'scribbling': 2070, 'notebook': 1615, 'bad': 127, 'bits': 189, 'pieces': 1760, 'gone': 1012, 'think': 2521, 'maybe': 1470, 'making': 1442, 'worse': 2816, 'dont': 680, 'wonder': 2800, 'lays': 1337, 'hands': 1077, 'shoulders': 2157, 'steadies': 2351, 'sooner': 2268, 'going': 1011, 'remember': 1951, 'good': 1014, 'softens': 2260, 'smiles': 2241, 'kisses': 1293, 'leads': 1342, 'getting': 986, 'bed': 162, 'turning': 2625, 'settled': 2110, 'waiting': 2713, 'pill': 1763, 'kick': 1281, 'im': 1194, 'okay': 1641, 'worry': 2815, 'like': 1369, 'nightmare': 1603, 'mean': 1473, 'try': 2614, 'ignore': 1190, 'hesitates': 1124, 'gets': 985, 'knows': 1310, 'right': 1987, 'opening': 1651, 'hes': 1123, 'letting': 1359, 'resistance': 1963, 'folding': 907, 'childlike': 380, 'gathering': 972, 'contd': 491, 'sleep': 2218, 'better': 181, 'memories': 1487, 'dream': 695, 'having': 1094, 'ends': 752, 'day': 586, 'takes': 2464, 'beat': 159, 'make': 1440, 'silence': 2182, 'strokes': 2389, 'gives': 993, 'tenderness': 2499, 'fading': 804, 'waifs': 2710, 'dark': 577, 'running': 2022, 'sun': 2418, 'punishing': 1865, 'sand': 2038, 'strong': 2391, 'effortless': 737, 'deep': 601, 'focused': 903, 'stunning': 2400, 'conjunction': 479, 'scenery': 2055, 'lost': 1414, 'busy': 289, 'market': 1457, 'fishing': 871, 'lots': 1416, 'young': 2842, 'western': 2761, 'faces': 799, 'rundown': 2021, 'happening': 1083, 'shopping': 2152, 'filling': 852, 'bag': 128, 'local': 1391, 'produce': 1841, 'leaving': 1352, 'putting': 1875, 'groceries': 1040, 'away': 116, 'stops': 2369, 'photograph': 1753, 'windowsill': 2790, 'snapshot': 2250, 'arms': 87, 'protector': 1853, 'big': 183, 'alive': 51, 'love': 1419, 'funky': 955, 'colonial': 444, 'facades': 797, 'vivid': 2698, 'subcontinental': 2404, 'technicolor': 2483, 'loud': 1417, 'morning': 1545, 'traffic': 2576, 'camera': 313, 'finds': 858, 'coming': 449, 'store': 2370, 'bottle': 233, 'water': 2742, 'finished': 864, 'run': 2020, 'standing': 2329, 'chugging': 393, 'checking': 376, 'scene': 2054, 'catches': 344, 'eye': 794, 'street': 2380, 'silver': 2188, 'newish': 1596, 'pulling': 1861, 'block': 203, 'quite': 1888, 'whos': 2779, 'driving': 707, 'casual': 342, 'passing': 1714, 'notice': 1616, 'alert': 49, 'follows': 912, 'foot': 915, 'natural': 1575, 'cruising': 551, 'sidewalk': 2174, 'blending': 198, 'mix': 1525, 'ahead': 43, 'corner': 508, 'slowing': 2233, 'reaches': 1912, 'parked': 1700, 'guy': 1059, 'welldressed': 2757, 'physical': 1755, 'sunglasses': 2420, 'kirill': 1291, 'heading': 1100, 'building': 270, 'telegraph': 2491, 'office': 1632, 'watch': 2739, 'perimeter': 1739, 'mr': 1557, 'mohan': 1534, 'desk': 614, 'crisp': 541, 'proper': 1849, 'man': 1443, 'handed': 1074, 'old': 1642, 'passport': 1715, 'picture': 1758, 'question': 1879, 'sir': 2197, 'sister': 2200, 'death': 592, 'family': 818, 'place': 1772, 'know': 1307, 'called': 306, 'note': 1614, 'table': 2460, 'come': 446, 'read': 1917, 'balling': 135, 'quickly': 1884, 'fact': 800, 'bailing': 131, 'fast': 822, 'calm': 309, 'methodical': 1501, 'exfil': 782, 'procedure': 1839, 'honed': 1150, 'choreographed': 390, 'packing': 1678, 'machine': 1432, 'rapid': 1908, 'cuts': 566, 'backpacks': 123, 'thrown': 2534, 'house': 1163, 'cash': 338, 'pulled': 1860, 'lamp': 1318, 'base': 147, 'credit': 535, 'cards': 325, 'taped': 2470, 'counter': 514, 'bank': 139, 'mission': 1521, 'accomplished': 20, 'starting': 2338, 'glancing': 999, 'nice': 1599, 'easy': 730, 'cool': 503, 'gear': 975, 'makes': 1441, 'slow': 2231, 'pass': 1707, 'marketplace': 1458, 'stripped': 2384, 'thing': 2519, 'shit': 2143, 'missed': 1519, 'looking': 1406, 'earlier': 724, 'jamming': 1246, 'pocket': 1793, 'begins': 169, 'sweep': 2444, 'jogging': 1259, 'keeping': 1273, 'low': 1422, 'neighborhood': 1587, 'alleys': 53, 'random': 1905, 'worked': 2807, 'crowded': 550, 'tourists': 2566, 'sunbathers': 2419, 'favorite': 828, 'spot': 2298, 'talking': 2467, 'women': 2799, 'laughing': 1335, 'happy': 1085, 'burly': 281, 'jeep': 1252, 'comes': 447, 'roaring': 2003, 'spots': 2300, 'parks': 1702, 'end': 751, 'methodically': 1502, 'way': 2746, 'blue': 214, 'tent': 2503, 'towel': 2567, 'opposite': 1657, 'arrives': 92, 'looks': 1407, 'sees': 2089, 'yards': 2831, 'hard': 1086, 'bends': 178, 'gotta': 1018, 'tone': 2552, 'voice': 2702, 'grabs': 1020, 'quick': 1883, 'goodbye': 1015, 'friends': 946, 'hurry': 1177, 'uses': 2667, 'cover': 523, 'retreats': 1969, 'reach': 1911, 'drill': 698, 'tossed': 2559, 'pulls': 1862, 'blown': 212, 'ago': 39, 'fine': 859, 'careful': 328, 'pushed': 1871, 'got': 1017, 'lazy': 1338, 'following': 911, 'main': 1437, 'blocked': 204, 'huge': 1172, 'automatic': 113, 'pistol': 1768, 'travel': 2591, 'narrow': 1573, 'little': 1385, 'passageway': 1708, 'windshield': 2791, 'packed': 1677, 'liking': 1370, 'decide': 596, 'campground': 315, 'yesterday': 2838, 'wrong': 2827, 'rental': 1953, 'dollar': 674, 'sneakers': 2251, 'pull': 1859, 'thats': 2511, 'crazy': 533, 'real': 1921, 'throwing': 2533, 'reverse': 1974, 'hyundai': 1183, 'trapped': 2589, 'gridlock': 1036, 'freezing': 941, 'alley': 52, 'disappear': 645, 'backing': 121, 'came': 312, 'blowing': 211, 'horn': 1156, 'van': 2672, 'blocks': 206, 'leaning': 1344, 'theyve': 2517, 'wait': 2712, 'want': 2726, 'againi': 33, 'clear': 405, 'shack': 2117, 'safe': 2031, 'hang': 1078, 'awhile': 118, 'check': 374, 'wheres': 2773, 'left': 1353, 'places': 1774, 'afford': 30, 'possessed': 1809, 'familiar': 816, 'tactical': 2462, 'patience': 1720, 'doesnt': 672, 'sense': 2098, 'checks': 378, 'rearview': 1927, 'fuck': 951, 'taking': 2465, 'hell': 1117, 'forward': 934, 'blocking': 205, 'drive': 702, 'squeezing': 2313, 'switch': 2455, 'bridge': 259, 'scrambling': 2061, 'seat': 2078, 'squirts': 2316, 'wheel': 2771, 'adrenaline': 28, 'pumping': 1864, 'thirty': 2524, 'skidding': 2208, 'turn': 2624, 'clipping': 421, 'vehicle': 2677, 'mirror': 1517, 'shattering': 2130, 'speeding': 2285, 'scanning': 2050, 'veering': 2676, 'oncoming': 1645, 'bus': 285, 'jesus': 1256, 'yeah': 2832, 'ready': 1920, 'bearing': 157, 'smile': 2240, 'knowing': 1308, 'stopping': 2368, 'short': 2153, 'rise': 1995, 'bit': 188, 'view': 2692, 'half': 1067, 'headed': 1099, 'gonna': 1013, 'lose': 1412, 'kirills': 1292, 'mind': 1513, 'racing': 1890, 'duffle': 720, 'abandons': 3, 'preps': 1825, 'meet': 1484, 'hour': 1161, 'bail': 130, 'follow': 909, 'crosses': 546, 'warned': 2731, 'told': 2551, 'leave': 1350, 'thisit': 2526, 'wont': 2801, 'choice': 385, 'concrete': 471, 'slams': 2216, 'precise': 1819, 'sniper': 2252, 'rifle': 1985, 'hand': 1073, 'spare': 2276, 'clip': 420, 'roll': 2007, 'tell': 2496, 'scope': 2059, 'rumbling': 2019, 'target': 2472, 'drivers': 705, 'headrest': 1103, 'finger': 860, 'firing': 869, 'jerking': 1254, 'fender': 838, 'tearing': 2480, 'guard': 1045, 'rail': 1895, 'cement': 353, 'shards': 2126, 'air': 46, 'reaching': 1913, 'late': 1332, 'finally': 855, 'crashes': 531, 'flimsy': 892, 'guardrail': 1046, 'plummets': 1791, 'splashes': 2295, 'sink': 2194, 'sight': 2177, 'lowers': 1425, 'basically': 150, 'unnoticed': 2646, 'nook': 1611, 'silenced': 2183, 'people': 1737, 'rushing': 2026, 'woman': 2797, 'directly': 643, 'doorway': 685, 'indian': 1211, 'goa': 1006, 'drills': 699, 'sinks': 2195, 'inside': 1215, 'swallowed': 2439, 'scans': 2051, 'surface': 2429, 'river': 2000, 'mud': 1559, 'plumes': 1790, 'settles': 2111, 'tries': 2599, 'urge': 2661, 'killers': 1287, 'unbroken': 2634, 'woodwork': 2803, 'breaks': 253, 'moments': 1538, 'goes': 1010, 'held': 1116, 'jeeps': 1253, 'canvas': 317, 'gulp': 1054, 'frantic': 937, 'unclip': 2636, 'seatbelt': 2079, 'jammed': 1245, 'chucked': 392, 'drifting': 697, 'disappears': 647, 'red': 1935, 'halo': 1071, 'growing': 1043, 'bigger': 184, 'blood': 208, 'pauses': 1723, 'maries': 1455, 'blank': 196, 'dead': 588, 'realizing': 1924, 'pick': 1756, 'briefcase': 260, 'telephoto': 2492, 'lens': 1356, 'teddyradio': 2486, 'vo': 2700, 'seller': 2093, 'arrived': 91, 'berlin': 180, 'chinese': 383, 'restaurant': 1966, 'squarely': 2312, 'seen': 2088, 'enters': 760, 'stark': 2335, 'men': 1489, 'cross': 545, 'square': 2310, 'vic': 2686, 'steelass': 2354, 'intel': 1220, 'operator': 1655, 'carries': 332, 'large': 1330, 'samples': 2037, 'case': 337, 'mike': 1509, 'younger': 2843, 'exnavyseal': 786, 'hub': 1169, 'secure': 2083, 'anonymous': 68, 'space': 2274, 'city': 400, 'shades': 2118, 'drawn': 693, 'cabled': 301, 'stale': 2325, 'improvised': 1203, 'feel': 833, 'temporary': 2498, 'outpost': 1664, 'room': 2014, 'pamela': 1687, 'landy': 1322, 'senior': 2097, 'cia': 395, 'counterintelligence': 516, 'officer': 1633, 'hovering': 1165, 'communications': 455, 'console': 487, 'cronin': 543, 'pamelas': 1689, 'early': 725, 'forties': 932, 'stonecold': 2365, 'facade': 796, 'quarterbacking': 1878, 'operation': 1653, 'radio': 1893, 'kurt': 1312, 'kim': 1289, 'techs': 2484, 'headphones': 1101, 'ruggedized': 2017, 'laptops': 1329, 'comm': 450, 'spread': 2301, 'survey': 2434, 'teddy': 2485, 'military': 1511, 'rig': 1986, 'mobile': 1527, 'motion': 1548, 'shake': 2121, 'tired': 2549, 'coworkers': 526, 'parting': 1704, 'ways': 2747, 'walking': 2720, 'entering': 759, 'doors': 684, 'smiling': 2242, 'approached': 79, 'shift': 2139, 'security': 2085, 'hear': 1107, 'mikeradio': 1510, 'sleeve': 2222, 'earpiece': 726, 'escort': 767, 'command': 451, 'post': 1812, 'works': 2811, 'board': 219, 'teams': 2478, 'listen': 1381, 'final': 854, 'green': 1031, 'listening': 1382, 'word': 2804, 'raises': 1901, 'langley': 1326, 'patched': 1718, 'surprised': 2431, 'martin': 1461, 'mandarins': 1448, 'sit': 2201, 'round': 2016, 'marshall': 1460, 'deputy': 611, 'vicedirector': 2687, 'charge': 367, 'tense': 2501, 'donnie': 679, 'jack': 1241, 'weller': 2758, 'understand': 2638, 'using': 2668, 'allocation': 54, 'buy': 292, 'lot': 1415, 'money': 1540, 'pam': 1686, 'raw': 1910, 'unprocessed': 2649, 'kgb': 1280, 'files': 849, 'comparison': 459, 'shop': 2151, 'thief': 2518, 'mole': 1535, 'vetted': 2683, 'source': 2272, 'marty': 1462, 'does': 671, 'list': 1380, 'suspects': 2436, 'bargain': 144, 'times': 2544, 'price': 1829, 'mandarin': 1447, 'quality': 1876, 'issue': 1236, 'yes': 2837, 'total': 2562, 'agreement': 40, 'theyre': 2516, 'fakes': 810, 'expensive': 787, 'furious': 956, 'impatient': 1200, 'gentlemen': 980, 'ive': 1240, 'site': 2202, 'play': 1784, 'honestly': 1151, 'talk': 2466, 'mandarians': 1446, 'convinced': 500, 'opportunity': 1656, 'wash': 2736, 'game': 963, 'puts': 1874, 'nodding': 1606, 'croninradio': 544, 'repeat': 1957, 'passed': 1709, 'muster': 1567, 'elevator': 741, 'vicradio': 2688, 'waits': 2714, 'small': 2237, 'wiring': 2794, 'infrastructure': 1214, 'lit': 1384, 'glare': 1000, 'someones': 2264, 'maglight': 1436, 'gloved': 1005, 'racks': 1892, 'electrical': 739, 'risers': 1997, 'carefully': 329, 'explosive': 790, 'device': 625, 'pack': 1675, 'cigarettes': 397, 'riser': 1996, 'second': 2081, 'ones': 1646, 'special': 2282, 'taken': 2463, 'plastic': 1781, 'mounted': 1551, 'floor': 897, 'subpanel': 2406, 'hold': 1143, 'piece': 1759, 'tape': 2469, 'transferring': 2583, 'pressing': 1827, 'button': 290, 'close': 425, 'rises': 1998, 'bracing': 246, 'door': 681, 'ivan': 1238, 'russian': 2027, 'outside': 1665, 'darkened': 578, 'hallway': 1070, 'holding': 1144, 'flips': 893, 'million': 1512, 'dollars': 675, 'suite': 2416, 'offices': 1634, 'clean': 403, 'caspiexpetroleum': 340, 'cherbourg': 379, 'moscow': 1546, 'rome': 2009, 'tehran': 2490, 'curtains': 562, 'lights': 1368, 'sitting': 2204, 'counting': 517, 'poring': 1806, 'document': 667, 'dozens': 691, 'sheets': 2134, 'financial': 856, 'data': 582, 'incomprehensibly': 1208, 'cyrillic': 568, 'marked': 1456, 'judging': 1265, 'seals': 2074, 'clearance': 406, 'sign': 2178, 'offs': 1637, 'topsecret': 2557, 'tinny': 2546, 'pop': 1805, 'tune': 2620, 'started': 2337, 'playing': 1786, 'hall': 1069, 'said': 2033, 'doublecrossed': 687, 'ankle': 67, 'shut': 2170, 'freaked': 938, 'feet': 835, 'pushing': 1873, 'rushes': 2025, 'past': 1716, 'sample': 2036, 'spilling': 2290, 'snapph': 2248, 'suppressed': 2427, 'caliber': 305, 'shots': 2155, 'falls': 815, 'crashing': 532, 'bullets': 276, 'tear': 2479, 'hit': 1139, 'unscrewing': 2652, 'silencer': 2184, 'tucking': 2619, 'weapon': 2748, 'whats': 2770, 'climbing': 419, 'duffel': 719, 'stuffing': 2398, 'ivans': 1239, 'file': 847, 'backpack': 122, 'remove': 1952, 'single': 2193, 'sheet': 2133, 'paper': 1694, 'exactly': 778, 'stuff': 2397, 'tucked': 2618, 'page': 1682, 'blur': 216, 'slipping': 2229, 'underneath': 2637, 'tossing': 2561, 'fell': 836, 'struggle': 2392, 'detonation': 624, 'decives': 599, 'blows': 213, 'tidy': 2538, 'selfcontained': 2091, 'explosion': 789, 'flicker': 886, 'fail': 805, 'cast': 341, 'darkness': 579, 'sudden': 2411, 'urgent': 2662, 'power': 1818, 'went': 2759, 'whiff': 2774, 'dread': 694, 'location': 1392, 'voices': 2703, 'piling': 1762, 'confusion': 478, 'cascading': 336, 'ab': 1, 'drone': 708, 'barn': 146, 'stepping': 2357, 'carrying': 333, 'gretkov': 1033, 'professional': 1842, 'trim': 2601, 'polished': 1802, 'dominant': 677, 'complaining': 460, 'bring': 264, 'tosses': 2560, 'photocopy': 1752, 'doing': 673, 'stripping': 2385, 'shower': 2164, 'long': 1403, 'plane': 1779, 'dumping': 721, 'sheds': 2132, 'clothes': 431, 'workmen': 2810, 'cluster': 435, 'cable': 300, 'winches': 2786, 'raised': 1900, 'pours': 1816, 'crime': 539, 'police': 1801, 'workers': 2808, 'media': 1479, 'vans': 2673, 'clogging': 423, 'mood': 1543, 'black': 190, 'ashes': 96, 'need': 1582, 'working': 2809, 'stands': 2330, 'silent': 2186, 'staring': 2334, 'disaster': 649, 'heartbroken': 1112, 'footlocker': 917, 'stash': 2340, 'setting': 2108, 'aside': 97, 'work': 2806, 'things': 2520, 'needs': 1586, 'separate': 2103, 'pile': 1761, 'phony': 1750, 'student': 2394, 'ids': 1188, 'loose': 1411, 'photos': 1754, 'hairdos': 1066, 'vacuumpacked': 2671, 'bags': 129, 'shoes': 2147, 'gasolinestoked': 967, 'burning': 283, 'rocklined': 2006, 'pit': 1770, 'feeding': 832, 'papers': 1695, 'belongings': 174, 'crinkles': 540, 'reveal': 1973, 'photo': 1751, 'burn': 282, 'gassoaked': 969, 'holds': 1145, 'flames': 876, 'rules': 2018, 'say': 2045, 'drop': 709, 'sticks': 2360, 'hefting': 1115, 'strides': 2382, 'covered': 524, 'xeroxed': 2829, 'paperwork': 1696, 'showandtell': 2162, 'charges': 368, 'placed': 1773, 'lines': 1376, 'failed': 806, 'fingerprint': 861, 'didnt': 633, ...} 123# Mapping: 단어 &lt;-&gt; 벡터안의 index no. invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()} # value : keyprint(str(invert_index_vectorizer)[:100]+'...') {1898: 'raining', 1366: 'light', 2387: 'strobes', 2763: 'wet', 1001: 'glass', 1978: 'rhythmic', 1673... (3) 중요 단어 추출 - Top 3 TF-IDF 먼저 TF-IDF Matrix 첫번째 행 (첫 씬)의 Top 3 단어의 index을 출력해보겠습니다. 1np.argsort(tf_idf_vect[0].toarray())[0][-3:] array([1984, 2387, 1978], dtype=int64) 즉 단어 벡터중의 1984번(0.2902), 2387번(0.3109), 1978번째(0.3109) 단어가 첫 씬에서 제일 중요한 단어로 뽑혔습니다. 이제 전체 TF-IDF matrix 에 적용해볼게요. 1np.argsort(tf_idf_vect.toarray())[:, -3:] array([[1984, 2387, 1978], [1297, 1971, 1097], [1693, 2221, 968], [ 690, 299, 1482], [2823, 1951, 1454], [2218, 2815, 1454], [2038, 737, 2418], [ 852, 2761, 2570], [2022, 156, 1352], [2250, 2241, 1454], [ 342, 321, 2188], [ 614, 1557, 1534], [ 535, 1884, 1614], [2188, 139, 20], [ 503, 730, 1458], [2790, 2384, 724], [ 169, 915, 2444], [1905, 1259, 53], [2566, 1335, 828], [2300, 281, 1702], [2503, 1502, 2567], [ 794, 1454, 1018], [ 698, 2559, 1252], [1871, 237, 1454], [ 204, 911, 2591], [ 237, 596, 1454], [ 52, 941, 1036], [ 211, 1156, 206], [1193, 2712, 1454], [ 52, 1809, 2462], [ 237, 1454, 702], [2130, 237, 1454], [1995, 1890, 321], [1011, 259, 1454], [1985, 2216, 1819], [ 420, 2276, 1454], [2019, 1103, 2059], [2177, 353, 1252], [1642, 1291, 2797], [1454, 1012, 2439], [2000, 2429, 2051], [2111, 1559, 2661], [1737, 1291, 2429], [1116, 2079, 46], [2634, 697, 392], [1723, 1015, 1071], [1443, 2700, 2486], [ 332, 2354, 786], [1165, 543, 975], [1169, 2434, 1986], [1509, 2486, 2335], [1001, 2686, 1509], [1289, 854, 219], [2758, 1687, 1460], [ 544, 1031, 854], [2686, 741, 726], [ 790, 625, 2794], [ 955, 367, 2583], [ 337, 1238, 2686], [ 340, 1634, 2490], [1238, 2248, 2686], [ 624, 789, 2538], [ 579, 341, 805], [ 543, 270, 577], [ 68, 146, 708], [1291, 1100, 1069], [ 162, 1291, 1033], [ 300, 2786, 1900], [ 543, 1687, 2380], [ 431, 782, 917], [2006, 128, 1751], [1687, 543, 2485], [ 552, 1866, 235], [2695, 762, 1102], [ 265, 2641, 56], [ 612, 1691, 441], [1654, 692, 1363], [2136, 243, 1688], [ 7, 2848, 6], [2596, 1687, 4], [2635, 1483, 841], [ 653, 2650, 1882], [1687, 481, 4], [1715, 1198, 1633], [2219, 2482, 169], [1541, 2423, 538], [ 322, 1633, 1198], [1460, 4, 414], [1307, 270, 1593], [1407, 1687, 543], [ 237, 491, 1593], [ 792, 348, 2665], [1055, 1747, 1593], [2010, 1868, 1403], [ 80, 2415, 2611], [2361, 1747, 1593], [1687, 1594, 1688], [1310, 1593, 143], [2825, 1688, 1594], [1460, 4, 1687], [ 4, 508, 2848], [1745, 983, 1612], [ 180, 491, 1690], [1496, 132, 1600], [1687, 1105, 1600], [2849, 2211, 2378], [ 173, 1687, 1600], [1757, 2379, 652], [2368, 2086, 1609], [ 4, 1312, 1572], [1725, 1863, 1418], [1873, 1274, 270], [1291, 2718, 683], [ 108, 1033, 2421], [1104, 650, 382], [ 491, 237, 1248], [2768, 1561, 1247], [ 939, 237, 1248], [1561, 2008, 1721], [ 883, 623, 1248], [ 944, 1561, 1246], [ 855, 1248, 1055], [ 458, 1914, 1250], [ 930, 966, 2550], [2380, 2516, 2625], [ 670, 228, 81], [2660, 1780, 902], [2371, 2553, 2515], [1716, 706, 93], [ 926, 780, 2848], [1585, 659, 1118], [1967, 1180, 208], [1992, 268, 2741], [ 237, 1521, 2377], [2178, 2003, 1296], [ 608, 2504, 970], [2849, 237, 706], [1655, 1049, 2700], [1160, 1322, 2197], [2375, 2223, 1776], [2475, 725, 2381], [ 703, 258, 481], [2646, 2229, 467], [ 237, 1059, 2067], [1828, 1669, 2112], [ 741, 1390, 783], [1562, 2407, 543], [ 295, 1269, 1906], [2437, 771, 703], [1291, 2245, 260], [2368, 2008, 2437], [ 891, 1400, 1960], [ 62, 2185, 1676], [1465, 919, 241], [2658, 1358, 1582], [1091, 595, 1002], [ 738, 1424, 1600], [2726, 1600, 1687], [2451, 2787, 273], [ 4, 680, 1687], [ 755, 401, 155], [1593, 4, 1413], [ 607, 1600, 1817], [2329, 1144, 254], [ 80, 1601, 2582], [ 606, 1600, 2582], [1434, 630, 1381], [2551, 1350, 173], [ 727, 2137, 2702], [1931, 1600, 1307], [1692, 372, 1893], [1830, 237, 1600], [ 504, 2344, 606], [2408, 819, 763], [2623, 1342, 1228], [1033, 1319, 890], [ 521, 481, 293], [ 10, 826, 2586], [ 167, 237, 1600], [2014, 1687, 2137], [1700, 218, 2120], [1225, 2750, 942], [ 442, 2848, 4], [ 981, 258, 1918], [2323, 2355, 2848], [ 221, 4, 2848], [ 1, 0, 1160], [ 8, 2014, 411], [1984, 694, 1552], [1775, 2133, 829], [1441, 2053, 2205], [ 55, 275, 267], [1647, 1306, 2014], [2415, 2416, 878], [2780, 650, 1171], [2329, 673, 256], [ 833, 1111, 1813], [ 366, 543, 2485], [1896, 2495, 1196], [2604, 651, 1389], [1344, 1272, 882], [1558, 1517, 1590], [1697, 1661, 2192], [1444, 319, 2507], [2615, 2318, 255], [2746, 2477, 2441], [1730, 2014, 1988], [ 124, 133, 2435], [1439, 1712, 1700], [1282, 1137, 896], [2788, 1548, 152], [1659, 1340, 2015], [1486, 2441, 2011], [1555, 2767, 349], [ 204, 762, 784], [1894, 407, 2014], [ 796, 88, 2077], [2728, 2323, 2659], [1291, 2011, 1110], [ 577, 374, 2485], [2300, 1830, 11], [1160, 2485, 4], [1147, 2777, 954], [2284, 973, 823], [ 116, 1548, 2480], [2235, 334, 1721], [2207, 1801, 2000], [ 684, 1895, 2582], [2149, 1321, 1354], [1056, 580, 45], [2424, 1346, 145], [2159, 145, 2582], [ 752, 498, 2246], [2014, 1687, 543], [1687, 1590, 543], [1193, 1033, 4], [1333, 2234, 2082], [2230, 1291, 1033], [ 238, 870, 4], [ 543, 1687, 2485], [1372, 493, 91], [1396, 1970, 2341], [1301, 318, 2236], [1506, 791, 2577], [1687, 2460, 2485], [ 615, 230, 1800], [2537, 474, 2696], [1490, 1312, 543], [ 357, 237, 1454], [ 187, 2244, 2597], [ 543, 1687, 4], [ 741, 1999, 1201], [1685, 1747, 1033], [2588, 543, 1747], [1305, 1033, 4], [1938, 925, 924], [1305, 543, 2485], [1170, 1687, 4], [ 653, 728, 1782], [1266, 90, 817], [2626, 703, 2475], [1416, 1279, 2305], [2711, 703, 2475], [ 113, 1177, 1769], [1225, 2166, 820], [1272, 881, 297], [2638, 880, 674], [2129, 960, 1500], [ 351, 903, 774], [1731, 2240, 2446], [ 642, 2137, 2475], [2076, 1371, 369], [ 72, 703, 2475], [1915, 2070, 2475], [1546, 334, 2480], [ 707, 1913, 1989], [2215, 872, 2669], [1999, 488, 1182], [1291, 508, 2208], [2367, 1361, 2286], [ 918, 1670, 179], [2215, 1713, 1670], [1649, 2831, 1142], [1768, 1268, 1361], [ 434, 137, 2535], [2081, 181, 2140], [1867, 664, 1219], [1344, 1056, 1546], [1556, 197, 1123], [1077, 1546, 709], [2086, 2390, 89], [1175, 160, 1176], [1057, 1315, 1088], [1110, 1260, 454], [1546, 2022, 1457], [1127, 1045, 2085], [ 630, 2304, 533], [1019, 280, 1453], [1291, 2304, 2327], [2469, 716, 233], [ 549, 884, 844], [2701, 493, 2449], [ 237, 295, 296], [ 754, 327, 1729], [1683, 702, 466], [1061, 1316, 714], [ 334, 1987, 1801], [2622, 1291, 295], [ 237, 1407, 51], [ 654, 1636, 2453], [1155, 2569, 1737], [1037, 1494, 2488], [ 419, 887, 1271], [1277, 676, 661], [ 555, 237, 1232], [ 445, 177, 1560], [ 237, 51, 1687], [1600, 1621, 2102], [ 162, 1566, 1864], [ 46, 803, 646]], dtype=int64) 이를 변수로 저장해서 원본 데이터셋에 추가하면 다음와 같습니다. 123top_3_words = np.argsort(tf_idf_vect.toarray())[:, -3:]df['important_word_index'] = pd.Series(top_3_words.tolist())df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } page_no scene_title text processed_text important_word_index 0 1 1 EXT. MERCEDES WINDSHIELD -- DUSK 1 It's raining... ... its raining light strobes across the wet glas... [1984, 2387, 1978] 1 1 A1 INT. MERCEDES -- NIGHT A1 On his knee -- a syringe an... a on his knee a syringe and a gun the eyes of... [1297, 1971, 1097] 2 1 2 INT. COTTAGE BEDROOM -- NIGHT 2 BOURNE'S EYES OPEN! -- panic... bournes eyes open panicked gasping trying to ... [1693, 2221, 968] 3 1 A2 INT. COTTAGE LIVING AREA/BATHROOM ... A2 BOURNE moving for the medic... a bourne moving for the medicine cabinet digs... [690, 299, 1482] 4 2 3 INT./EXT. COTTAGE LIVING ROOM/VERA... 3 One minute later. BOURNE mo... one minute later bourne moves out onto the ve... [2823, 1951, 1454] 하지만 지금 중요한 단어의 index만 표시 되고, 과연 어떤 단어인지를 모릅니다. 그래서 우리는 방금 추출한 “벡터”-“단어” Mapping 결과를 이용해 index에 해당하는 단어들을 추출하여 데이터셋에 저장하겠습니다. 1234567# index -&gt; word 변환함수 만들기def convert_to_word(x): word_list = [] for index in x: word_list.append(invert_index_vectorizer[index]) return word_list 12df['important_words'] = df['important_word_index'].apply(lambda x: convert_to_word(x))df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } page_no scene_title text processed_text important_word_index important_words 0 1 1 EXT. MERCEDES WINDSHIELD -- DUSK 1 It's raining... ... its raining light strobes across the wet glas... [1984, 2387, 1978] [riding, strobes, rhythmic] 1 1 A1 INT. MERCEDES -- NIGHT A1 On his knee -- a syringe an... a on his knee a syringe and a gun the eyes of... [1297, 1971, 1097] [knee, returns, head] 2 1 2 INT. COTTAGE BEDROOM -- NIGHT 2 BOURNE'S EYES OPEN! -- panic... bournes eyes open panicked gasping trying to ... [1693, 2221, 968] [panicked, sleeps, gasping] 3 1 A2 INT. COTTAGE LIVING AREA/BATHROOM ... A2 BOURNE moving for the medic... a bourne moving for the medicine cabinet digs... [690, 299, 1482] [downs, cabinet, medicine] 4 2 3 INT./EXT. COTTAGE LIVING ROOM/VERA... 3 One minute later. BOURNE mo... one minute later bourne moves out onto the ve... [2823, 1951, 1454] [write, remember, marie] 이제 장면별 중요한 단어 Top 3가 모두 출력됐습니다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【EXERCISE】","slug":"【EXERCISE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/"},{"name":"Python","slug":"【EXERCISE】/Python","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Text Mining","slug":"Text-Mining","permalink":"https://hyemin-kim.github.io/tags/Text-Mining/"}]},{"title":"Python >> 텍스트 마이닝 (Text Mining) 소개","slug":"S-Python-TextMining1","date":"2020-08-19T11:10:26.000Z","updated":"2020-11-25T12:50:36.918Z","comments":true,"path":"2020/08/19/S-Python-TextMining1/","link":"","permalink":"https://hyemin-kim.github.io/2020/08/19/S-Python-TextMining1/","excerpt":"","text":"텍스트 마이닝 (Text Mining) 소개 1. 텍스트 마이닝의 개념 2. 텍스트 마이닝 응용분야 3. 텍스트 데이터의 처리 방법 3-1. BoW (Bag of Words) 3-2. 문서 단어 행렬 (Document-Term Matrix, DTM) 3-3. 단어의 중요도를 계산하는 방법 (TF-IDF) 1. 텍스트 마이닝의 개념 텍스트 마이닝은 비정형 및 반정형 텍스트 데이터에 대하여 자연어 처리(Natural Langrage Precessing) 기술과 문서 처리 기술을 적용하여 가치와 의미가 있는 정보를 찾아내는(Mining) 기술입니다. 2. 텍스트 마이닝 응용분야 단어의 빈도수 기반 Word Cloud: 텍스트 데이터에서의 단어 등장 빈도수 시각화 문서 분류: 감성 분류 Topic Modeling: 텍스트 데이터를 분석하여 여러 Topic으로 Clustering 하는 작업 단어의 의미 기반 Semantic Analysis: 사람처럼 자연어를 이해하기 3. 텍스트 데이터의 처리 방법 3-1. BoW (Bag of Words) 단어 가방(Bag of Words) 모델은 문장의 문법 및 단어 순서를 무시하고 텍스트 문서를 \"단어\"로 변환한 후 다양한 측정 값을 계산할 수 있도록 \"가방\"형식으로 저장해놓는 겁니다. 단어 가방 모델에서 계산 된 가장 일반적인 유형의 특성 또는 기능은 용어 빈도, 즉 용어가 텍스트에 나타나는 횟수입니다. 《기생충》중의 한 대사로 예를 들어 볼게요. 이 문장에서 “그”, “을”, \"듯\"과 같이 실질적인 의미가 없는 \"불용어\"를 제외하고 의미 있는 “형태소” 단어와 해당 형태소의 등장 횟수을 추출합니다. 그럼 다음과 같은 표로 요약할 수 있겠습니다. 이것이 바로 \"Bag of Words 모델\"입니다. 3-2. 문서 단어 행렬 (Document-Term Matrix, DTM) 위에 설명드린 Bag of Words는 한 문장에 대해 적용하는 것이고, 문장이 여러 개가 있을 때는 (DataFrame 형태) 문서 단어 행렬 (Document-Term Matrix) 로 표현됩니다. 똑같이 《기생충》중의 대사들로 예를 들어 볼게요. 이처럼 여러 문장의 경우에 \"문서 단어 행렬\"은 위와 같이 표현 됩니다. 3-3. 단어의 중요도를 계산하는 방법 (TF-IDF) 문서 단어 행렬은 그저 단어의 등장 횟수를 단순히 세는 겁니다. 각 문장에서 어떤 단어가 중요한지 알 수 없습니다. 이를 알아내기 위해 우리는 “TF-IDF (Term Frequency-Inverse Document Frequency)” 라는 지표를 사용합니다. TF (Term Frequency): 특정 문서에서 특정 단어의 등장 횟수 DF (Document Frequency): 특정 단어가 등장한 문서의 수 IDF (Inverse Document Frequency): DF와 반비례 값을 가지는 수식 IDF(d,t)=ln⁡(n1+DF(t))IDF(d,t) = \\ln ( \\frac{n}{1+DF(t)} )IDF(d,t)=ln(1+DF(t)n​) TF-IDF (Term Frequency-Inverse Document Frequency): TF와 IDF를 곱한 값 TF-IDF로 특정 문서 안의 특정 단어의 중요도를 나타나는 원리는: 특정 문서에서는 많이 등장했으면서 다른 문서에서 잘 등장하지 않은 단어가 결국 이 문서에서 가장 중요한 단어가 될것이다라는 가설입니다. [예] 문서1에서 \"아들\"과 “계획” 이 두 단어의 TF-IDF를 한번 계산해봅시다. Step 1. TF A: 1 B: 1 Step 2. DF A: 1 B: 3 Step 3. IDF A: ln⁡(41+1)=ln⁡2≈0.6931\\ln( \\frac{4}{1+1} ) = \\ln 2 \\approx 0.6931ln(1+14​)=ln2≈0.6931 B: ln⁡(41+3)=0\\ln( \\frac{4}{1+3} ) = 0ln(1+34​)=0 Step 4. TF-IDF A: 1 * 0.6931 = 0.6931 B: 1 * 0 = 0 혜석: \"계획\"이라는 단어가 《기생충》의 문장들에서 너무 많이 등장해서 문서1에서 특별히 중요한 단어라고 볼 수 없다. 하지만 \"아들\"이라는 단어가 다른 문장에서 한번도 나타나지 않았기 때문에 문장1에서는 매우 중요하다고 볼 수 있다 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - Text Mining","slug":"【STUDY-Python】/Python-Text-Mining","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-Text-Mining/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Text Mining","slug":"Text-Mining","permalink":"https://hyemin-kim.github.io/tags/Text-Mining/"}]},{"title":"【실습】 Python >> Classification -- 포켓몬 분류 분석","slug":"E-Python-Classification-1","date":"2020-08-13T14:00:08.000Z","updated":"2020-10-28T06:49:38.772Z","comments":true,"path":"2020/08/13/E-Python-Classification-1/","link":"","permalink":"https://hyemin-kim.github.io/2020/08/13/E-Python-Classification-1/","excerpt":"","text":"【Classification 실습】 – 포켓몬 데이터셋 0. 개요 1. Library &amp; Data Import 2. EDA (Exploratoty Data Analysis: 탐색적 데이터 분석) 2-1. 데이터셋 기본 정보 탐색 2-2. 변수(Feature) 특징 탐색 (1) 각 능력치 분포 (2) 총 능력치 (Total) 분포 (3) 포켓몬 타입 (Type 1 &amp; Type 2) 분포 (4) 포켓몬 세대(Generation) 분포 3. 지도 학습 기반 분류 분석 – Logistic Regression 3-1. 데이터 전처리 (1) 데이터 타입 변경 (2) One-Hot Encoding (3) Feature 포준화 (4) Training set / Test set 나누기 3-2. Logistic Regression 모델 학습 (1) 모델 학습 (2) 모델 평가 3-3. 클래스 불균형 조정 4. 비지도 학습 기반 군집 분석 – K-Means Clustering 4-1. K-Means 군집 분석 (1) 2차원 군집 분석 (2) 다차원 군집 분석 0. 개요 포켓몬 데이터셋을 이용해 분류 분석을 진행합니다. 지도 학습 (Logistic Regression): “전설의 포켓몬” 여부 예측 – “Legendary” = 0/1 비지도 학습 (K-Means Clustering): 포켓몬 군집 분석 1. Library &amp; Data Import &gt;&gt; 사용할 Library 123456789%matplotlib inlineimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.filterwarnings(\"ignore\") 1plt.rcParams['font.family'] = 'Malgun Gothic' # 한글 깨짐 방지 1plt.rcParams['figure.figsize'] = (10, 8) # figsize 설정 &gt;&gt; 사용할 데이터셋 - Pokemon Dataset 1df = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/Pokemon.csv\") 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # Name Type 1 Type 2 Total HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary 0 1 Bulbasaur Grass Poison 318 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 405 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 525 80 82 83 100 100 80 1 False 3 3 VenusaurMega Venusaur Grass Poison 625 80 100 123 122 120 80 1 False 4 4 Charmander Fire NaN 309 39 52 43 60 50 65 1 False &gt;&gt; Feature Description Name: 포켓몬 이름 Type 1: 포켓몬 타입 1 Type 2: 포켓몬 타입 2 Total: 포켓몬 총 능력치 (Sum of ‘HP’, ‘Attack’, ‘Defense’, ‘Sp.Atk’, ‘Sp.Def’ and ‘Speed’) HP: 포켓몬 HP 능력치 Attack: 포켓몬 Attack 능력치 Defense: 포켓몬 Defense 능력치 Sp.Atk: 포켓몬 Sp.Atk 능력치 Sp.Def: 포켓몬 Sp.Def 능력치 Speed: 포켓몬 Speed 능력치 Generation: 포켓몬 세대 Legendary: 전설의 포켓몬 여부 2. EDA (Exploratoty Data Analysis: 탐색적 데이터 분석) 12# 그래프 배경 설정sns.set_style('darkgrid') 2-1. 데이터셋 기본 정보 탐색 &gt;&gt; 전체 데이터셋 12# demensiondf.shape (800, 13) 12# information (data type)df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 800 entries, 0 to 799 Data columns (total 13 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 # 800 non-null int64 1 Name 800 non-null object 2 Type 1 800 non-null object 3 Type 2 414 non-null object 4 Total 800 non-null int64 5 HP 800 non-null int64 6 Attack 800 non-null int64 7 Defense 800 non-null int64 8 Sp. Atk 800 non-null int64 9 Sp. Def 800 non-null int64 10 Speed 800 non-null int64 11 Generation 800 non-null int64 12 Legendary 800 non-null bool dtypes: bool(1), int64(9), object(3) memory usage: 75.9+ KB 12# 결측치df.isnull().sum() # 0 Name 0 Type 1 0 Type 2 386 Total 0 HP 0 Attack 0 Defense 0 Sp. Atk 0 Sp. Def 0 Speed 0 Generation 0 Legendary 0 dtype: int64 &gt;&gt; 개별 Feature 탐색 분류할 목표 Feature: “Legendary” (전설의 포켓몬 여부) 12# class 별 데이터 수 확인df['Legendary'].value_counts() False 735 True 65 Name: Legendary, dtype: int64 “Generation” (포켓몬 세대) 12# 세대별 데이터 수 확인df['Generation'].value_counts() 1 166 5 165 3 160 4 121 2 106 6 82 Name: Generation, dtype: int64 12345# 세대 순서로 데이터 갯수 시각화fig = plt.figure(figsize=(8, 6))df['Generation'].value_counts().sort_index().plot()plt.title(\"'Generation 별 데이터 갯수'\")plt.show() “Type 1” &amp; “Type 2” (포켓몬 타입) 12# unique value of \"Type 1\"df['Type 1'].unique() array(['Grass', 'Fire', 'Water', 'Bug', 'Normal', 'Poison', 'Electric', 'Ground', 'Fairy', 'Fighting', 'Psychic', 'Rock', 'Ghost', 'Ice', 'Dragon', 'Dark', 'Steel', 'Flying'], dtype=object) 12# No. of unique value for \"Type 1\"len(df['Type 1'].unique()) 18 12# unique value of \"Type 2\" (exclude \"NaN\")df[df['Type 2'].notnull()]['Type 2'].unique() array(['Poison', 'Flying', 'Dragon', 'Ground', 'Fairy', 'Grass', 'Fighting', 'Psychic', 'Steel', 'Ice', 'Rock', 'Dark', 'Water', 'Electric', 'Fire', 'Ghost', 'Bug', 'Normal'], dtype=object) 12# No. of unique value for \"Type 2\"len(df[df['Type 2'].notnull()]['Type 2'].unique()) 18 2-2. 변수(Feature) 특징 탐색 각 변수(Feature)의 분포를 관찰함으로써, 변수들의 특징을 알아보도록 하겠습니다. 특히, 저희가 분류할 목표 Feature가 \"Legendary\"이므로, 각 변수의 분포를 탐색 시: 각 항목(feature)에서 전체 데이터의 분포 뿐만 아닌 \"Legendary\"변수 class 별의 데이터 분포도 함께 살펴볼게요. GUIDE 【전체 데이터 탐색】 &amp; 【“Legendary” class별 탐색】 각 능력치 분포 총 능력치 (Toal) 분포 포켓몬 타입 (Type 1 &amp; Type 2) 분포 포켓몬 세대 (Generation) 분포 (1) 각 능력치 분포 123456# 전체 포켓몬의 각 능력치 분포stats = ['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'] # 능력치 변수 집합sns.boxplot(data = df[stats])plt.title('각 능력치 분포')plt.show() 12345678910111213# \"전설의 포켓몬\" 여부에 따른 능력치 분포fig = plt.figure(figsize=(16, 8))plt.subplot(1,2,1)sns.boxplot(data = df[df['Legendary']==1][stats])plt.title('Legendary = True')plt.subplot(1,2,2)sns.boxplot(data = df[df['Legendary']==0][stats])plt.title('Legendary = False')plt.show() \"전설의 포켓몬\"은 전체적으로 더 높은 능력치를 보유하고 있으며, Attack와 Sp.Atk가 특히 높은 것으로 보입니다. (2) 총 능력치 (Total) 분포 12345# 전체 포켓몬의 총 능력치 분포df['Total'].hist(bins=50)plt.title('총 능력치 (Total) 분포')plt.show() 1234# 세대별 총 능력치 분포sns.boxplot(data = df, x = \"Generation\", y=\"Total\")plt.title(\"세대별 총 능력치 분포\")plt.show() 1234# 각 세대 \"전설의 포켓몬\" 여부에 따른 총 능력치 분포sns.boxplot(data = df, x = \"Generation\", y=\"Total\", hue=\"Legendary\")plt.title(\"각 세대 '전설의 포켓몬' 여부에 따른 총 능력치 분포\")plt.show() 1234# 타입(Type 1)별 총 능력치 분포sns.boxplot(data = df, x = 'Type 1', y = 'Total')plt.title(\"타입(Type 1)별 총 능력치 분포\")plt.show() (3) 포켓몬 타입 (Type 1 &amp; Type 2) 분포 123# 전체 포켓몬 -- Type 1 분포df['Type 1'].value_counts(sort=False).sort_index().plot.barh() &lt;matplotlib.axes._subplots.AxesSubplot at 0x2e92fab51c8&gt; 12345678# \"전설의 포켓몬\" 여부에 따른 Type 1 분포T1_Total = pd.DataFrame(df['Type 1'].value_counts().sort_index())T1_NotLeg = pd.DataFrame(df[df['Legendary']==0].groupby('Type 1').size())T1_count = pd.concat([T1_Total, T1_NotLeg], axis = 1)T1_count.columns = ['Total', 'Not Legend']T1_count['Legend'] = T1_count['Total'] - T1_count['Not Legend']T1_count .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total Not Legend Legend Type 1 Bug 69 69 0 Dark 31 29 2 Dragon 32 20 12 Electric 44 40 4 Fairy 17 16 1 Fighting 27 27 0 Fire 52 47 5 Flying 4 2 2 Ghost 32 30 2 Grass 70 67 3 Ground 32 28 4 Ice 24 22 2 Normal 98 96 2 Poison 28 28 0 Psychic 57 43 14 Rock 44 40 4 Steel 27 23 4 Water 112 108 4 1T1_count[['Not Legend', 'Legend']].plot.barh(width=0.7) &lt;matplotlib.axes._subplots.AxesSubplot at 0x2e92ff18b88&gt; 123456# 전체 포켓몬 -- Type 2 분포df['Type 2'].value_counts(sort=False).sort_index().plot.barh()plt.title('\"전설의 포켓몬\" 여부에 따른 Type 1 분포')plt.show() 12345678# \"전설의 포켓몬\" 여부에 따른 Type 2 분포T2_Total = pd.DataFrame(df['Type 2'].value_counts().sort_index())T2_NotLeg = pd.DataFrame(df[df['Legendary']==0].groupby('Type 2').size())T2_count = pd.concat([T2_Total, T2_NotLeg], axis = 1)T2_count.columns = ['Total', 'Not Legend']T2_count['Legend'] = T2_count['Total'] - T2_count['Not Legend']T2_count .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total Not Legend Legend Type 2 Bug 3 3 0 Dark 20 19 1 Dragon 18 14 4 Electric 6 5 1 Fairy 23 21 2 Fighting 26 22 4 Fire 12 9 3 Flying 97 84 13 Ghost 14 13 1 Grass 25 25 0 Ground 35 34 1 Ice 14 11 3 Normal 4 4 0 Poison 34 34 0 Psychic 33 28 5 Rock 14 14 0 Steel 22 21 1 Water 14 13 1 1234T2_count[['Not Legend', 'Legend']].plot.barh(width=0.7)plt.title('\"전설의 포켓몬\" 여부에 따른 Type 2 분포')plt.show() (4) 포켓몬 세대(Generation) 분포 123# 전체 포켓몬 -- 세대 분포df['Generation'].value_counts().sort_index().plot.bar() &lt;matplotlib.axes._subplots.AxesSubplot at 0x2e930887d08&gt; 1234567# \"전설의 포켓몬\" 여부에 따른 세대 분포gene_Leg = pd.DataFrame(df[df['Legendary']==1].groupby('Generation').size())gene_NotLeg = pd.DataFrame(df[df['Legendary']==0].groupby('Generation').size())gene_count = pd.concat([gene_Leg, gene_NotLeg], axis=1)gene_count.columns = ['Legend', 'Not Legend']gene_count .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Legend Not Legend Generation 1 6 160 2 5 101 3 18 142 4 13 108 5 15 150 6 8 74 123gene_count.plot.bar()plt.title('\"전설의 포켓몬\" 여부에 따른 세대 분포')plt.show() 3. 지도 학습 기반 분류 분석 – Logistic Regression 3-1. 데이터 전처리 (1) 데이터 타입 변경 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # Name Type 1 Type 2 Total HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary 0 1 Bulbasaur Grass Poison 318 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 405 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 525 80 82 83 100 100 80 1 False 3 3 VenusaurMega Venusaur Grass Poison 625 80 100 123 122 120 80 1 False 4 4 Charmander Fire NaN 309 39 52 43 60 50 65 1 False 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 800 entries, 0 to 799 Data columns (total 13 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 # 800 non-null int64 1 Name 800 non-null object 2 Type 1 800 non-null object 3 Type 2 414 non-null object 4 Total 800 non-null int64 5 HP 800 non-null int64 6 Attack 800 non-null int64 7 Defense 800 non-null int64 8 Sp. Atk 800 non-null int64 9 Sp. Def 800 non-null int64 10 Speed 800 non-null int64 11 Generation 800 non-null int64 12 Legendary 800 non-null bool dtypes: bool(1), int64(9), object(3) memory usage: 75.9+ KB 분류예측 목표 Feature인 \"Lengendary\"의 값은 현재 “True”/\"False\"로 구성되어있습니다. 예측 모델에 적용하기 위해 “1”/\"0\"으로 바꾸겠습니다. 포켓몬의 세대를 나타나는 Feature인 \"Generation\"의 타입은 지금 \"int\"로 되어있지만, Feature의 의미상 해당 숫자는 분류 역할을 하고 있으므로 \"str\"타입으로 변환시키겠습니다. 분류 예측 시 이름 데이터가 예측에 도움이 없으므로 \"Name\"을 빼고 데이터셋을 제구성하겠습니다. 12345df['Legendary'] = df['Legendary'].astype(int)df['Generation'] = df['Generation'].astype(str)preprocessed_df = df[['Type 1', 'Type 2', 'Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', 'Generation', 'Legendary']]preprocessed_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Type 1 Type 2 Total HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary 0 Grass Poison 318 45 49 49 65 65 45 1 0 1 Grass Poison 405 60 62 63 80 80 60 1 0 2 Grass Poison 525 80 82 83 100 100 80 1 0 3 Grass Poison 625 80 100 123 122 120 80 1 0 4 Fire NaN 309 39 52 43 60 50 65 1 0 (2) One-Hot Encoding Categorical Variable에 대해서 dummy화 작업을 진행하겠습니다. 1 data one-label: One-hot Encoding 1 data multi-label: Multi-label Encoding &gt;&gt; 타입 (Type) – Multi-label Encoding 먼저 Type 1과 Type 2를 하나의 변수(Type)로 묶는다. 그 다음 1~2개의 label를 가진 Type변수에 대해서 Multi-label Encoding을 진행한다. 12345678910# pokemon type list 생성def make_list(x1, x2): type_list = [] type_list.append(x1) if x2 is not np.nan: type_list.append(x2) return type_listpreprocessed_df['Type'] = preprocessed_df.apply(lambda x: make_list(x['Type 1'], x['Type 2']), axis=1)preprocessed_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Type 1 Type 2 Total HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary Type 0 Grass Poison 318 45 49 49 65 65 45 1 0 [Grass, Poison] 1 Grass Poison 405 60 62 63 80 80 60 1 0 [Grass, Poison] 2 Grass Poison 525 80 82 83 100 100 80 1 0 [Grass, Poison] 3 Grass Poison 625 80 100 123 122 120 80 1 0 [Grass, Poison] 4 Fire NaN 309 39 52 43 60 50 65 1 0 [Fire] 123del preprocessed_df['Type 1']del preprocessed_df['Type 2']preprocessed_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary Type 0 318 45 49 49 65 65 45 1 0 [Grass, Poison] 1 405 60 62 63 80 80 60 1 0 [Grass, Poison] 2 525 80 82 83 100 100 80 1 0 [Grass, Poison] 3 625 80 100 123 122 120 80 1 0 [Grass, Poison] 4 309 39 52 43 60 50 65 1 0 [Fire] 123456# multi-lacel encodingfrom sklearn.preprocessing import MultiLabelBinarizerml = MultiLabelBinarizer()preprocessed_df = preprocessed_df.join(pd.DataFrame(ml.fit_transform(preprocessed_df.pop('Type')), columns = ml.classes_)) [pandas.DataFrame.join]: Join columns of another DataFrame [pandas.DataFrame.pop (item) ]: Return item and drop from frame 1preprocessed_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary Bug ... Ghost Grass Ground Ice Normal Poison Psychic Rock Steel Water 0 318 45 49 49 65 65 45 1 0 0 ... 0 1 0 0 0 1 0 0 0 0 1 405 60 62 63 80 80 60 1 0 0 ... 0 1 0 0 0 1 0 0 0 0 2 525 80 82 83 100 100 80 1 0 0 ... 0 1 0 0 0 1 0 0 0 0 3 625 80 100 123 122 120 80 1 0 0 ... 0 1 0 0 0 1 0 0 0 0 4 309 39 52 43 60 50 65 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 5 rows × 27 columns &gt;&gt; 세대 (Generation) – One-hot Encoding 1234# apply one-hot encoding to 'Generation'preprocessed_df = pd.get_dummies(preprocessed_df) # df name입력하면 str var를 자동 식별하여 encoding 진행함# preprocessed_ddf = pd.get_dummies(preprocessed_df['Generation']) # 작업할 var 지정preprocessed_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total HP Attack Defense Sp. Atk Sp. Def Speed Legendary Bug Dark ... Psychic Rock Steel Water Generation_1 Generation_2 Generation_3 Generation_4 Generation_5 Generation_6 0 318 45 49 49 65 65 45 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 1 405 60 62 63 80 80 60 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 2 525 80 82 83 100 100 80 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 3 625 80 100 123 122 120 80 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 4 309 39 52 43 60 50 65 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 5 rows × 32 columns (3) Feature 포준화 Numerical Feature간의 Scale차이를 없애기 위해 feature 표준화를 진행합니다. 1preprocessed_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 800 entries, 0 to 799 Data columns (total 32 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Total 800 non-null int64 1 HP 800 non-null int64 2 Attack 800 non-null int64 3 Defense 800 non-null int64 4 Sp. Atk 800 non-null int64 5 Sp. Def 800 non-null int64 6 Speed 800 non-null int64 7 Legendary 800 non-null int32 8 Bug 800 non-null int32 9 Dark 800 non-null int32 10 Dragon 800 non-null int32 11 Electric 800 non-null int32 12 Fairy 800 non-null int32 13 Fighting 800 non-null int32 14 Fire 800 non-null int32 15 Flying 800 non-null int32 16 Ghost 800 non-null int32 17 Grass 800 non-null int32 18 Ground 800 non-null int32 19 Ice 800 non-null int32 20 Normal 800 non-null int32 21 Poison 800 non-null int32 22 Psychic 800 non-null int32 23 Rock 800 non-null int32 24 Steel 800 non-null int32 25 Water 800 non-null int32 26 Generation_1 800 non-null uint8 27 Generation_2 800 non-null uint8 28 Generation_3 800 non-null uint8 29 Generation_4 800 non-null uint8 30 Generation_5 800 non-null uint8 31 Generation_6 800 non-null uint8 dtypes: int32(19), int64(7), uint8(6) memory usage: 107.9 KB 1234567from sklearn.preprocessing import StandardScaler# feature standardizationscaler = StandardScaler()scale_columns = ['Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']preprocessed_df[scale_columns] = scaler.fit_transform(preprocessed_df[scale_columns])preprocessed_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total HP Attack Defense Sp. Atk Sp. Def Speed Legendary Bug Dark ... Psychic Rock Steel Water Generation_1 Generation_2 Generation_3 Generation_4 Generation_5 Generation_6 0 -0.976765 -0.950626 -0.924906 -0.797154 -0.239130 -0.248189 -0.801503 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 1 -0.251088 -0.362822 -0.524130 -0.347917 0.219560 0.291156 -0.285015 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 2 0.749845 0.420917 0.092448 0.293849 0.831146 1.010283 0.403635 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 3 1.583957 0.420917 0.647369 1.577381 1.503891 1.729409 0.403635 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 4 -1.051836 -1.185748 -0.832419 -0.989683 -0.392027 -0.787533 -0.112853 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 5 rows × 32 columns (4) Training set / Test set 나누기 12345from sklearn.model_selection import train_test_splitX = preprocessed_df.loc[:, preprocessed_df.columns != 'Legendary']y = preprocessed_df['Legendary']x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1) 1x_train.shape, y_train.shape ((600, 31), (600,)) 1x_test.shape, y_test.shape ((200, 31), (200,)) 3-2. Logistic Regression 모델 학습 (1) 모델 학습 123456789from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score# Fit in Training setlogit = LogisticRegression(random_state=0)logit.fit(x_train, y_train)# Predict in Test sety_pred = logit.predict(x_test) (2) 모델 평가 123456# classification result for test setprint(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred))print(\"Precision: %.3f\" % precision_score(y_test, y_pred))print(\"Recall: %.3f\" % recall_score(y_test, y_pred))print(\"F1: %.3f\" % f1_score(y_test, y_pred)) accuracy: 0.93 Precision: 0.615 Recall: 0.471 F1: 0.533 위 모델 평가 결과를 확인해보면, 해당 모델은 정확도(accuracy) 만 높고, 정밀도(Precision), 재현율(Recall), F1 score 등 모두 낮습니다. 이는 학습 데이터의 클래스 불균형으로 인한 정확도 함정 문제일 가능성이 높습니다. (참고: [Python &gt;&gt; sklearn - (2) 분류] 4-2. !!정확도 함정!!) 추가 확인을 위해 Confusion Matrix를 출력해 봅니다. 1np.set_printoptions(suppress=True) 123456789from sklearn.metrics import confusion_matrix# print confusion matrixconfu = confusion_matrix(y_true = y_test, y_pred = y_pred)plt.figure(figsize=(4, 3))sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')plt.title('Confusion Matrix')plt.show() Positive Condition ( “Legendary” = True/1 ) &lt;17&gt; 대비 Negative Condition ( “Legendary” = False/0 ) &lt;183&gt;인 케이스가 훨씬 많다는 것을 볼 수 있습니다. 따라서, 클래스 불균형으로 인한 정확도 함정 문제가 맞으며, 클래스 불균형을 조정한 후 다시 학습 시키도록 하겠습니다. 3-3. 클래스 불균형 조정 1preprocessed_df['Legendary'].value_counts() 0 735 1 65 Name: Legendary, dtype: int64 &gt;&gt; 1:1 샘플링 Positive Condition 케이스와 Negative Condition 케이스를 1:1비율로 샘플링 합니다. 12positive_random_idx = preprocessed_df[preprocessed_df['Legendary']==1].sample(65, random_state=12).index.tolist()negative_random_idx = preprocessed_df[preprocessed_df['Legendary']==0].sample(65, random_state=12).index.tolist() &gt;&gt; Training set / Test set 나누기 1234random_idx = positive_random_idx + negative_random_idxX = preprocessed_df.loc[random_idx, preprocessed_df.columns != 'Legendary']y = preprocessed_df['Legendary'][random_idx]x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1) 1x_train.shape, y_train.shape ((97, 31), (97,)) 1x_test.shape, y_test.shape ((33, 31), (33,)) &gt;&gt; 모델 재학습 123456# Fit in Training setlogit2 = LogisticRegression(random_state=0)logit2.fit(x_train, y_train)# Predict in Test sety_pred2 = logit2.predict(x_test) &gt;&gt; 모델 재평가 123456# clssification result for test setprint(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred2))print(\"Precision: %.3f\" % precision_score(y_test, y_pred2))print(\"Recall: %.3f\" % recall_score(y_test, y_pred2))print(\"F1: %.3f\" % f1_score(y_test, y_pred2)) accuracy: 1.00 Precision: 1.000 Recall: 1.000 F1: 1.000 12345678# confusion matrixconfu2 = confusion_matrix(y_true=y_test, y_pred = y_pred2)plt.figure(figsize=(4, 3))sns.heatmap(confu2, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')plt.title('Confusion Matrix')plt.show() 클래스 불균형을 조정한 후, 새롭게 학습된 모델의 performance가 많이 좋아졌습니다. 4. 비지도 학습 기반 군집 분석 – K-Means Clustering 4-1. K-Means 군집 분석 (1) 2차원 군집 분석 12345678910111213141516from sklearn.cluster import KMeans# K-means train &amp; Elbow methodX = preprocessed_df[['Attack', 'Defense']]k_list = []cost_list = []for k in range (1, 8): kmeans = KMeans(n_clusters=k).fit(X) interia = kmeans.inertia_ # inertia: Sum of squared distances of samples to their closest cluster center. print(\"k:\", k, \"| cost:\", interia) k_list.append(k) cost_list.append(interia)plt.figure(figsize=(8, 6))plt.plot(k_list, cost_list) k: 1 | cost: 1600.0 k: 2 | cost: 853.3477298974242 k: 3 | cost: 642.3911470107209 k: 4 | cost: 480.49450250321513 k: 5 | cost: 403.97191765107124 k: 6 | cost: 343.98696660525184 k: 7 | cost: 295.56093457429847 [&lt;matplotlib.lines.Line2D at 0x2e930467c48&gt;] 추세를 봤을 때, 4 clusters가 제일 적당한 것으로 보입니다. 따라서, cluster를 4로 지정한 후 다시 학습시킨 뒤, 각 데이터가 분류된 cluster 결과를 원 데이터셋에 추가합니다. 12345# k-means fitting and predictkmeans = KMeans(n_clusters=4).fit(X)cluster_num = pd.Series(kmeans.predict(X))preprocessed_df['cluster_num'] = cluster_num.valuespreprocessed_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total HP Attack Defense Sp. Atk Sp. Def Speed Legendary Bug Dark ... Rock Steel Water Generation_1 Generation_2 Generation_3 Generation_4 Generation_5 Generation_6 cluster_num 0 -0.976765 -0.950626 -0.924906 -0.797154 -0.239130 -0.248189 -0.801503 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 1 -0.251088 -0.362822 -0.524130 -0.347917 0.219560 0.291156 -0.285015 0 0 0 ... 0 0 0 1 0 0 0 0 0 2 2 0.749845 0.420917 0.092448 0.293849 0.831146 1.010283 0.403635 0 0 0 ... 0 0 0 1 0 0 0 0 0 2 3 1.583957 0.420917 0.647369 1.577381 1.503891 1.729409 0.403635 0 0 0 ... 0 0 0 1 0 0 0 0 0 1 4 -1.051836 -1.185748 -0.832419 -0.989683 -0.392027 -0.787533 -0.112853 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 5 rows × 33 columns 1print(preprocessed_df['cluster_num'].value_counts()) 2 309 0 253 3 128 1 110 Name: cluster_num, dtype: int64 &gt;&gt; 군집 시각화 1234567891011121314151617181920# Visualizationplt.scatter(preprocessed_df[preprocessed_df['cluster_num'] == 0]['Attack'], preprocessed_df[preprocessed_df['cluster_num'] == 0]['Defense'], s = 50, c = 'red', alpha = 0.5, label = 'Pokemon Group 1')plt.scatter(preprocessed_df[preprocessed_df['cluster_num'] == 1]['Attack'], preprocessed_df[preprocessed_df['cluster_num'] == 1]['Defense'], s = 50, c = 'green', alpha = 0.7, label = 'Pokemon Group 2')plt.scatter(preprocessed_df[preprocessed_df['cluster_num'] == 2]['Attack'], preprocessed_df[preprocessed_df['cluster_num'] == 2]['Defense'], s = 50, c = 'blue', alpha = 0.5, label = 'Pokemon Group 3')plt.scatter(preprocessed_df[preprocessed_df['cluster_num'] == 3]['Attack'], preprocessed_df[preprocessed_df['cluster_num'] == 3]['Defense'], s = 50, c = 'yellow', alpha = 0.8, label = 'Pokemon Group 4')plt.title('Pokemon Cluster by \"Attack\" &amp; \"Defense\"')plt.xlabel('Attack')plt.ylabel('Defense')plt.legend()plt.show() (2) 다차원 군집 분석 12345678910111213141516from sklearn.cluster import KMeans# K-Means train &amp; Elbow methodX = preprocessed_df[['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']]k_list = []cost_list = []for k in range (1, 15): kmeans = KMeans(n_clusters=k).fit(X) interia = kmeans.inertia_ # inertia: Sum of squared distances of samples to their closest cluster center. print(\"k:\", k, \"| cost:\", interia) k_list.append(k) cost_list.append(interia)plt.figure(figsize=(8, 6))plt.plot(k_list, cost_list) k: 1 | cost: 4800.0 k: 2 | cost: 3275.3812330305977 k: 3 | cost: 2862.057922495397 k: 4 | cost: 2566.5807792995274 k: 5 | cost: 2328.0706840275643 k: 6 | cost: 2182.759972635841 k: 7 | cost: 2070.734327066247 k: 8 | cost: 1957.5240844927844 k: 9 | cost: 1854.3770148227836 k: 10 | cost: 1778.3178764912984 k: 11 | cost: 1721.845255688537 k: 12 | cost: 1644.3967658442484 k: 13 | cost: 1579.4938049394318 k: 14 | cost: 1536.785887021493 [&lt;matplotlib.lines.Line2D at 0x2e930efbb88&gt;] 이 경우에는 cluster가 5일 때가 제일 적당해 보입니다. 12345# k-means fitting and predictkmeans = KMeans(n_clusters=5).fit(X)cluster_num = pd.Series(kmeans.predict(X))preprocessed_df['cluster_num'] = cluster_num.valuespreprocessed_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Total HP Attack Defense Sp. Atk Sp. Def Speed Legendary Bug Dark ... Rock Steel Water Generation_1 Generation_2 Generation_3 Generation_4 Generation_5 Generation_6 cluster_num 0 -0.976765 -0.950626 -0.924906 -0.797154 -0.239130 -0.248189 -0.801503 0 0 0 ... 0 0 0 1 0 0 0 0 0 4 1 -0.251088 -0.362822 -0.524130 -0.347917 0.219560 0.291156 -0.285015 0 0 0 ... 0 0 0 1 0 0 0 0 0 1 2 0.749845 0.420917 0.092448 0.293849 0.831146 1.010283 0.403635 0 0 0 ... 0 0 0 1 0 0 0 0 0 1 3 1.583957 0.420917 0.647369 1.577381 1.503891 1.729409 0.403635 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 4 -1.051836 -1.185748 -0.832419 -0.989683 -0.392027 -0.787533 -0.112853 0 0 0 ... 0 0 0 1 0 0 0 0 0 4 5 rows × 33 columns &gt;&gt; 군집별 특성 시각화 2차원이 아니기 때문에 위와 같이 군집 결과를 시각화하기 어렵습니다. 군집화 결과를 확인하기 위해 각 Feature의 군집별 특성을 시각화하도록 하겠습니다. 1234# HPsns.boxplot(x = \"cluster_num\", y = \"HP\", data = preprocessed_df)plt.title('군집별 \"HP\" 분포')plt.show() 1234# Attacksns.boxplot(x = 'cluster_num', y = 'Attack', data = preprocessed_df)plt.title('군집별 \"Attack\" 분포')plt.show() 1234# Defensesns.boxplot(x = 'cluster_num', y = 'Defense', data = preprocessed_df)plt.title('군집별 \"Defense\" 분포')plt.show() 1234# Sp. Atksns.boxplot(x = 'cluster_num', y = 'Sp. Atk', data = preprocessed_df)plt.title('군집별 \"Sp. Atk\" 분포')plt.show() 1234# Sp. Defsns.boxplot(x = 'cluster_num', y = 'Sp. Def', data = preprocessed_df)plt.title('군집별 \"Sp. Def\" 분포')plt.show() 1234# Speedsns.boxplot(x = 'cluster_num', y = 'Speed', data = preprocessed_df)plt.title('군집별 \"Speed\" 분포')plt.show() document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【EXERCISE】","slug":"【EXERCISE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/"},{"name":"Python","slug":"【EXERCISE】/Python","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"sklearn","slug":"sklearn","permalink":"https://hyemin-kim.github.io/tags/sklearn/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://hyemin-kim.github.io/tags/Machine-Learning/"},{"name":"분류","slug":"분류","permalink":"https://hyemin-kim.github.io/tags/%EB%B6%84%EB%A5%98/"}]},{"title":"【실습】 Python >> EDA & Linear Regression -- 부동산 가격 예측","slug":"E-Python-LinearRegression-1","date":"2020-08-10T15:42:27.000Z","updated":"2020-10-28T06:49:47.961Z","comments":true,"path":"2020/08/11/E-Python-LinearRegression-1/","link":"","permalink":"https://hyemin-kim.github.io/2020/08/11/E-Python-LinearRegression-1/","excerpt":"","text":"【EDA &amp; Regression 실습】 – 부동산 데이터 0. 개요 1. Library &amp; Data Import 2. 데이터 파악 (EDA: 탐색적 데이터 분석) 2-1. 데이터셋 기본 정보 파악 2-2. 종속 변수(목표 변수) 탐색 2-3. 설명 변수 탐색 2-4. 설명변수와 종속변수 간의 관계 탐색 3. 주택 가격 예측 모델링: 회귀 분석 3-1. 데이터 전처리 3-2. 회귀 모델링 3-3. 모델 해석 3-4. 모델 예측 결과 및 성능 평가 0. 개요 미국 매사추세츠주의 주택 가격 데이터(Boston Housing 1970)를 활용해 지역의 평균 주택 가격을 예측하는 선형 회귀 모델을 만들었고, 이를 기초하여 주택 가격의 영향 요소 파악 및 주택 가격 예측을 진행하였습니다. 전체 분석 절차는 다음과 같습니다: 데이터 파악 (EDA: 탐색적 데이터 분석) 데이터셋 기본 정보 파악 변수 특징 탐색 변수간 관계 탐색 데이터 전처리 모델링 주택 가격 영향 요소 파악 주택 가격 예측 및 모델 예측 성능 평가 1. Library &amp; Data Import &gt;&gt; 사용할 Library 123456789%matplotlib inlineimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.filterwarnings('ignore') &gt;&gt; 사용할 데이터셋 – Boston Housing Dataset 분석에 사용될 데이터셋은 Boston Housing 1970데이터의 일부 변수를 추출한 데이터입니다. 여기에 미국 매사추세츠주 92개 도시(TOWN)의 506개 지역의 주택 가격 및 기타 지역 특성 데이터가 포함되어 있습니다. (Dataset Introduction) 데이터셋을 불러와서 첫 다섯 줄을 출력하여 데이터의 구성을 한 번 살펴볼게요. 1df = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/BostonHousing2.csv\") 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TOWN LON LAT CMEDV CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 0 Nahant -70.955 42.2550 24.0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 1 Swampscott -70.950 42.2875 21.6 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 2 Swampscott -70.936 42.2830 34.7 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 3 Marblehead -70.928 42.2930 33.4 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 4 Marblehead -70.922 42.2980 36.2 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 5.33 &gt;&gt; Feature Description 각 변수의 의미는 다음과 같습니다: TOWN: 소속 도시 이름 LON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보 CMEDV: 해당 지역의 주택 가격 (중앙값) (corrected median values of housing in USD 1000) CRIM: 지역 범죄율 (per capita crime) ZN: 소속 도시에 25,000 제곱 피트(sq.ft) 이상의 주택지 비율 INDUS: 소속 도시에 상업적 비즈니스에 활용되지 않는 농지 면적 CHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable) NOX: 소속 도시의 산화질소 농도 RM: 해당 지역의 자택당 평균 방 갯수 AGE: 해당 지역에 1940년 이전에 건설된 주택의 비율 DIS: 5개의 보스턴 고용 센터와의 거리에 따른 가중치 부여 RAD: 소속 도시가 Radial 고속도로와의 접근성 지수 TAX: 소속 도시의 10000달러당 재산세 PTRATIO: 소속 도시의 학생-교사 비율 B: 해당 지역의 흑인 지수 (1000(Bk - 0.63)^2), Bk는 흑인의 비율 LSTAT: 해당 지역의 빈곤층 비율 2. 데이터 파악 (EDA: 탐색적 데이터 분석) 이제 데이터셋의 기본 정보 및 각 변수의 특성을 파악해 보겠습니다. 12# 그래프 배경 설정sns.set_style('darkgrid') 2-1. 데이터셋 기본 정보 파악 먼저 데이터셋의 기본 정보부터 알아볼게요. 12# shape (dimension)df.shape (506, 17) 12# 결측치df.isnull().sum() TOWN 0 LON 0 LAT 0 CMEDV 0 CRIM 0 ZN 0 INDUS 0 CHAS 0 NOX 0 RM 0 AGE 0 DIS 0 RAD 0 TAX 0 PTRATIO 0 B 0 LSTAT 0 dtype: int64 데이터셋은 총 506개의 관측치(observations)과 17개의 변수(variables)로 구성되어 있고 결측치는 존재하지 않습니다. 각 변수의 타입 및 기초 통계량 (범주형 변수는 범주 구성) 을 확인 해보면 다음과 같습니다. 12# data typedf.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 506 entries, 0 to 505 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 TOWN 506 non-null object 1 LON 506 non-null float64 2 LAT 506 non-null float64 3 CMEDV 506 non-null float64 4 CRIM 506 non-null float64 5 ZN 506 non-null float64 6 INDUS 506 non-null float64 7 CHAS 506 non-null int64 8 NOX 506 non-null float64 9 RM 506 non-null float64 10 AGE 506 non-null float64 11 DIS 506 non-null float64 12 RAD 506 non-null int64 13 TAX 506 non-null int64 14 PTRATIO 506 non-null float64 15 B 506 non-null float64 16 LSTAT 506 non-null float64 dtypes: float64(13), int64(3), object(1) memory usage: 67.3+ KB 이중 TOWN(소속 도시 이름)만 문자형 변수이고, 이를 제외한 모든 변수가 숫자형 변수입니다. 12# numerical variabledf.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LON LAT CMEDV CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT count 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 mean -71.056389 42.216440 22.528854 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063 std 0.075405 0.061777 9.182176 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062 min -71.289500 42.030000 5.000000 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.730000 25% -71.093225 42.180775 17.025000 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.950000 50% -71.052900 42.218100 21.200000 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.360000 75% -71.019625 42.252250 25.000000 3.677082 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000 max -70.810000 42.381000 50.000000 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000 1234# categorical variablenum_town = df['TOWN'].unique()print(len(num_town))num_town 92 array(['Nahant', 'Swampscott', 'Marblehead', 'Salem', 'Lynn', 'Sargus', 'Lynnfield', 'Peabody', 'Danvers', 'Middleton', 'Topsfield', 'Hamilton', 'Wenham', 'Beverly', 'Manchester', 'North Reading', 'Wilmington', 'Burlington', 'Woburn', 'Reading', 'Wakefield', 'Melrose', 'Stoneham', 'Winchester', 'Medford', 'Malden', 'Everett', 'Somerville', 'Cambridge', 'Arlington', 'Belmont', 'Lexington', 'Bedford', 'Lincoln', 'Concord', 'Sudbury', 'Wayland', 'Weston', 'Waltham', 'Watertown', 'Newton', 'Natick', 'Framingham', 'Ashland', 'Sherborn', 'Brookline', 'Dedham', 'Needham', 'Wellesley', 'Dover', 'Medfield', 'Millis', 'Norfolk', 'Walpole', 'Westwood', 'Norwood', 'Sharon', 'Canton', 'Milton', 'Quincy', 'Braintree', 'Randolph', 'Holbrook', 'Weymouth', 'Cohasset', 'Hull', 'Hingham', 'Rockland', 'Hanover', 'Norwell', 'Scituate', 'Marshfield', 'Duxbury', 'Pembroke', 'Boston Allston-Brighton', 'Boston Back Bay', 'Boston Beacon Hill', 'Boston North End', 'Boston Charlestown', 'Boston East Boston', 'Boston South Boston', 'Boston Downtown', 'Boston Roxbury', 'Boston Savin Hill', 'Boston Dorchester', 'Boston Mattapan', 'Boston Forest Hills', 'Boston West Roxbury', 'Boston Hyde Park', 'Chelsea', 'Revere', 'Winthrop'], dtype=object) 2-2. 종속 변수(목표 변수) 탐색 &gt;&gt; Target Variable: ‘CMEDV’(주택 가격) 탐색 이제 각 변수의 특성을 시각화 도구를 통해 파악해보겠습니다. 먼저 우리가 예측하고자 하는 대상, 즉 회귀 모델의 종속 변수인 “주택 가격”(‘CMECV’) 부터 살펴볼게요. 12# 기초 통계량df['CMEDV'].describe() count 506.000000 mean 22.528854 std 9.182176 min 5.000000 25% 17.025000 50% 21.200000 75% 25.000000 max 50.000000 Name: CMEDV, dtype: float64 12# 분포df['CMEDV'].hist(bins=50) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ca69ebcf48&gt; boxplot: Pandas Function (pandas.DataFrame.boxplot) Matplotlib Function (matplotlib.pyplot.boxplot) 123# boxplot - Pandasdf.boxplot(column=['CMEDV'])plt.show() 123# boxplot - matplotlibplt.boxplot(df['CMEDV'])plt.show() 분포를 살펴보면, 주택 가격이 대부분 $17,000 ~ $25,000 사이에 분포되어 있으며, 소수의 $40,000 이상인 고가 주택도 존재합니다. 2-3. 설명 변수 탐색 &gt;&gt; 설명 변수의 분포 탐색 12345678# numerical features (except \"LON\" &amp; \"LAT\")numerical_columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']fig = plt.figure(figsize = (16, 20))ax = fig.gca() # Axes 생성df[numerical_columns].hist(ax=ax)plt.show() 2-4. 설명변수와 종속변수 간의 관계 탐색 &gt;&gt; 변수간의 상관계수 파악 먼저 변수간의 상관계수를 추출해보겠습니다. 12345# Person 상관계수cols = ['CMEDV', 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']corr = df[cols].corr(method = 'pearson')corr .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CMEDV CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT CMEDV 1.000000 -0.389582 0.360386 -0.484754 0.175663 -0.429300 0.696304 -0.377999 0.249315 -0.384766 -0.471979 -0.505655 0.334861 -0.740836 CRIM -0.389582 1.000000 -0.200469 0.406583 -0.055892 0.420972 -0.219247 0.352734 -0.379670 0.625505 0.582764 0.289946 -0.385064 0.455621 ZN 0.360386 -0.200469 1.000000 -0.533828 -0.042697 -0.516604 0.311991 -0.569537 0.664408 -0.311948 -0.314563 -0.391679 0.175520 -0.412995 INDUS -0.484754 0.406583 -0.533828 1.000000 0.062938 0.763651 -0.391676 0.644779 -0.708027 0.595129 0.720760 0.383248 -0.356977 0.603800 CHAS 0.175663 -0.055892 -0.042697 0.062938 1.000000 0.091203 0.091251 0.086518 -0.099176 -0.007368 -0.035587 -0.121515 0.048788 -0.053929 NOX -0.429300 0.420972 -0.516604 0.763651 0.091203 1.000000 -0.302188 0.731470 -0.769230 0.611441 0.668023 0.188933 -0.380051 0.590879 RM 0.696304 -0.219247 0.311991 -0.391676 0.091251 -0.302188 1.000000 -0.240265 0.205246 -0.209847 -0.292048 -0.355501 0.128069 -0.613808 AGE -0.377999 0.352734 -0.569537 0.644779 0.086518 0.731470 -0.240265 1.000000 -0.747881 0.456022 0.506456 0.261515 -0.273534 0.602339 DIS 0.249315 -0.379670 0.664408 -0.708027 -0.099176 -0.769230 0.205246 -0.747881 1.000000 -0.494588 -0.534432 -0.232471 0.291512 -0.496996 RAD -0.384766 0.625505 -0.311948 0.595129 -0.007368 0.611441 -0.209847 0.456022 -0.494588 1.000000 0.910228 0.464741 -0.444413 0.488676 TAX -0.471979 0.582764 -0.314563 0.720760 -0.035587 0.668023 -0.292048 0.506456 -0.534432 0.910228 1.000000 0.460853 -0.441808 0.543993 PTRATIO -0.505655 0.289946 -0.391679 0.383248 -0.121515 0.188933 -0.355501 0.261515 -0.232471 0.464741 0.460853 1.000000 -0.177383 0.374044 B 0.334861 -0.385064 0.175520 -0.356977 0.048788 -0.380051 0.128069 -0.273534 0.291512 -0.444413 -0.441808 -0.177383 1.000000 -0.366087 LSTAT -0.740836 0.455621 -0.412995 0.603800 -0.053929 0.590879 -0.613808 0.602339 -0.496996 0.488676 0.543993 0.374044 -0.366087 1.000000 상관계수를 좀 더 직관적인 Heatmap으로 표현해볼게요. 123456789# heatmap (seaborn)fig = plt.figure(figsize = (16, 12))ax = fig.gca()sns.set(font_scale = 1.5) # heatmap 안의 font-size 설정heatmap = sns.heatmap(corr.values, annot = True, fmt='.2f', annot_kws={'size':15}, yticklabels = cols, xticklabels = cols, ax=ax, cmap = \"RdYlBu\")plt.tight_layout()plt.show() 우리의 관심사인 target variable **“CMEDV - 주택 가격”**과 다른 변수간의 상관관계를 살펴보면, **“CMEDV - 주택 가격”**은 “RM - 자택당 평균 방 갯수”(0.7) 및 **“LSTAT - 빈곤층의 비율”(-0.74)**과 강한 상관관계를 보이고 있다는 것을 알 수 있습니다. 이 두 변수와의 관계를 좀 더 자세히 살펴볼게요. &gt;&gt; 종속 변수와 설명 변수간의 관계 탐색 주택 가격 ( “CMEDV” ) ~ 방 갯수 ( “RM” ) 1234# scatter plotsns.scatterplot(data=df, x='RM', y='CMEDV', markers='o', color='blue', alpha=0.6)plt.title('Scatter Plot')plt.show() 주택 가격이 방 갯수와 양의 상관관계(positive correlation)를 갖고 있습니다. 즉, 방 갯수가 많은 주택들이 상대적으로 더 높은 가격을 갖고 있습니다. 주택 가격(“CMEDV”) ~ 빈곤층의 비율(“LSTAT”) 1234# scatter plotsns.scatterplot(data=df, x='LSTAT', y='CMEDV', markers='o', color='blue', alpha=0.6)plt.title('Scatter Plot')plt.show() 주택 가격이 빈곤층의 비율과 음의 상관관계(negative correlation)를 갖고 있습니다. 즉, 빈곤층의 비율이 높은 지역의 주택 가격이 상대적으로 낮은 경향이 있습니다. &gt;&gt; 도시별 차이 탐색 데이터를 살펴보면 여러 지역이 같은 도시에 속한 경우가 있습니다. 변수 중에서도 도시 단위로 측정되는 변수가 많고요. 따라서 우리는 자연스럽게 도시 간의 차이를 궁금하게 됩니다. 먼저 각 도시의 데이터 갯수부터 살펴볼게요. 12# 도시별 데이터 갯수df['TOWN'].value_counts() Cambridge 30 Boston Savin Hill 23 Lynn 22 Boston Roxbury 19 Newton 18 .. Hanover 1 Hull 1 Sherborn 1 Hamilton 1 Dover 1 Name: TOWN, Length: 92, dtype: int64 12# 도시별 데이터 갯수 (bar plot)df['TOWN'].value_counts().hist(bins=50) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ca6c233a48&gt; 이제 각 도시의 주택 가격 분포를 box plot을 통해 표현 해보겠습니다. 123# 도시별 주택 가격 특징 (boxplot 이용)fig = plt.figure(figsize = (12, 20))sns.boxplot(x='CMEDV', y='TOWN', data=df) &lt;matplotlib.axes._subplots.AxesSubplot at 0x23cf3ff9ec8&gt; 그림을 보면, Boston 지역(Boston으로 시작하는 도시)의 주택 가격이 전반적으로 다른 지역보다 낮다는 것을 알 수 있습니다. 도시별 범죄율을 한 번 확인 해보면, 123# 도시별 범죄율 특징fig = plt.figure(figsize = (12, 20))sns.boxplot(x='CRIM', y='TOWN', data=df) &lt;matplotlib.axes._subplots.AxesSubplot at 0x23cf407fec8&gt; Boston 지역의 범죄율이 유독 높다는 것을 확인할 수 있고, 따라서 범죄율이 높은 지역의 주택 가격이 상대적으로 낮다는 것을 추측해볼 수 있겠습니다. 3. 주택 가격 예측 모델링: 회귀 분석 이제 변수들을 활용하여 매사추세츠주 각 지역의 주택 가격을 예측하는 회귀 모델을 만들어 보겠습니다. 3-1. 데이터 전처리 &gt;&gt; Feature 표준화 먼저 Feature 들의 scale 차이를 없애기 위해 수치형 Feature에 대해서 표준화를 진행해야 합니다. 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TOWN LON LAT CMEDV CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 0 Nahant -70.955 42.2550 24.0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 1 Swampscott -70.950 42.2875 21.6 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 2 Swampscott -70.936 42.2830 34.7 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 3 Marblehead -70.928 42.2930 33.4 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 4 Marblehead -70.922 42.2980 36.2 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 5.33 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 506 entries, 0 to 505 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 TOWN 506 non-null object 1 LON 506 non-null float64 2 LAT 506 non-null float64 3 CMEDV 506 non-null float64 4 CRIM 506 non-null float64 5 ZN 506 non-null float64 6 INDUS 506 non-null float64 7 CHAS 506 non-null int64 8 NOX 506 non-null float64 9 RM 506 non-null float64 10 AGE 506 non-null float64 11 DIS 506 non-null float64 12 RAD 506 non-null int64 13 TAX 506 non-null int64 14 PTRATIO 506 non-null float64 15 B 506 non-null float64 16 LSTAT 506 non-null float64 dtypes: float64(13), int64(3), object(1) memory usage: 67.3+ KB 문자형 변수인 \"TOWN\"와 범주형 변수인 “CHAS” (Dummy variable)를 제외하여 모든 수치형 변수에 대해서 표준화를 진행합니다. 123456from sklearn.preprocessing import StandardScaler# feature standardization (numerical_columns except dummy var.-\"CHAS\")scaler = StandardScaler() # 평균 0, 표준편차 1scale_columns = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']df[scale_columns] = scaler.fit_transform(df[scale_columns]) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TOWN LON LAT CMEDV CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 0 Nahant -70.955 42.2550 24.0 -0.419782 0.284830 -1.287909 0 -0.144217 0.413672 -0.120013 0.140214 -0.982843 -0.666608 -1.459000 0.441052 -1.075562 1 Swampscott -70.950 42.2875 21.6 -0.417339 -0.487722 -0.593381 0 -0.740262 0.194274 0.367166 0.557160 -0.867883 -0.987329 -0.303094 0.441052 -0.492439 2 Swampscott -70.936 42.2830 34.7 -0.417342 -0.487722 -0.593381 0 -0.740262 1.282714 -0.265812 0.557160 -0.867883 -0.987329 -0.303094 0.396427 -1.208727 3 Marblehead -70.928 42.2930 33.4 -0.416750 -0.487722 -1.306878 0 -0.835284 1.016303 -0.809889 1.077737 -0.752922 -1.106115 0.113032 0.416163 -1.361517 4 Marblehead -70.922 42.2980 36.2 -0.412482 -0.487722 -1.306878 0 -0.835284 1.228577 -0.511180 1.077737 -0.752922 -1.106115 0.113032 0.441052 -1.026501 &gt;&gt; Training set / Test set 나누기 나중에 도출될 예측 모델의 예측 성능을 평가하기 위해, 먼저 전체 데이터셋을 \"Training set\"과 \"Test set\"으로 나누겠습니다. Training set에서 모델을 학습하고 Test set에서 모델의 예측 성능을 검증할 겁니다. 12# features for linear regression modeldf[numerical_columns].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 0 -0.419782 0.284830 -1.287909 0 -0.144217 0.413672 -0.120013 0.140214 -0.982843 -0.666608 -1.459000 0.441052 -1.075562 1 -0.417339 -0.487722 -0.593381 0 -0.740262 0.194274 0.367166 0.557160 -0.867883 -0.987329 -0.303094 0.441052 -0.492439 2 -0.417342 -0.487722 -0.593381 0 -0.740262 1.282714 -0.265812 0.557160 -0.867883 -0.987329 -0.303094 0.396427 -1.208727 3 -0.416750 -0.487722 -1.306878 0 -0.835284 1.016303 -0.809889 1.077737 -0.752922 -1.106115 0.113032 0.416163 -1.361517 4 -0.412482 -0.487722 -1.306878 0 -0.835284 1.228577 -0.511180 1.077737 -0.752922 -1.106115 0.113032 0.441052 -1.026501 123456from sklearn.model_selection import train_test_split# split dataset into training &amp; testX = df[numerical_columns]y = df['CMEDV']X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) 1X_train.shape, y_train.shape ((404, 13), (404,)) 1X_test.shape, y_test.shape ((102, 13), (102,)) &gt;&gt; 다중공선성 회귀 분석에서 하나의 feature(예측 변수)가 다른 feature와의 상관 관계가 높으면(즉, 다중공선성이 존재하면), 회귀 분석 시 부정적인 영향을 미칠 수 있기 때문에, 모델링 하기 전에 먼저 다중공선성의 존재 여부를 확인해야합니다. 보통 다중공선성을 판단할 때 VIF값을 확인합니다. 일반적으로, VIF &gt; 10인 feature들은 다른 변수와의 상관관계가 높아, 다중공선성이 존재하는 것으로 판단합니다. 123456from statsmodels.stats.outliers_influence import variance_inflation_factorvif = pd.DataFrame()vif['features'] = X_train.columnsvif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]vif.round(1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } features VIF Factor 0 CRIM 1.7 1 ZN 2.5 2 INDUS 3.8 3 CHAS 1.1 4 NOX 4.4 5 RM 1.9 6 AGE 3.2 7 DIS 4.2 8 RAD 8.1 9 TAX 9.8 10 PTRATIO 1.9 11 B 1.4 12 LSTAT 3.0 VIF값을 확인해보면, 모든 변수의 VIF값이 다 10 이하입니다. 따라서 다중공선성 문제가 존재하지 않아 모든 feature을 활용하여 회귀 모델링을 진행하면 됩니다. 3-2. 회귀 모델링 먼저 Training set에서 선형 회귀 예측 모델을 학습합니다. 그 다음 도출된 모델을 Test set에 적용해 주택 가격(“CMEDV”)을 예측합니다. 이 결과는 다중에 실제 “CMEDV” 값과 비교하여 모델의 예측 성능을 평가하는 데 활용하게 됩니다. 12345678from sklearn import linear_model# fit regression model in training setlr = linear_model.LinearRegression()model = lr.fit(X_train, y_train)# predict in test setpred_test = lr.predict(X_test) 3-3. 모델 해석 &gt;&gt; coefficients 확인하기 먼저 각 feature의 회귀 계수를 확인해보겠습니다. 12# print coefprint(lr.coef_) [-0.9479409 1.39796831 0.14786968 2.13469673 -2.25995614 2.15879342 0.12103297 -3.23121173 2.63662665 -1.95959865 -2.05639351 0.65670428 -3.93702535] 123# \"feature - coefficients\" DataFrame 만들기coefs = pd.DataFrame(zip(df[numerical_columns].columns, lr.coef_), columns = ['feature', 'coefficients'])coefs .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature coefficients 0 CRIM -0.947941 1 ZN 1.397968 2 INDUS 0.147870 3 CHAS 2.134697 4 NOX -2.259956 5 RM 2.158793 6 AGE 0.121033 7 DIS -3.231212 8 RAD 2.636627 9 TAX -1.959599 10 PTRATIO -2.056394 11 B 0.656704 12 LSTAT -3.937025 123# 크기 순서로 나열coefs_new = coefs.reindex(coefs.coefficients.abs().sort_values(ascending=False).index)coefs_new .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } feature coefficients 12 LSTAT -3.937025 7 DIS -3.231212 8 RAD 2.636627 4 NOX -2.259956 5 RM 2.158793 3 CHAS 2.134697 10 PTRATIO -2.056394 9 TAX -1.959599 1 ZN 1.397968 0 CRIM -0.947941 11 B 0.656704 2 INDUS 0.147870 6 AGE 0.121033 1234567891011## coefficients 시각화# figure sizeplt.figure(figsize = (8, 8))# bar plotplt.barh(coefs_new['feature'], coefs_new['coefficients'])plt.title('\"feature - coefficient\" Graph')plt.xlabel('coefficients')plt.ylabel('features')plt.show() &gt;&gt; feature 유의성 검정 12345import statsmodels.api as smX_train2 = sm.add_constant(X_train)model2 = sm.OLS(y_train, X_train2).fit()model2.summary() OLS Regression Results Dep. Variable: CMEDV R-squared: 0.734 Model: OLS Adj. R-squared: 0.725 Method: Least Squares F-statistic: 82.86 Date: Tue, 11 Aug 2020 Prob (F-statistic): 1.72e-103 Time: 00:22:07 Log-Likelihood: -1191.9 No. Observations: 404 AIC: 2412. Df Residuals: 390 BIC: 2468. Df Model: 13 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] const 22.4313 0.245 91.399 0.000 21.949 22.914 CRIM -0.9479 0.290 -3.263 0.001 -1.519 -0.377 ZN 1.3980 0.372 3.758 0.000 0.667 2.129 INDUS 0.1479 0.458 0.323 0.747 -0.753 1.049 CHAS 2.1347 0.899 2.375 0.018 0.367 3.902 NOX -2.2600 0.490 -4.617 0.000 -3.222 -1.298 RM 2.1588 0.332 6.495 0.000 1.505 2.812 AGE 0.1210 0.415 0.292 0.771 -0.695 0.937 DIS -3.2312 0.477 -6.774 0.000 -4.169 -2.293 RAD 2.6366 0.671 3.931 0.000 1.318 3.955 TAX -1.9596 0.731 -2.679 0.008 -3.398 -0.522 PTRATIO -2.0564 0.319 -6.446 0.000 -2.684 -1.429 B 0.6567 0.272 2.414 0.016 0.122 1.191 LSTAT -3.9370 0.405 -9.723 0.000 -4.733 -3.141 Omnibus: 169.952 Durbin-Watson: 1.935 Prob(Omnibus): 0.000 Jarque-Bera (JB): 859.012 Skew: 1.762 Prob(JB): 2.94e-187 Kurtosis: 9.213 Cond. No. 10.7 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. &gt;&gt; 주택 가격 영향 요소 위 결과를 종합해 보면, 주택 가격의 영향 요소에 관하여 다음과 같은 결론들을 도출할 수 있습니다: \"INDUS\"(상업적 비즈니스에 활용되지 않는 농지 면적)과 “AGE”(1940년 이전에 건설된 비율)은 유의하지 않습니다. (p value &gt; 0.05) \"ZN\"(25,000 제곱 피트(sq.ft) 이상의 주택지 비율), \"CHAS\"(Charles 강과 접하고 있는지 여부), \"RM\"(자택당 평균 방 갯수), \"RAD\"(소속 도시가 Radial 고속도로와의 접근성 지수), \"B\"(흑인 지수)는 주택 가격에 Positive한 영향을 미칩니다. 즉, 다른 변수의 값이 고정했을 때, 해당 변수의 값이 클수록 주택의 가격이 높을 것입니다. \"CRIM\"(지역 범죄율), \"NOX\"(산화질소 농도), \"DIS\"(보스턴 고용 센터와의 거리), \"TAX\"(재산세), \"PTRATIO\"(학생-교사 비율), \"LSTAT\"(빈곤층 비율)은 주택 가격에 Negative한 영향을 미칩니다. 즉, 다른 변수의 값이 고정했을 때, 해당 변수의 값이 작을수록 주택의 가격이 높을 것입니다. 3-4. 모델 예측 결과 및 성능 평가 &gt;&gt; 예측 결과 시각화 학습한 모델을 Test set에 적용하여 y값(“CMEDV”)을 예측합니다. 예측 결과를 확인하기 위해 실제값과 예측값을 한 plot에 출력해 시각화해보겠습니다. 12345678910# 예측 결과 시각화 (test set)df = pd.DataFrame({'actual': y_test, 'prediction': pred_test})df = df.sort_values(by='actual').reset_index(drop=True)plt.figure(figsize=(12, 9))plt.scatter(df.index, df['prediction'], marker='x', color='r')plt.scatter(df.index, df['actual'], alpha=0.7, marker='o', color='black')plt.title(\"Prediction Result in Test Set\", fontsize=20)plt.legend(['prediction', 'actual'], fontsize=12)plt.show() &gt;&gt; 모델 성능 평가 모델의 예측 성능을 평가하기 위해 모델의 R square과 RMSE를 계산해볼게요. R square 123# R squareprint(model.score(X_train, y_train)) # training setprint(model.score(X_test, y_test)) # test set 0.7341832055169144 0.7639579157366423 RMSE 12345678910# RMSEfrom sklearn.metrics import mean_squared_errorfrom math import sqrt# training setpred_train = lr.predict(X_train)print(sqrt(mean_squared_error(y_train, pred_train)))# test setprint(sqrt(mean_squared_error(y_test, pred_test))) 4.624051760840334 4.829847098176557 Test set에서 해당 예측 모델의 R square가 0.76이고, RMSE가 4.83입니다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【EXERCISE】","slug":"【EXERCISE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/"},{"name":"Python","slug":"【EXERCISE】/Python","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"sklearn","slug":"sklearn","permalink":"https://hyemin-kim.github.io/tags/sklearn/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://hyemin-kim.github.io/tags/Machine-Learning/"},{"name":"회귀","slug":"회귀","permalink":"https://hyemin-kim.github.io/tags/%ED%9A%8C%EA%B7%80/"}]},{"title":"Python >> sklearn - (5) 비지도 학습 (Unsupervised Learning)","slug":"S-Python-sklearn5","date":"2020-08-06T04:57:29.000Z","updated":"2020-11-06T05:20:43.506Z","comments":true,"path":"2020/08/06/S-Python-sklearn5/","link":"","permalink":"https://hyemin-kim.github.io/2020/08/06/S-Python-sklearn5/","excerpt":"","text":"비지도 학습 (Unsupervised Learning) 1. 비지도 학습의 개요 2. 차원 축소 2-1. 데이터 로드 (iris 데이터) 2-2. PCA 차원 축소 2-3. LDA 차원 축소 2-4. SVD (특이값 분해) 3. 군집화 3-1. K-Means Clustering 3-2. DBSCAN 3-3. 실루엣 스코어 (군집화 평가) 1from IPython.display import Image 1. 비지도 학습의 개요 비지도 학습 (Unsupervised Learning)은 기계 학습의 일종으로, 데이터가 어떻게 구성되어 있는지를 알아내는 문제의 범주에 속한다. 이 방법은 지도 학습 (Supervised Learning) 혹은 강화 학습 (Reinforcement Learning)과는 달리 입력값에 대한 목표치가 주어지지 않는다 차원 축소: PCA, LDA, SVD 군집화: KMeans Clustering, DBSCAN 군집화 평가 2. 차원 축소 feature의 갯수를 줄이는 것을 뛰어 넘어, 특징을 추출하는 역할응 하기도 함 계산 비용을 감소하는 효과 전반적인 데이터에 대한 이해도를 높이는 효과 1234from sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCAfrom sklearn import datasetsimport pandas as pd 2-1. 데이터 로드 (iris 데이터) 1iris = datasets.load_iris() 1data = iris['data'] 1data[:5] array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2]]) 1df = pd.DataFrame(data, columns = iris['feature_names']) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 1df['target'] = iris['target'] 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 2-2. PCA 차원 축소 참고: PCA 원리 관련 블로그 주성분 분석 (PCA, Principal Component Analysis) 는 선형 차원 축소 기법이다. 매우 인기 있게 사용되는 차원 축소 기법중 하나다. PCA는 먼저 데이터에 가장 가까운 초평면(hyperplane)을 구한 다음, 데이터를 이 초평면에 투영(projection)시킨다. 주요 특징 중의 하나는 분산(variance)을 촤대한 보존한다는 점이다. 분산 보존 PCA는 데이터의 분산이 최대가 되는 축을 찾는다. 즉, 원본 데이터셋과 투영된 데이터셋 간의 평균제곱거리를 최소화하는 축을 찾는다. PCA 실현 과정 학습 데이터셋에서 분산이 최대인 축(axis)을 찾는다 이렇게 찾은 첫 번째 축과 직교(orthogonal)하면서 분산이 최대인 두 번째 축을 찾는다 첫 번째 축과 두 번째 축에 직교하고 분산을 최대한 보존하는 세 번째 축을 찾는다 1~3과 같은 방법으로 데이터셋의 차원(특성 수)만큼의 축을 찾는다 이렇게 i-번째 축을 정의하는 **단위 벡터(unit vector)**를 i-번째 주성분(PC, Principle Component)이라고 한다. &gt;&gt; sklearn에서 실현 [sklearn.decomposition.PCA] Documnet n_components에 1보다 작은 값을 넣으면, 분산을 기준으로 차원 축소 n_components에 1보다 큰 값을 넣으면, 해당 값을 기준으로 feature를 축소 (1) 주성분 2개로 지정 (n_components = 2) 1from sklearn.decomposition import PCA 12345678# 모델 선언pca = PCA(n_components=2)# data scalingdata_scaled = StandardScaler().fit_transform(df.loc[:, 'sepal length (cm)' : 'petal width (cm)'])# PCA 실행pca_data = pca.fit_transform(data_scaled) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 1data_scaled[:5] array([[-0.90068117, 1.01900435, -1.34022653, -1.3154443 ], [-1.14301691, -0.13197948, -1.34022653, -1.3154443 ], [-1.38535265, 0.32841405, -1.39706395, -1.3154443 ], [-1.50652052, 0.09821729, -1.2833891 , -1.3154443 ], [-1.02184904, 1.24920112, -1.34022653, -1.3154443 ]]) 1pca_data[:5] array([[-2.26470281, 0.4800266 ], [-2.08096115, -0.67413356], [-2.36422905, -0.34190802], [-2.29938422, -0.59739451], [-2.38984217, 0.64683538]]) 주성분에 따른 데이터 시각화 12345import matplotlib.pyplot as pltfrom matplotlib import cmimport seaborn as sns%matplotlib inline 1plt.scatter(pca_data[:, 0], pca_data[:, 1], c=df['target']) # c: color 기준 &lt;matplotlib.collections.PathCollection at 0x201028bf148&gt; (2) 분산을 기준으로 차원축소 (n_components &lt; 1) 123pca2 = PCA(n_components=0.99)pca_data2 = pca2.fit_transform(data_scaled)pca_data2[:5] array([[-2.26470281, 0.4800266 , -0.12770602], [-2.08096115, -0.67413356, -0.23460885], [-2.36422905, -0.34190802, 0.04420148], [-2.29938422, -0.59739451, 0.09129011], [-2.38984217, 0.64683538, 0.0157382 ]]) 1234567891011from mpl_toolkits.mplot3d import Axes3Dimport numpy as npfig = plt.figure(figsize=(10, 5))ax = fig.add_subplot(111, projection='3d') # Axe3D objectsample_size = 50ax.scatter(pca_data2[:,0], pca_data2[:,1], pca_data2[:,2], alpha=0.6, c=df['target'])plt.savefig('./tmp.svg')plt.title('ax.plot')plt.show() 2-3. LDA 차원 축소 참고 블로그: 차원 축소 - LDA(Linear Discriminant Analysis) 개요 머신러닝 기초9 - LDA (Linear Discriminant Analysis) LDA (Linear Discriminant Analysis): 선형 판별 분석법 (PCA와 유사) LDA는 클래스(Class)분리를 최대화하는 축을 찾기 위해 클래스 간 분산(between-class scatter)과 내분 분산(within-class scatter)의 비율을 최대화하는 방식으로 차원을 축소함. 즉, 클래스 간 분산은 최대한 크게 가져가고, 클래스 내부의 분산은 최대한 작게 가져가는 방식이다. &gt;&gt; sklearn에서 실현 12from sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.preprocessing import StandardScaler 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 12345678# 모델 선언lda = LinearDiscriminantAnalysis(n_components=2)# data scalingdata_scaled = StandardScaler().fit_transform(df.loc[:, 'sepal length (cm)' : 'petal width (cm)'])# LDA 실행lda_data = lda.fit_transform(data_scaled, df['target']) 1lda_data[:5] array([[-8.06179978, 0.30042062], [-7.12868772, -0.78666043], [-7.48982797, -0.26538449], [-6.81320057, -0.67063107], [-8.13230933, 0.51446253]]) 시각화 12# LDAplt.scatter(lda_data[:,0], lda_data[:,1], c=df['target']) &lt;matplotlib.collections.PathCollection at 0x20102cd5608&gt; PCA 결과와 비교 12# PCAplt.scatter(pca_data[:,0], pca_data[:,1], c=df['target']) &lt;matplotlib.collections.PathCollection at 0x20102ba6908&gt; 2-4. SVD (특이값 분해) 위키문서 SVD (Singular Value Decomposition): 특이값 분해 기법이다 PCA와 유사한 차원 축소 기법이다 scikit-learn 패키지에서는 truncated SVD (aka LSA)을 사용한다 상품의 추천 시스템에도 활용되어지는 알고리즘 (추천시스템) 1from sklearn.decomposition import TruncatedSVD 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 123svd = TruncatedSVD(n_components = 2)data_scaled = StandardScaler().fit_transform(df.loc[:, 'sepal length (cm)' : 'petal width (cm)'])svd_data = svd.fit_transform(data_scaled) 1svd_data[:5] array([[-2.26470281, 0.4800266 ], [-2.08096115, -0.67413356], [-2.36422905, -0.34190802], [-2.29938422, -0.59739451], [-2.38984217, 0.64683538]]) 시각화 12# SVDplt.scatter(svd_data[:,0], svd_data[:,1], c=df['target']) &lt;matplotlib.collections.PathCollection at 0x20102b2ed08&gt; PCA &amp; LDA와 비교 12# PCAplt.scatter(pca_data[:,0], pca_data[:,1], c=df['target']) &lt;matplotlib.collections.PathCollection at 0x20102ad7d88&gt; 12# LDAplt.scatter(lda_data[:,0], lda_data[:,1], c=df['target']) &lt;matplotlib.collections.PathCollection at 0x20102d43e08&gt; 3. 군집화 3-1. K-Means Clustering 위키문서 군집화에서 가장 대중적으로 사용되는 알고리즘이다. centroid라는 중점을 기준으로 가강 가까운 포인트를 선택하는 군집화 기법이다 원리: 주어진 데이터를 k개의 cluster로 묶는 방식, 거리 차이의 분산을 최소화하는 방식으로 동작. 1Image('https://image.slidesharecdn.com/patternrecognitionbinoy-06-kmeansclustering-160317135729/95/pattern-recognition-binoy-k-means-clustering-13-638.jpg') 사용되는 예제 스팸 문자 분류 뉴스 기사 분류 [sklearn.cluster.KMeans] Document 1from sklearn.cluster import KMeans 123kmeans = KMeans(n_clusters=3)data_scaled = StandardScaler().fit_transform(df.loc[:, 'sepal length (cm)' : 'petal width (cm)'])cluster_data = kmeans.fit_transform(data_scaled) 1cluster_data[:5] array([[3.12119834, 0.21295824, 3.98940603], [2.6755083 , 0.99604549, 4.01793312], [2.97416665, 0.65198444, 4.19343668], [2.88014429, 0.9034561 , 4.19784749], [3.30022609, 0.40215457, 4.11157152]]) 1kmeans.labels_ array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0]) 1sns.countplot(kmeans.labels_) &lt;matplotlib.axes._subplots.AxesSubplot at 0x201043c7fc8&gt; 1sns.countplot(df['target']) &lt;matplotlib.axes._subplots.AxesSubplot at 0x2010301bec8&gt; Hyper-parameter Tuning 1kmeans KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto', random_state=None, tol=0.0001, verbose=0) 1234# max_iter: maximum number of iterations for a single runkmeans2 = KMeans(n_clusters=3, max_iter=500)data_scaled = StandardScaler().fit_transform(df.loc[:, 'sepal length (cm)' : 'petal width (cm)'])cluster_data2 = kmeans2.fit_transform(data_scaled) 1sns.countplot(kmeans2.labels_) &lt;matplotlib.axes._subplots.AxesSubplot at 0x20105525688&gt; 3-2. DBSCAN 밀도 기반 클러스터링 (DBSCAN: Dencity-Based Spatial Clustering of Applications with Noise) 밀도가 높은 부분을 클러스터링 하는 방식 어느 점을 기준으로 반경 x내에 점이 n개 이상 있으면 하나의 군집으로 인식하는 방식 KMeans 에서는 n_cluster의 갯수를 반드시 지정해 주어야 하나, DBSCAN에서는 필요없음 기하학적인 clustering도 잘 찾아냄 1Image('https://image.slidesharecdn.com/pydatanyc2015-151119175854-lva1-app6891/95/pydata-nyc-2015-automatically-detecting-outliers-with-datadog-26-638.jpg') [sklearn.cluster.DBSCAN] Document 주의: 변환 시 fit_transform()대신 fit_predict() 를 쓴다 1from sklearn.cluster import DBSCAN 12345# eps: The maximum distance between two samples for one to be considered as in the neighborhoood of the otherdbscan = DBSCAN(eps=0.7, min_samples=2)data_scaled = StandardScaler().fit_transform(df.loc[:, 'sepal length (cm)' : 'petal width (cm)'])dbscan_data = dbscan.fit_predict(data_scaled)dbscan_data array([ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64) 3-3. 실루엣 스코어 (군집화 평가) 클러스터링의 품질을 정량적으로 평가해 주는 지표 1: 클러스터링의 품질이 좋다 0: 클러스터링의 품질이 안좋다 (클러스터링의 의미 없음) 음수: 잘못 분류됨 1from sklearn.metrics import silhouette_samples, silhouette_score 123data_scaled = StandardScaler().fit_transform(df.loc[:, 'sepal length (cm)' : 'petal width (cm)'])score = silhouette_score(data_scaled, kmeans.labels_)score 0.45994823920518635 12samples = silhouette_samples(data_scaled, kmeans.labels_)samples[:5] array([0.73419485, 0.56827391, 0.67754724, 0.62050159, 0.72847412]) silhouette analysis 시각화 Document 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586def plot_silhouette(X, num_cluesters): for n_clusters in num_cluesters: # Create a subplot with 1 row and 2 columns fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) # The 1st subplot is the silhouette plot # The silhouette coefficient can range from -1, 1 but in this example all # lie within [-0.1, 1] ax1.set_xlim([-0.1, 1]) # The (n_clusters+1)*10 is for inserting blank space between silhouette # plots of individual clusters, to demarcate them clearly. ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10]) # Initialize the clusterer with n_clusters value and a random generator # seed of 10 for reproducibility. clusterer = KMeans(n_clusters=n_clusters, random_state=10) cluster_labels = clusterer.fit_predict(X) # The silhouette_score gives the average value for all the samples. # This gives a perspective into the density and separation of the formed # clusters silhouette_avg = silhouette_score(X, cluster_labels) print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg) # Compute the silhouette scores for each sample sample_silhouette_values = silhouette_samples(X, cluster_labels) y_lower = 10 for i in range(n_clusters): # Aggregate the silhouette scores for samples belonging to # cluster i, and sort them ith_cluster_silhouette_values = \\ sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i) / n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7) # Label the silhouette plots with their cluster numbers at the middle ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Compute the new y_lower for next plot y_lower = y_upper + 10 # 10 for the 0 samples ax1.set_title(\"The silhouette plot for the various clusters.\") ax1.set_xlabel(\"The silhouette coefficient values\") ax1.set_ylabel(\"Cluster label\") # The vertical line for average silhouette score of all the values ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\") ax1.set_yticks([]) # Clear the yaxis labels / ticks ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]) # 2nd Plot showing the actual clusters formed colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters) ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k') # Labeling the clusters centers = clusterer.cluster_centers_ # Draw white circles at cluster centers ax2.scatter(centers[:, 0], centers[:, 1], marker='o', c=\"white\", alpha=1, s=200, edgecolor='k') for i, c in enumerate(centers): ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k') ax2.set_title(\"The visualization of the clustered data.\") ax2.set_xlabel(\"Feature space for the 1st feature\") ax2.set_ylabel(\"Feature space for the 2nd feature\") plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \" \"with n_clusters = %d\" % n_clusters), fontsize=14, fontweight='bold') plt.show() 1plot_silhouette(data_scaled, [2, 3, 4, 5]) For n_clusters = 2 The average silhouette_score is : 0.5817500491982808 For n_clusters = 3 The average silhouette_score is : 0.45994823920518635 For n_clusters = 4 The average silhouette_score is : 0.4188923398171004 For n_clusters = 5 The average silhouette_score is : 0.34551099599809465 빨간 점선은 평균 실루엣 계수를 의미함 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - Machine Learning","slug":"【STUDY-Python】/Python-Machine-Learning","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-Machine-Learning/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"sklearn","slug":"sklearn","permalink":"https://hyemin-kim.github.io/tags/sklearn/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://hyemin-kim.github.io/tags/Machine-Learning/"},{"name":"비지도 학습","slug":"비지도-학습","permalink":"https://hyemin-kim.github.io/tags/%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5/"}]},{"title":"Python >> sklearn - (4) 앙상블 (Ensemble)","slug":"S-Python-sklearn4","date":"2020-08-04T11:40:35.000Z","updated":"2020-11-06T05:20:39.672Z","comments":true,"path":"2020/08/04/S-Python-sklearn4/","link":"","permalink":"https://hyemin-kim.github.io/2020/08/04/S-Python-sklearn4/","excerpt":"","text":"앙상블 (Ensemble) 0. 데이터 셋 0-1. 데이터 로드 0-2. 데이터프레임 만들기 1. Training set / Test set 나누기 2. 평가 지표 만들기 2-1. 평가 지표 2-2. 모델 성능 확인을 위한 함수 3. 단일 회귀 모델 (지난 시간) (1) Linear Regression (2) Ridge (3) LASSO (4) ElasticNet (5) With Standard Scaling (6) Polynomial Features 4. 앙상블 (Ensemble) 알고리즘 4-1. 보팅 (Voting) &gt;&gt; 회귀 (Regression) &gt;&gt; 분류 (Classification) 4-2. 배깅 (Bagging) &gt;&gt; Random Forest 4-3. 부스팅 (Boosting) 4-3-1. Gradient Boost 4-3-2. XGBoost 4-3-3. LightGBM 4-4. 스태킹 (Stacking) 4-5. Weighted Blending 4-6. 앙상블 모델 정리 5. Cross Validation 5-1. Cross Validation 소개 5-2. Hyper-parameter 튜닝 (1) RandomizedSearchCV (2) GridSerchCV 머신러닝 앙상블이란 여러 개의 머신러닝 모델을 이용해 최적의 답을 찾아내는 기법이다. (여러 모델을 이용하여 데이터를 학습하고, 모든 모델의 예측결과를 평균하여 예측) 앙상블 기법의 종류 보팅 (Voting): 투표를 통해 결과 도출 배깅 (Bagging): 샘플 중복 생성을 통해 결과 도출 부스팅 (Boosting): 이전 오차를 보완하면서 가중치 부여 스태킹 (Stacking): 여러 모델을 기반으로 예측된 결과를 통해 meta 모델이 다시 한번 예측 참고자료 (블로그) 보팅 (Voting) 배경 (Bagging) 부스팅 (Boosting) 0. 데이터 셋 12345import pandas as pdimport numpy as npfrom IPython.display import Imagenp.set_printoptions(suppress=True) # If True, print floating point numbers instead of scientific notation 1from sklearn.datasets import load_boston 0-1. 데이터 로드 1data = load_boston() 1print(data['DESCR']) .. _boston_dataset: Boston house prices dataset --------------------------- **Data Set Characteristics:** :Number of Instances: 506 :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. :Attribute Information (in order): - CRIM per capita crime rate by town - ZN proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS proportion of non-retail business acres per town - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX nitric oxides concentration (parts per 10 million) - RM average number of rooms per dwelling - AGE proportion of owner-occupied units built prior to 1940 - DIS weighted distances to five Boston employment centres - RAD index of accessibility to radial highways - TAX full-value property-tax rate per $10,000 - PTRATIO pupil-teacher ratio by town - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT % lower status of the population - MEDV Median value of owner-occupied homes in $1000's :Missing Attribute Values: None :Creator: Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/ ​ This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics &amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, 'Regression diagnostics ...', Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. .. topic:: References - Belsley, Kuh &amp; Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261. - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. ​ 0-2. 데이터프레임 만들기 12df = pd.DataFrame(data['data'], columns = data['feature_names'])df['MEDV'] = data['target'] 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 컬럼 소게 (feature 13 + target 1): CRIM: 범죄율 ZN: 25,000 square feet 당 주거용 토지의 비율 INDUS: 비소매(non-retail) 비즈니스 면적 비율 CHAS: 찰스 강 더미 변수 (통로가 하천을 향하면 1; 그렇지 않으면 0) NOX: 산화 질소 농도 (천만 분의 1) RM:주거 당 평균 객실 수 AGE: 1940 년 이전에 건축된 자가 소유 점유 비율 DIS: 5 개의 보스턴 고용 센터까지의 가중 거리 RAD: 고속도로 접근성 지수 TAX: 10,000 달러 당 전체 가치 재산 세율 PTRATIO 도시 별 학생-교사 비율 B: 1000 (Bk-0.63) ^ 2 여기서 Bk는 도시 별 검정 비율입니다. LSTAT: 인구의 낮은 지위 MEDV: 자가 주택의 중앙값 (1,000 달러 단위) 1. Training set / Test set 나누기 1from sklearn.model_selection import train_test_split 1x_train, x_test, y_train, y_test = train_test_split(df.drop('MEDV', 1), df['MEDV'], random_state=23) 1x_train.shape, y_train.shape ((379, 13), (379,)) 1x_test.shape, y_test.shape ((127, 13), (127,)) 2. 평가 지표 만들기 2-1. 평가 지표 (1) MAE (Mean Absolute Error) MAE (평균 절대 오차): 에측값과 실제값의 차이의 절대값에 대하여 평균을 낸 것 MAE=1n∑i=1n∣yi−yi^∣MAE = \\frac{1}{n} \\sum_{i=1}^n \\left\\vert y_i - \\widehat{y_i} \\right\\vert MAE=n1​i=1∑n​∣yi​−yi​​∣ (2) MSE (Mean Squared Error) MSE (평균 제곱 오차): 예측값과 실제값의 차이의 제곱에 대하여 평균을 낸 것 MSE=1n∑i=1n(yi−yi^)2MSE = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\widehat{y_i} \\right)^2 MSE=n1​i=1∑n​(yi​−yi​​)2 (3) RMSE (Root Mean Squared Error) RMSE (평균 제곱근 오차): 예측값과 실제값의 차이의 제곱에 대하여 평균을 낸 뒤 루트를 씌운 것 RMSE=1n∑i=1n(yi−yi^)2RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\widehat{y_i} \\right)^2} RMSE=n1​i=1∑n​(yi​−yi​​)2​ 2-2. 모델 성능 확인을 위한 함수 12# sklearn 평가지표 활용from sklearn.metrics import mean_absolute_error, mean_squared_error 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import matplotlib.pyplot as pltimport seaborn as snsmy_predictions = {}colors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown', 'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick', 'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive', 'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate', 'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray', 'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato' ]# prediction plotdef plot_predictions(name_, actual, pred): df = pd.DataFrame({'actual': y_test, 'prediction': pred}) df = df.sort_values(by='actual').reset_index(drop=True) plt.figure(figsize=(12, 9)) plt.scatter(df.index, df['prediction'], marker='x', color='r') plt.scatter(df.index, df['actual'], alpha=0.7, marker='o', color='black') plt.title(name_, fontsize=15) plt.legend(['prediction', 'actual'], fontsize=12) plt.show()# evaluation plotdef mse_eval(name_, actual, pred): global predictions global colors plot_predictions(name_, actual, pred) mse = mean_squared_error(actual, pred) my_predictions[name_] = mse y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=True) df = pd.DataFrame(y_value, columns=['model', 'mse']) print(df) min_ = df['mse'].min() - 10 max_ = df['mse'].max() + 10 length = len(df) plt.figure(figsize=(10, length)) ax = plt.subplot() ax.set_yticks(np.arange(len(df))) ax.set_yticklabels(df['model'], fontsize=15) bars = ax.barh(np.arange(len(df)), df['mse']) for i, v in enumerate(df['mse']): idx = np.random.choice(len(colors)) bars[i].set_color(colors[idx]) ax.text(v + 2, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold') plt.title('MSE Error', fontsize=18) plt.xlim(min_, max_) plt.show()# remove modeldef remove_model(name_): global my_predictions try: del my_predictions[name_] except KeyError: return False return True# coefficients visulizationdef plot_coef(columns, coef): coef_df = pd.DataFrame(list(zip(columns, coef))) coef_df.columns=['feature', 'coef'] coef_df = coef_df.sort_values('coef', ascending=False).reset_index(drop=True) fig, ax = plt.subplots(figsize=(9, 7)) ax.barh(np.arange(len(coef_df)), coef_df['coef']) idx = np.arange(len(coef_df)) ax.set_yticks(idx) ax.set_yticklabels(coef_df['feature']) fig.tight_layout() plt.show() 3. 단일 회귀 모델 (지난 시간) 1234567from sklearn.linear_model import LinearRegressionfrom sklearn.linear_model import Ridgefrom sklearn.linear_model import Lassofrom sklearn.linear_model import ElasticNetfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScalerfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import PolynomialFeatures (1) Linear Regression 1234linear_reg = LinearRegression(n_jobs=-1)linear_reg.fit(x_train, y_train)linear_pred = linear_reg.predict(x_test)mse_eval('LinearRegression', y_test, linear_pred) model mse 0 LinearRegression 22.770784 (2) Ridge 1234ridge = Ridge(alpha=1)ridge.fit(x_train, y_train)ridge_pred = ridge.predict(x_test)mse_eval('Ridge(alpha=1)', y_test, ridge_pred) model mse 0 LinearRegression 22.770784 1 Ridge(alpha=1) 22.690411 (3) LASSO 1234lasso = Lasso(alpha=0.01)lasso.fit(x_train, y_train)lasso_pred = lasso.predict(x_test)mse_eval('Lasso(alpha=0.01)', y_test, lasso_pred) model mse 0 LinearRegression 22.770784 1 Ridge(alpha=1) 22.690411 2 Lasso(alpha=0.01) 22.635614 (4) ElasticNet 1234elasticnet = ElasticNet(alpha=0.5, l1_ratio=0.2)elasticnet.fit(x_train, y_train)elas_pred = elasticnet.predict(x_test)mse_eval('ElasticNet(l1_ratio=0.2)', y_test, elas_pred) model mse 0 ElasticNet(l1_ratio=0.2) 24.481069 1 LinearRegression 22.770784 2 Ridge(alpha=1) 22.690411 3 Lasso(alpha=0.01) 22.635614 (5) With Standard Scaling 1234567standard_elasticnet = make_pipeline( StandardScaler(), ElasticNet(alpha=0.5, l1_ratio=0.2))elas_scaled_pred = standard_elasticnet.fit(x_train, y_train).predict(x_test)mse_eval('Standard ElasticNet', y_test, elas_scaled_pred) model mse 0 Standard ElasticNet 26.010756 1 ElasticNet(l1_ratio=0.2) 24.481069 2 LinearRegression 22.770784 3 Ridge(alpha=1) 22.690411 4 Lasso(alpha=0.01) 22.635614 (6) Polynomial Features 123456789# 2-Degree Polynomial Features + Standard Scalingpoly_elasticnet = make_pipeline( PolynomialFeatures(degree=2, include_bias=False), StandardScaler(), ElasticNet(alpha=0.5, l1_ratio=0.2))poly_pred = poly_elasticnet.fit(x_train, y_train).predict(x_test)mse_eval('Poly ElasticNet', y_test, poly_pred) model mse 0 Standard ElasticNet 26.010756 1 ElasticNet(l1_ratio=0.2) 24.481069 2 LinearRegression 22.770784 3 Ridge(alpha=1) 22.690411 4 Lasso(alpha=0.01) 22.635614 5 Poly ElasticNet 20.805986 4. 앙상블 (Ensemble) 알고리즘 [sklearn.ensemble] Document 앙상블 기법의 종류 보팅 (Voting): 투표를 통해 결과 도출 배깅 (Bagging): 샘플 중복 생성을 통해 결과 도출 부스팅 (Boosting): 이전 오차를 보완하면서 가중치 부여 스태킹 (Stacking): 여러 모델을 기반으로 예측된 결과를 통해 meta 모델이 다시 한번 예측 4-1. 보팅 (Voting) &gt;&gt; 회귀 (Regression) Voting은 단어 뜻 그대로 투표를 통해 최종 결과를 결정하는 방식이다. Voting과 Bagging은 모두 투표방식이지만, 다음과 같은 큰 차이점이 있다: Voting은 다른 알고리즘 model을 조합해서 사용함 Bagging은 같은 알고리즘 내에서 다른 sample 조합을 사용함 1from sklearn.ensemble import VotingRegressor 반드시, Tuple 형태로 모델을 정의해야 한다. 123456789# 보팅에 참여한 single models 지정single_models = [ ('linear_reg', linear_reg), ('ridge', ridge), ('lasso', lasso), ('elasticnet', elasticnet), ('standard_elasticnet', standard_elasticnet), ('poly_elasticnet', poly_elasticnet)] 12# voting regressor 만들기voting_regressor = VotingRegressor(single_models, n_jobs=-1) 1voting_regressor.fit(x_train, y_train) VotingRegressor(estimators=[('linear_reg', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)), ('ridge', Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='auto', tol=0.001)), ('lasso', Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, pr... interaction_only=False, order='C')), ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('elasticnet', ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.2, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False))], verbose=False))], n_jobs=-1, weights=None) 12voting_pred = voting_regressor.predict(x_test)mse_eval('Voting Ensemble', y_test, voting_pred) model mse 0 Standard ElasticNet 26.010756 1 ElasticNet(l1_ratio=0.2) 24.481069 2 LinearRegression 22.770784 3 Ridge(alpha=1) 22.690411 4 Lasso(alpha=0.01) 22.635614 5 Voting Ensemble 22.092158 6 Poly ElasticNet 20.805986 &gt;&gt; 분류 (Classification) 참고 자료 (Blog) 분류기 모델을 만들때, Voting 앙상블은 1가지의 중요한 parameter가 있다: voting = {‘hard’, ‘soft’} class를 0, 1로 분류 예측을 하는 이진 분류를 예로 들어 보자. (1) hard 로 설정한 경우 Hard Voting 방식에서는 결과 값에 대한 다수 class를 사용한다. 분류를 예측한 값이 1, 0, 0, 1, 1 이었다고 가정한다면 1이 3표, 0이 2표를 받았기 때문에 Hard Voting 방식에서는 1이 최종 값으로 예측을 하게 된다. (2) soft 로 설정한 경우 soft voting 방식은 각각의 확률의 평균 값을 계산한다음에 가장 확률이 높은 값으로 확정짓게 된다. 가령 class 0이 나올 확률이 (0.4, 0.9, 0.9, 0.4, 0.4)이었고, class 1이 나올 확률이 (0.6, 0.1, 0.1, 0.6, 0.6) 이었다면, class 0이 나올 최종 확률은 (0.4+0.9+0.9+0.4+0.4) / 5 = 0.44, class 1이 나올 최종 확률은 (0.6+0.1+0.1+0.6+0.6) / 5 = 0.4 가 되기 때문에 앞선 Hard Voting의 결과와는 다른 결과 값이 최종으로 선출되게 된다. 12from sklearn.ensemble import VotingClassifierfrom sklearn.linear_model import LogisticRegression, RidgeClassifier 1234models = [ ('Logit', LogisticRegression()), ('ridge', RidgeClassifier())] voting 옵션 지정 1vc = VotingClassifier(models, voting='soft') 4-2. 배깅 (Bagging) 참고 자료 (Blog) Bagging은 Bootstrap Aggregating의 줄임말이다. Bootstrap은 여러 개의 dataset을 중첩을 허용하게 하여 샘플링하여 분할하는 방식. 데이터 셋의 구성이 [1, 2, 3, 4, 5]로 되어 있다면, group 1 = [1, 2, 3] group 2 = [1, 3, 4] group 3 = [2, 3, 5] 1Image('https://teddylee777.github.io/images/2019-12-17/image-20191217015537872.png') Voting VS Bagging Voting은 여러 알고리즘의 조합에 대한 앙상블 Bagging은 하나의 단일 알고리즘에 대하여 여러 개의 샘플 조합으로 앙상블 대표적인 Bagging 앙상블 Random Forest Bagging &gt;&gt; Random Forest Decision Tree 기반 Bagging 앙상블 굉장히 인기있는 앙상블 모델 사용성이 쉽고, 성능도 우수함 [sklearn.ensemble.RandomForestRegressor] Document [sklearn.ensemble.RandomForestClassifier] Document 회귀 (Regression) Hyper-parameter의 default value로 모델 학습 1from sklearn.ensemble import RandomForestRegressor 12rfr = RandomForestRegressor(random_state=1)rfr.fit(x_train, y_train) RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=1, verbose=0, warm_start=False) 12rfr_pred = rfr.predict(x_test)mse_eval('RandomForest Ensemble', y_test, rfr_pred) model mse 0 Standard ElasticNet 26.010756 1 ElasticNet(l1_ratio=0.2) 24.481069 2 LinearRegression 22.770784 3 Ridge(alpha=1) 22.690411 4 Lasso(alpha=0.01) 22.635614 5 Voting Ensemble 22.092158 6 Poly ElasticNet 20.805986 7 RandomForest Ensemble 13.781191 주요 Hyper-parameter random_state: random seed 고정 값 n_jobs: CPU 사용 갯수 max_depth: 깊어질 수 있는 최대 깊이. 과대적합 방지용 n_estimators: 암상블하는 트리의 갯수 max_features: best split을 판단할 때 최대로 사용할 feature의 갯수 {‘auto’, ‘sqrt’, ‘log2’}. 과대적합 방지용 min_samples_split: 트리가 분할할 때 최소 샘플의 갯수. default=2. 과대적합 방지용 1Image('https://teddylee777.github.io/images/2020-01-09/decistion-tree.png', width=600) With Hyper-parameter Tuning 1234rfr_t = RandomForestRegressor(random_state=1, n_estimators=500, max_depth=7, max_features='sqrt')rfr_t.fit(x_train, y_train)rfr_t_pred = rfr_t.predict(x_test)mse_eval('RandomForest Ensemble w/ Tuning', y_test, rfr_t_pred) model mse 0 Standard ElasticNet 26.010756 1 ElasticNet(l1_ratio=0.2) 24.481069 2 LinearRegression 22.770784 3 Ridge(alpha=1) 22.690411 4 Lasso(alpha=0.01) 22.635614 5 Voting Ensemble 22.092158 6 Poly ElasticNet 20.805986 7 RandomForest Ensemble 13.781191 8 RandomForest Ensemble w/ Tuning 11.481491 4-3. 부스팅 (Boosting) 참고 자료 (Blog) 악한 학습기를 순차적으로 학습을 하되, 이전 학습에 대하여 잘멋 예측된 데이터에 가중치를 부여해 오차를 보완해 나가는 방식이다. 장점 성능이 매우 우수하다 (LightGBM, XGBoost) 단점 부스팅 알고리즘의 특성상 계속 약점(오분류/잔차)을 보완하려고 하기 때문에 잘못된 레이블링이나 아웃라이어에 필요 이상으로 민감할 수 있다 다른 앙상블 대비 학습 시간이 오래걸린다는 단점이 존재 1Image('https://keras.io/img/graph-kaggle-1.jpeg', width=800) 대표적인 Boosting 앙상블 AdaBoost GradientBoost LightGBM (LGBM) XGBoost 4-3-1. Gradient Boost 장점: 성능이 우수함 단점: 학습 시간이 너무 오래 걸린다 [sklearn.ensemble.GradientBoostingRegressor] Document 1from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier 123# default value로 학습gbr = GradientBoostingRegressor(random_state=1)gbr.fit(x_train, y_train) GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse', init=None, learning_rate=0.1, loss='ls', max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, presort='deprecated', random_state=1, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) 12gbr_pred = gbr.predict(x_test)mse_eval('GradientBoost Ensemble', y_test, gbr_pred) model mse 0 Standard ElasticNet 26.010756 1 ElasticNet(l1_ratio=0.2) 24.481069 2 LinearRegression 22.770784 3 Ridge(alpha=1) 22.690411 4 Lasso(alpha=0.01) 22.635614 5 Voting Ensemble 22.092158 6 Poly ElasticNet 20.805986 7 RandomForest Ensemble 13.781191 8 GradientBoost Ensemble 13.451877 9 RandomForest Ensemble w/ Tuning 11.481491 주요 Hyper-parameter random_state: random seed 고정 값 n_jobs: CPU 사용 갯수 learning rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. default=0.1 (n_estimators와 같이 튜닝해야 함) n_estimators: 부스팅 스테이지 수. default=100 (Random Forest 트리의 갯수 설정과 비슷) subsample: 샘플 사용 비율 (max_features와 비슷). 과대적합 방지용 min_samples_split: 노드 분할시 최소 샘플의 갯수. default=2. 과대적합 방지용 There’s a trade-off between learning_rate and n_estimators. 둘의 곱을 유지하는 것이 좋다 123456# with hyper-parameter tuning# learning_rate=0.01 (without tuning n_estimators together)gbr_t = GradientBoostingRegressor(random_state=1, learning_rate=0.01)gbr_t.fit(x_train, y_train)gbr_t_pred = gbr_t.predict(x_test)mse_eval('GradientBoost Ensemble w/ tuning (lr=0.01)', y_test, gbr_t_pred) model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 RandomForest Ensemble 13.781191 9 GradientBoost Ensemble 13.451877 10 RandomForest Ensemble w/ Tuning 11.481491 12345# tuning: learning_rate=0.01, n_estimators=1000gbr_t2 = GradientBoostingRegressor(random_state=1, learning_rate=0.01, n_estimators=1000)gbr_t2.fit(x_train, y_train)gbr_t2_pred = gbr_t2.predict(x_test)mse_eval('GradientBoost Ensemble w/ tuning (lr=0.01, est=1000)', y_test, gbr_t2_pred) model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 RandomForest Ensemble 13.781191 9 GradientBoost Ensemble 13.451877 10 GradientBoost Ensemble w/ tuning (lr=0.01, est... 13.002472 11 RandomForest Ensemble w/ Tuning 11.481491 12345# tuning: learning_rate=0.01, n_estimators=1000, subsample=0.8gbr_t3 = GradientBoostingRegressor(random_state=42, learning_rate=0.01, n_estimators=1000, subsample=0.7)gbr_t3.fit(x_train, y_train)gbr_t3_pred = gbr_t3.predict(x_test)mse_eval('GradientBoost Ensemble w/ tuning (lr=0.01, est=1000, subsample=0.7)', y_test, gbr_t3_pred) model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 RandomForest Ensemble 13.781191 9 GradientBoost Ensemble 13.451877 10 GradientBoost Ensemble w/ tuning (lr=0.01, est... 13.002472 11 GradientBoost Ensemble w/ tuning (lr=0.01, est... 12.607717 12 RandomForest Ensemble w/ Tuning 11.481491 4-3-2. XGBoost eXtreme Gradient Boosting [XGBoost] Document 주요 특징 scikit-learn 패키지 아님 성능이 우수함 GBM보다는 빠르고 성능도 향상됨 여전히 학습 속도가 느림 1pip install xgboost Requirement already satisfied: xgboost in d:\\anaconda\\lib\\site-packages (1.1.1) Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.4.1) Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.18.1) Note: you may need to restart the kernel to use updated packages. 1from xgboost import XGBRegressor, XGBClassifier 123# default value로 학습xgb = XGBRegressor(random_state=1)xgb.fit(x_train, y_train) XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, objective='reg:squarederror', random_state=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) 12xgb_pred = xgb.predict(x_test)mse_eval('XGBoost', y_test, xgb_pred) model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 XGBoost 13.841454 9 RandomForest Ensemble 13.781191 10 GradientBoost Ensemble 13.451877 11 GradientBoost Ensemble w/ tuning (lr=0.01, est... 13.002472 12 GradientBoost Ensemble w/ tuning (lr=0.01, est... 12.607717 13 RandomForest Ensemble w/ Tuning 11.481491 주요 Hyper-parameter random_state: random seed 고정 값 n_jobs: CPU 사용 갯수 learning_rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1 n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100 max_depth: 트리의 깊이. 과대적합 방지용. default=3. subsample: 샘플 사용 비율. 과대적합 방지용. default=1.0 max_features: 최대로 사용할 feature의 비율. 과대적합 방지용. default=1.0 12345# with hyeper-parameter tuningxgb_t = XGBRegressor(random_state=1, learning_rate=0.01, n_estimators=1000, subsample=0.7, max_features=0.8, max_depth=7)xgb_t.fit(x_train, y_train)xgb_t_pred = xgb_t.predict(x_test)mse_eval('XGBoost w/ Tuning', y_test, xgb_t_pred) [16:55:00] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: Parameters: { max_features } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. ​ ​ model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 XGBoost 13.841454 9 RandomForest Ensemble 13.781191 10 GradientBoost Ensemble 13.451877 11 GradientBoost Ensemble w/ tuning (lr=0.01, est... 13.002472 12 GradientBoost Ensemble w/ tuning (lr=0.01, est... 12.607717 13 XGBoost w/ Tuning 11.987602 14 RandomForest Ensemble w/ Tuning 11.481491 4-3-3. LightGBM [LightGBM] Document 주요 특징 scikit-learn 패키지가 아님 성능이 우수함 속도도 매우 빠름 1pip install lightgbm Requirement already satisfied: lightgbm in d:\\anaconda\\lib\\site-packages (2.3.1) Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.4.1) Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.18.1) Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (from lightgbm) (0.22.1) Requirement already satisfied: joblib&gt;=0.11 in d:\\anaconda\\lib\\site-packages (from scikit-learn-&gt;lightgbm) (0.14.1) Note: you may need to restart the kernel to use updated packages. 1from lightgbm import LGBMRegressor, LGBMClassifier 123# default value 로 학습lgbm = LGBMRegressor(random_state=1)lgbm.fit(x_train, y_train) LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0, importance_type='split', learning_rate=0.1, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=None, random_state=1, reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0, subsample_for_bin=200000, subsample_freq=0) 12lgbm_pred = lgbm.predict(x_test)mse_eval('LightGBM', y_test, lgbm_pred) model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 XGBoost 13.841454 9 RandomForest Ensemble 13.781191 10 GradientBoost Ensemble 13.451877 11 GradientBoost Ensemble w/ tuning (lr=0.01, est... 13.002472 12 LightGBM 12.882170 13 GradientBoost Ensemble w/ tuning (lr=0.01, est... 12.607717 14 XGBoost w/ Tuning 11.987602 15 RandomForest Ensemble w/ Tuning 11.481491 주요 Hyperparameter random_state: random seed 고정 값 n_jobs: CPU 사용 갯수 learning_rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1 n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100 max_depth: 트리의 깊이. 과대적합 방지용. default=3. colsample_bytree: 샘플 사용 비율 (max_features와 비슷한 개념). 과대적합 방지용. default=1.0 12345# with hyper-parameter tuninglgbm_t = LGBMRegressor(random_state=1, learning_rate=0.01, n_estimators=2000, colsample_bytree=0.9, subsample=0.7, max_depth=5)lgbm_t.fit(x_train, y_train)lgbm_t_pred = lgbm_t.predict(x_test)mse_eval('LightGBM w/ Tuning', y_test, lgbm_t_pred) model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 XGBoost 13.841454 9 RandomForest Ensemble 13.781191 10 GradientBoost Ensemble 13.451877 11 GradientBoost Ensemble w/ tuning (lr=0.01, est... 13.002472 12 LightGBM 12.882170 13 GradientBoost Ensemble w/ tuning (lr=0.01, est... 12.607717 14 LightGBM w/ Tuning 12.200040 15 XGBoost w/ Tuning 11.987602 16 RandomForest Ensemble w/ Tuning 11.481491 4-4. 스태킹 (Stacking) 개별 모델이 예측한 데이터를 기반으로 final_estimators 종합하여 예측을 수행 성능을 극으로 끌오올릴 때 활용하기도 함 과대적합을 유발할 수 있다. (특히, 데이터셋이 적은 경우) [sklearn.ensemble.StackingRegressor] Document 1from sklearn.ensemble import StackingRegressor 12345stack_models = [ ('elasticnet', poly_elasticnet), ('randomforest', rfr_t), ('lgbm', lgbm_t)] 1234stack_reg = StackingRegressor(stack_models, final_estimator=xgb, n_jobs=-1)stack_reg.fit(x_train, y_train)stack_pred = stack_reg.predict(x_test)mse_eval('Stacking Ensemble', y_test, stack_pred) model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 Stacking Ensemble 16.906090 9 XGBoost 13.841454 10 RandomForest Ensemble 13.781191 11 GradientBoost Ensemble 13.451877 12 GradientBoost Ensemble w/ tuning (lr=0.01, est... 13.002472 13 LightGBM 12.882170 14 GradientBoost Ensemble w/ tuning (lr=0.01, est... 12.607717 15 LightGBM w/ Tuning 12.200040 16 XGBoost w/ Tuning 11.987602 17 RandomForest Ensemble w/ Tuning 11.481491 4-5. Weighted Blending 각 모델의 예측값에 대하여 weight를 곱해혀 최종 output 산출 모델에 대한 가중치를 조절하여, 최종 output을 산출함 가중치의 합은 1.0이 되도록 설정 12345final_outputs = { 'randomforest': rfr_t_pred, 'xgboost': xgb_t_pred, 'lgbm': lgbm_t_pred} 1234final_prediction=\\final_outputs['randomforest'] * 0.5\\+final_outputs['xgboost'] * 0.3\\+final_outputs['lgbm'] * 0.2\\ 1mse_eval('Weighted Blending', y_test, final_prediction) model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 Stacking Ensemble 16.906090 9 XGBoost 13.841454 10 RandomForest Ensemble 13.781191 11 GradientBoost Ensemble 13.451877 12 GradientBoost Ensemble w/ tuning (lr=0.01, est... 13.002472 13 LightGBM 12.882170 14 GradientBoost Ensemble w/ tuning (lr=0.01, est... 12.607717 15 LightGBM w/ Tuning 12.200040 16 XGBoost w/ Tuning 11.987602 17 RandomForest Ensemble w/ Tuning 11.481491 18 Weighted Blending 10.585610 4-6. 앙상블 모델 정리 앙상블은 대체적으로 단일 모델 대비 성능이 좋다 앙상블을 앙상블하는 기법인 Stacking과 Weighted Blending도 참고해 볼만 하다 앙상블 모델은 적절한 Hyper-parameter Tuning이 중요하다 앙상블 모델은 대체적으로 학습시간이 더 오래 걸린다 따라서, 모델 튜닝을 하는 데에 시간이 오래 소유된다 5. Cross Validation 5-1. Cross Validation 소개 Cross Validation 알아보기 참고 자료: 딥러닝 모델의 K-겹 교차검증 (K-fold Cross Validation) 전에 진행했던 실습에서도 보였듯이, Hyper-parameter의 값은 모델의 성능을 좌우한다. 그러므로 예측 모델의 성능을 높이기 위해, Hyper-parameter Tuning이 매우 중요하다. 이를 실현하기 위해 저희는 Training data을 다시 Training set과 Validation set으로 나눈다. Trainging set에서 Hyper-parameter값을 바뀌가면서 모델 학습하고, Validation set에서 모델의 성능을 평가하여, 모델 성능을 제일 높일 수 있는 Hyper-parameter값을 선택한다 하지만, 데이터의 일부만 Validation set으로 사용해 모델 성능을 평가하게 되면, 훈련된 모델이 Test set에 대한 성능 평가의 신뢰성이 떨어질 수 있다. 이를 방지하기 위해 **K-fold Cross Validation (K-겹 교차검증)**을 많이 활용한다 K겹 교차 검증은 모든 데이터가 최소 한 번은 validation set으로 쓰이도록 한다 (아래의 그림을 보면, 데이터를 5개로 쪼개 매번 validation set을 바꿔나가는 것을 볼 수 있다) K번 검증을 통해 구한 K 개의 평가지표 값을 평균 내어 모델 성능을 평가한다 [예시] Estimation 1일 때, Training set: [2, 3, 4, 5] / Validation set: [1] Estimation 2일 때, Training set: [1, 3, 4, 5] / Validation set: [2] 1from sklearn.model_selection import KFold 12n_splits = 5kfold = KFold(n_splits=n_splits, random_state=1, shuffle = True) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 12X = np.array(df.drop('MEDV', 1))Y = np.array(df['MEDV']) 1lgbm_fold = LGBMRegressor(random_state=1) 123456789101112i = 1total_error = 0for train_index, test_index in kfold.split(X): x_train_fold, x_test_fold = X[train_index], X[test_index] y_train_fold, y_test_fold = Y[train_index], Y[test_index] lgbm_fold_pred = lgbm_fold.fit(x_train_fold, y_train_fold).predict(x_test_fold) error = mean_squared_error(y_test_fold, lgbm_fold_pred) print('Fold = {}, prediction score = {:.2f}'.format(i, error)) total_error += error i+=1print('---'*10)print('Average Error: %s' % (total_error / n_splits)) Fold = 1, prediction score = 9.76 Fold = 2, prediction score = 20.58 Fold = 3, prediction score = 6.95 Fold = 4, prediction score = 12.18 Fold = 5, prediction score = 10.87 ------------------------------ Average Error: 12.06743160435072 5-2. Hyper-parameter 튜닝 Hyper-parameter 튜닝 시 경우의 수가 너무 많으므로 우리는 자동화할 틸요가 있다 sklearn 패키지에서 자주 사용되는 hyper-parameter 튜닝을 돕는 클래스는 다음 2가지가 있다: RandomizedSerchCV GridSerchCV 적용하는 방법 사용할 Search 방법을 선택한다 hyper-parameter 도메인(값의 범위)을 설정한다 (max_depth, n_estimators… 등등) 학습을 시킨 후, 기다린다 도출된 결과 값을 모델에 적용하고 성능을 비교한다 (1) RandomizedSearchCV 모든 매개 변수 값이 시도되는 것이 아니라 지정된 분포에서 고정 된 수의 매개 변수 설정이 샘플링된다. 시도 된 매개 변수 설정의 수는 n_iter에 의해 제공됨. 주요 Hyper-parameter (LGBM) random_state: random seed 고정 값 n_jobs: CPU 사용 갯수 learning_rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 적절한 값을 찾아야함. n_estimators와 같이 튜닝. default=0.1 n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100 max_depth: 트리의 깊이. 과대적합 방지용. default=3. colsample_bytree: 샘플 사용 비율 (max_features와 비슷한 개념). 과대적합 방지용. default=1.0 1234567params = { 'learning_rate': [0.005, 0.01, 0.03, 0.05], 'n_estimators': [500, 1000, 2000, 3000], 'max_depth': [3, 5, 7], 'colsample_bytree': [0.8, 0.9, 1.0], 'subsample': [0.7, 0.8, 0.9, 1.0],} 1from sklearn.model_selection import RandomizedSearchCV 조절하여, 총 몇 회의 시도를 진행할 것인자 정의한다 12345(회수가 늘어나면, 더 좋은 parameter를 찾을 확률은 올라가지만, 그만큼 시간이 오래걸린다.)```pythonrcv_lgbm = RandomizedSearchCV(LGBMRegressor(), params, random_state=1, cv=5, n_iter=100, scoring='neg_mean_squared_error') 1rcv_lgbm.fit(x_train, y_train) RandomizedSearchCV(cv=5, error_score=nan, estimator=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0, importance_type='split', learning_rate=0.1, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0, silen... subsample_freq=0), iid='deprecated', n_iter=100, n_jobs=None, param_distributions={'colsample_bytree': [0.8, 0.9, 1.0], 'learning_rate': [0.005, 0.01, 0.03, 0.05], 'max_depth': [3, 5, 7], 'n_estimators': [500, 1000, 2000, 3000], 'subsample': [0.7, 0.8, 0.9, 1.0]}, pre_dispatch='2*n_jobs', random_state=1, refit=True, return_train_score=False, scoring='neg_mean_squared_error', verbose=0) 1rcv_lgbm.best_score_ -11.132039701508374 1rcv_lgbm.best_params_ {'subsample': 0.8, 'n_estimators': 1000, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.9} 123lgbm_best = LGBMRegressor(learning_rate=0.05, n_estimators=1000, subsample=0.8, max_depth=3, colsample_bytree=0.9)lgbm_best_pred = lgbm_best.fit(x_train, y_train).predict(x_test)mse_eval('RandomSearch LGBM', y_test, lgbm_best_pred) model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 Stacking Ensemble 16.906090 9 XGBoost 13.841454 10 RandomForest Ensemble 13.781191 11 GradientBoost Ensemble 13.451877 12 GradientBoost Ensemble w/ tuning (lr=0.01, est... 13.002472 13 LightGBM 12.882170 14 RandomSearch LGBM 12.661917 15 GradientBoost Ensemble w/ tuning (lr=0.01, est... 12.607717 16 LightGBM w/ Tuning 12.200040 17 XGBoost w/ Tuning 11.987602 18 RandomForest Ensemble w/ Tuning 11.481491 19 Weighted Blending 10.585610 (2) GridSerchCV 모든 매개 변수 값에 대하여 완전 탐색을 시도한다 따라서, 최적화할 parameter가 많다면, 시간이 매우 오래걸린다 1234567params = { 'learning_rate': [0.04, 0.05, 0.06], 'n_estimators': [800, 1000, 1200], 'max_depth': [3, 4, 5], 'colsample_bytree': [0.8, 0.85, 0.9], 'subsample': [0.8, 0.85, 0.9],} 1from sklearn.model_selection import GridSearchCV 1grid_search = GridSearchCV(LGBMRegressor(), params, cv=5, n_jobs=-1, scoring='neg_mean_squared_error') 1grid_search.fit(x_train, y_train) GridSearchCV(cv=5, error_score=nan, estimator=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0, importance_type='split', learning_rate=0.1, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0, subsample_for_bin=200000, subsample_freq=0), iid='deprecated', n_jobs=-1, param_grid={'colsample_bytree': [0.8, 0.85, 0.9], 'learning_rate': [0.04, 0.05, 0.06], 'max_depth': [3, 4, 5], 'n_estimators': [800, 1000, 1200], 'subsample': [0.8, 0.85, 0.9]}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring='neg_mean_squared_error', verbose=0) 1grid_search.best_score_ -11.10039780445118 1grid_search.best_params_ {'colsample_bytree': 0.9, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 800, 'subsample': 0.8} 123lgbm_best = LGBMRegressor(learning_rate=0.05, n_estimators=800, subsample=0.8, max_depth=3, colsample_bytree=0.9)lgbm_best_pred = lgbm_best.fit(x_train, y_train).predict(x_test)mse_eval('GridSearch LGBM', y_test, lgbm_best_pred) model mse 0 Standard ElasticNet 26.010756 1 GradientBoost Ensemble w/ tuning (lr=0.01) 24.599441 2 ElasticNet(l1_ratio=0.2) 24.481069 3 LinearRegression 22.770784 4 Ridge(alpha=1) 22.690411 5 Lasso(alpha=0.01) 22.635614 6 Voting Ensemble 22.092158 7 Poly ElasticNet 20.805986 8 Stacking Ensemble 16.906090 9 XGBoost 13.841454 10 RandomForest Ensemble 13.781191 11 GradientBoost Ensemble 13.451877 12 GradientBoost Ensemble w/ tuning (lr=0.01, est... 13.002472 13 LightGBM 12.882170 14 GridSearch LGBM 12.794172 15 RandomSearch LGBM 12.661917 16 GradientBoost Ensemble w/ tuning (lr=0.01, est... 12.607717 17 LightGBM w/ Tuning 12.200040 18 XGBoost w/ Tuning 11.987602 19 RandomForest Ensemble w/ Tuning 11.481491 20 Weighted Blending 10.585610 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - Machine Learning","slug":"【STUDY-Python】/Python-Machine-Learning","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-Machine-Learning/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"sklearn","slug":"sklearn","permalink":"https://hyemin-kim.github.io/tags/sklearn/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://hyemin-kim.github.io/tags/Machine-Learning/"},{"name":"앙상블","slug":"앙상블","permalink":"https://hyemin-kim.github.io/tags/%EC%95%99%EC%83%81%EB%B8%94/"}]},{"title":"Python >> sklearn - (3) 회귀 (Regression)","slug":"S-Python-sklearn3","date":"2020-07-29T09:53:05.000Z","updated":"2020-11-06T05:20:36.306Z","comments":true,"path":"2020/07/29/S-Python-sklearn3/","link":"","permalink":"https://hyemin-kim.github.io/2020/07/29/S-Python-sklearn3/","excerpt":"","text":"회귀 (Regression) 예측 0. 데이터 셋 0-1. 데이터 로드 0-2. 데이터프레임 만들기 1. Training set / Test set 나누기 2. 평가 지표 만들기 2-1. 평가 지표 계산식 2-2. 코딩으로 평가 지표 만들어 보기 2-3. sklearn의 평가 지표 활용하기 2-4. 모델 성능 확인을 위한 함수 3. 회귀 알고리즘 3-1. Linear Regression 3-2. Ridge &amp; LASSO &amp; ElasticNet (1) 개념 (2) 실습 4. Scaling 4-1. Scaler 소개 4-2. Scaling 후 모델 학습 – 파이프라인 활용 5. Polynomial Features [Supervised Learning] Document 특성: 수치형 값을 예측 (Y의 값이 연속형 수치로 표현) 예시: 주택 가격 예측 매출앵 예측 0. 데이터 셋 1234import pandas as pdimport numpy as npnp.set_printoptions(suppress=True) # If True, print floating point numbers instead of scientific notation 1from sklearn.datasets import load_boston [Boston Dataset] 0-1. 데이터 로드 1data = load_boston() 1print(data['DESCR']) # data description .. _boston_dataset: Boston house prices dataset --------------------------- **Data Set Characteristics:** :Number of Instances: 506 :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. :Attribute Information (in order): - CRIM per capita crime rate by town - ZN proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS proportion of non-retail business acres per town - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX nitric oxides concentration (parts per 10 million) - RM average number of rooms per dwelling - AGE proportion of owner-occupied units built prior to 1940 - DIS weighted distances to five Boston employment centres - RAD index of accessibility to radial highways - TAX full-value property-tax rate per $10,000 - PTRATIO pupil-teacher ratio by town - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT % lower status of the population - MEDV Median value of owner-occupied homes in $1000's :Missing Attribute Values: None :Creator: Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/ ​ This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics &amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, 'Regression diagnostics ...', Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. .. topic:: References - Belsley, Kuh &amp; Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261. - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. ​ 0-2. 데이터프레임 만들기 1234567# step 1. features (X)# data['data'] - feature data; data['feature_names'] - feature column namesdf = pd.DataFrame(data['data'], columns = data['feature_names'])# step 2. target (y) 추가 df['MEDV'] = data['target'] 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 컬럼 소게 (feature 13 + target 1): CRIM: 범죄율 ZN: 25,000 square feet 당 주거용 토지의 비율 INDUS: 비소매(non-retail) 비즈니스 면적 비율 CHAS: 찰스 강 더미 변수 (통로가 하천을 향하면 1; 그렇지 않으면 0) NOX: 산화 질소 농도 (천만 분의 1) RM:주거 당 평균 객실 수 AGE: 1940 년 이전에 건축된 자가 소유 점유 비율 DIS: 5 개의 보스턴 고용 센터까지의 가중 거리 RAD: 고속도로 접근성 지수 TAX: 10,000 달러 당 전체 가치 재산 세율 PTRATIO 도시 별 학생-교사 비율 B: 1000 (Bk-0.63) ^ 2 여기서 Bk는 도시 별 검정 비율입니다. LSTAT: 인구의 낮은 지위 MEDV: 자가 주택의 중앙값 (1,000 달러 단위) 1. Training set / Test set 나누기 1from sklearn.model_selection import train_test_split 1x_train, x_test, y_train, y_test = train_test_split(df.drop('MEDV', 1), df['MEDV'], random_state=23) 1x_train.shape, y_train.shape ((379, 13), (379,)) 1x_test.shape, y_test.shape ((127, 13), (127,)) 1x_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 112 0.12329 0.0 10.01 0.0 0.547 5.913 92.9 2.3534 6.0 432.0 17.8 394.95 16.21 301 0.03537 34.0 6.09 0.0 0.433 6.590 40.4 5.4917 7.0 329.0 16.1 395.75 9.50 401 14.23620 0.0 18.10 0.0 0.693 6.343 100.0 1.5741 24.0 666.0 20.2 396.90 20.32 177 0.05425 0.0 4.05 0.0 0.510 6.315 73.4 3.3175 5.0 296.0 16.6 395.60 6.29 69 0.12816 12.5 6.07 0.0 0.409 5.885 33.0 6.4980 4.0 345.0 18.9 396.90 8.79 1y_train.head() 112 18.8 301 22.0 401 7.2 177 24.6 69 20.9 Name: MEDV, dtype: float64 2. 평가 지표 만들기 2-1. 평가 지표 계산식 (1) MAE (Mean Absolute Error) MAE (평균 절대 오차): 에측값과 실제값의 차이의 절대값에 대하여 평균을 낸 것 MAE=1n∑i=1n∣yi−yi^∣MAE = \\frac{1}{n} \\sum_{i=1}^n \\left\\vert y_i - \\widehat{y_i} \\right\\vert MAE=n1​i=1∑n​∣yi​−yi​​∣ (2) MSE (Mean Squared Error) MSE (평균 제곱 오차): 예측값과 실제값의 차이의 제곱에 대하여 평균을 낸 것 MSE=1n∑i=1n(yi−yi^)2MSE = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\widehat{y_i} \\right)^2 MSE=n1​i=1∑n​(yi​−yi​​)2 (3) RMSE (Root Mean Squared Error) RMSE (평균 제곱근 오차): 예측값과 실제값의 차이의 제곱에 대하여 평균을 낸 뒤 루트를 씌운 것 RMSE=1n∑i=1n(yi−yi^)2RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\widehat{y_i} \\right)^2} RMSE=n1​i=1∑n​(yi​−yi​​)2​ 2-2. 코딩으로 평가 지표 만들어 보기 1import numpy as np 12actual = np.array([1, 2, 3])pred = np.array([3, 4, 5]) 12345# MAEdef my_mae(actual, pred): return np.abs(actual - pred).mean()my_mae(actual, pred) 2.0 12345# MSEdef my_mse(actual, pred): return ((actual - pred)**2).mean()my_mse(actual, pred) 4.0 12345# RMSEdef my_rmse(actual, pred): return np.sqrt(my_mse(actual, pred))my_rmse(actual, pred) 2.0 2-3. sklearn의 평가 지표 활용하기 1from sklearn.metrics import mean_absolute_error, mean_squared_error [sklearn.metrics.mean_absolute_error] [sklearn.metrics.mean_squared_error] 12# MAE (my_mae VS sklearn_mae)my_mae(actual, pred), mean_absolute_error(actual, pred) (2.0, 2.0) 12# MSE (my_mse VS sklearn_mse)my_mse(actual, pred), mean_squared_error(actual, pred) (4.0, 4.0) 2-4. 모델 성능 확인을 위한 함수 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import matplotlib.pyplot as pltimport seaborn as snsmy_predictions = {}colors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown', 'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick', 'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive', 'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate', 'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray', 'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato' ]# prediction plotdef plot_predictions(name_, actual, pred): df = pd.DataFrame({'actual': y_test, 'prediction': pred}) df = df.sort_values(by='actual').reset_index(drop=True) plt.figure(figsize=(12, 9)) plt.scatter(df.index, df['prediction'], marker='x', color='r') plt.scatter(df.index, df['actual'], alpha=0.7, marker='o', color='black') plt.title(name_, fontsize=15) plt.legend(['prediction', 'actual'], fontsize=12) plt.show()# evaluation plotdef mse_eval(name_, actual, pred): global predictions global colors plot_predictions(name_, actual, pred) mse = mean_squared_error(actual, pred) my_predictions[name_] = mse y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=True) df = pd.DataFrame(y_value, columns=['model', 'mse']) print(df) min_ = df['mse'].min() - 10 max_ = df['mse'].max() + 10 length = len(df) plt.figure(figsize=(10, length)) ax = plt.subplot() ax.set_yticks(np.arange(len(df))) ax.set_yticklabels(df['model'], fontsize=15) bars = ax.barh(np.arange(len(df)), df['mse']) for i, v in enumerate(df['mse']): idx = np.random.choice(len(colors)) bars[i].set_color(colors[idx]) ax.text(v + 2, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold') plt.title('MSE Error', fontsize=18) plt.xlim(min_, max_) plt.show()# remove modeldef remove_model(name_): global my_predictions try: del my_predictions[name_] except KeyError: return False return True 3. 회귀 알고리즘 3-1. Linear Regression [sklearn.linear_model.LinearRegression] Document 1from sklearn.linear_model import LinearRegression 123model = LinearRegression(n_jobs=-1) # n_jobs: CPU코어의 사용model.fit(x_train, y_train)pred = model.predict(x_test) 1mse_eval('LinearRegression', y_test, pred) model mse 0 LinearRegression 22.770784 3-2. Ridge &amp; LASSO &amp; ElasticNet (1) 개념 참고 규제(Regularization): 학습이 과적합 되는 것을 방지하고자 일종의 penalty를 부여하는 것. [원리] penalty를 부여하여 가중치(β\\betaβ)를 축소함으로써 학습 모델의 예측 variance를 감소 시키는 것 &gt;&gt; L2 규제 &amp; Ridge (릿지) L2 규제 (L2 Regularization): 각 가중치 제곱의 합에 규제 강도 (Regularization Strength) λ\\lambdaλ 를 곱한다 L2&nbsp;규제=λ∑j=1pβj2=λ&nbsp;∥β∥22L2 \\ 규제 = \\lambda \\sum_{j=1}^p \\beta_j^2 = \\lambda\\ \\lVert \\beta \\rVert_2^2 L2&nbsp;규제=λj=1∑p​βj2​=λ&nbsp;∥β∥22​ l2&nbsp;norm:∥β∥2=∑j=1pβj2l_2 \\ norm: \\lVert \\beta \\rVert_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2} l2​&nbsp;norm:∥β∥2​=j=1∑p​βj2​​ Ridge: Loss Function에 L2 규제를 더한 값을 최소화 시키는 것 min⁡βj&nbsp;[∑i=1n(yi−β0−∑j=1pβjxij)+λ&nbsp;∑j=1pβj2]=min⁡βj&nbsp;[RSS+λ&nbsp;∑j=1pβj2]\\min_{\\beta_j} \\ \\left[ \\sum_{i=1}^n \\left( y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij} \\right) + \\lambda\\ \\sum_{j=1}^p\\beta_j^2 \\right]= \\min_{\\beta_j} \\ \\left[ RSS + \\lambda\\ \\sum_{j=1}^p\\beta_j^2 \\right] βj​min​&nbsp;[i=1∑n​(yi​−β0​−j=1∑p​βj​xij​)+λ&nbsp;j=1∑p​βj2​]=βj​min​&nbsp;[RSS+λ&nbsp;j=1∑p​βj2​] λ\\lambdaλ 를 크게 하면 가중치(β\\betaβ) 가 더 많이 감소되고(규제를 중요시 함), λ\\lambdaλ 를 작게 하면 가중치(β\\betaβ) 가 증가한다(규제를 중요시하지 않음) &gt;&gt; L1 규제 &amp; LASSO (라쏘) L1 규제 (L1 Regularization): 각 가중치 절대값의 합에 규제 강도 (Regularization Strength) λ\\lambdaλ 를 곱한다 L1&nbsp;규제=λ∑j=1p∣βj∣=λ&nbsp;∥β∥1L1\\ 규제 = \\lambda \\sum_{j=1}^p \\left| \\beta_j \\right| = \\lambda \\ \\lVert \\beta \\rVert_1 L1&nbsp;규제=λj=1∑p​∣βj​∣=λ&nbsp;∥β∥1​ l1&nbsp;norm:∥β∥1=∑j=1p∣βj∣l1\\ norm: \\lVert \\beta \\rVert_1 = \\sum_{j=1}^p \\left| \\beta_j \\right| l1&nbsp;norm:∥β∥1​=j=1∑p​∣βj​∣ LASSO: Loss Function에 L1 규제를 더한 값을 최소화 시키는 것 min⁡βj&nbsp;[∑i=1n(yi−β0−∑j=1pβjxij)+λ∑j=1p∣βj∣]=min⁡βj&nbsp;[RSS+λ∑j=1p∣βj∣]\\min_{\\beta_j} \\ \\left[ \\sum_{i=1}^n \\left( y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij} \\right) + \\lambda \\sum_{j=1}^p \\left| \\beta_j \\right| \\right]= \\min_{\\beta_j} \\ \\left[ RSS + \\lambda \\sum_{j=1}^p \\left| \\beta_j \\right| \\right] βj​min​&nbsp;[i=1∑n​(yi​−β0​−j=1∑p​βj​xij​)+λj=1∑p​∣βj​∣]=βj​min​&nbsp;[RSS+λj=1∑p​∣βj​∣] 어떤 가중치(β\\betaβ) 는 실제로 0이 된다. 즉, 모델에서 완전히 제외되는 특성이 생기는 것이다 &gt;&gt; ElasticNet l1_ratio (default=0.5) l1_ratio = 0 (L2 규제만 사용) l1_ratio = 1 (L1 규제만 사용) 0 &lt; l1_ratio &lt;1 (L1 and L2 규제 혼합사용) (2) 실습 &gt;&gt; Ridge [Document] 1from sklearn.linear_model import Ridge 예측 결과 확인 123456789# lambda (규제강도) 범위 설정alphas = [100, 10, 1, 0.1, 0.01, 0.001]# 모델 학습for alpha in alphas: ridge = Ridge(alpha = alpha) ridge.fit(x_train, y_train) ridge_pred = ridge.predict(x_test) mse_eval('Ridge(alpha={})'.format(alpha), y_test, ridge_pred) model mse 0 Ridge(alpha=100) 23.487453 1 LinearRegression 22.770784 model mse 0 Ridge(alpha=100) 23.487453 1 Ridge(alpha=10) 22.793119 2 LinearRegression 22.770784 model mse 0 Ridge(alpha=100) 23.487453 1 Ridge(alpha=10) 22.793119 2 LinearRegression 22.770784 3 Ridge(alpha=1) 22.690411 model mse 0 Ridge(alpha=100) 23.487453 1 Ridge(alpha=10) 22.793119 2 LinearRegression 22.770784 3 Ridge(alpha=0.1) 22.718126 4 Ridge(alpha=1) 22.690411 model mse 0 Ridge(alpha=100) 23.487453 1 Ridge(alpha=10) 22.793119 2 LinearRegression 22.770784 3 Ridge(alpha=0.01) 22.764254 4 Ridge(alpha=0.1) 22.718126 5 Ridge(alpha=1) 22.690411 model mse 0 Ridge(alpha=100) 23.487453 1 Ridge(alpha=10) 22.793119 2 LinearRegression 22.770784 3 Ridge(alpha=0.001) 22.770117 4 Ridge(alpha=0.01) 22.764254 5 Ridge(alpha=0.1) 22.718126 6 Ridge(alpha=1) 22.690411 coefficents 값 확인 1x_train.columns Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='object') 1ridge.coef_ # for the last alpha in 'alphas' array([ -0.09608448, 0.04753482, 0.0259022 , 3.24479273, -18.89579975, 4.06725732, 0.0020486 , -1.46883742, 0.28149275, -0.0094656 , -0.87454099, 0.01240815, -0.52406249]) 1234567891011121314# coefficients visulizationdef plot_coef(columns, coef): coef_df = pd.DataFrame(list(zip(columns, coef))) coef_df.columns=['feature', 'coef'] coef_df = coef_df.sort_values('coef', ascending=False).reset_index(drop=True) fig, ax = plt.subplots(figsize=(9, 7)) ax.barh(np.arange(len(coef_df)), coef_df['coef']) idx = np.arange(len(coef_df)) ax.set_yticks(idx) ax.set_yticklabels(coef_df['feature']) fig.tight_layout() plt.show() 1plot_coef(x_train.columns, ridge.coef_) # alpha = 0.001 alpha 값에 따른 coef의 차이 1234567ridge_1 = Ridge(alpha=1)ridge_1.fit(x_train, y_train)ridge_pred_1 = ridge_1.predict(x_test)ridge_100 = Ridge(alpha=100)ridge_100.fit(x_train, y_train)ridge_pred_100 = ridge_100.predict(x_test) 1plot_coef(x_train.columns, ridge_1.coef_) # alpha = 1 1plot_coef(x_train.columns, ridge_100.coef_) # alpha = 100 &gt;&gt; LASSO [Document] 1from sklearn.linear_model import Lasso 예측 결과 확인 123456789# lambda (규제강도) 범위 설정alphas = [100, 10, 1, 0.1, 0.01, 0.001]# 모델 학습for alpha in alphas: lasso = Lasso(alpha=alpha) lasso.fit(x_train, y_train) lasso_pred = lasso.predict(x_test) mse_eval('Lasso(alpha={})'.format(alpha), y_test, lasso_pred) model mse 0 Lasso(alpha=100) 63.348818 1 Ridge(alpha=100) 23.487453 2 Ridge(alpha=10) 22.793119 3 LinearRegression 22.770784 4 Ridge(alpha=0.001) 22.770117 5 Ridge(alpha=0.01) 22.764254 6 Ridge(alpha=0.1) 22.718126 7 Ridge(alpha=1) 22.690411 model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Ridge(alpha=100) 23.487453 3 Ridge(alpha=10) 22.793119 4 LinearRegression 22.770784 5 Ridge(alpha=0.001) 22.770117 6 Ridge(alpha=0.01) 22.764254 7 Ridge(alpha=0.1) 22.718126 8 Ridge(alpha=1) 22.690411 model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Lasso(alpha=1) 27.493672 3 Ridge(alpha=100) 23.487453 4 Ridge(alpha=10) 22.793119 5 LinearRegression 22.770784 6 Ridge(alpha=0.001) 22.770117 7 Ridge(alpha=0.01) 22.764254 8 Ridge(alpha=0.1) 22.718126 9 Ridge(alpha=1) 22.690411 model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Lasso(alpha=1) 27.493672 3 Ridge(alpha=100) 23.487453 4 Lasso(alpha=0.1) 22.979708 5 Ridge(alpha=10) 22.793119 6 LinearRegression 22.770784 7 Ridge(alpha=0.001) 22.770117 8 Ridge(alpha=0.01) 22.764254 9 Ridge(alpha=0.1) 22.718126 10 Ridge(alpha=1) 22.690411 model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Lasso(alpha=1) 27.493672 3 Ridge(alpha=100) 23.487453 4 Lasso(alpha=0.1) 22.979708 5 Ridge(alpha=10) 22.793119 6 LinearRegression 22.770784 7 Ridge(alpha=0.001) 22.770117 8 Ridge(alpha=0.01) 22.764254 9 Ridge(alpha=0.1) 22.718126 10 Ridge(alpha=1) 22.690411 11 Lasso(alpha=0.01) 22.635614 model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Lasso(alpha=1) 27.493672 3 Ridge(alpha=100) 23.487453 4 Lasso(alpha=0.1) 22.979708 5 Ridge(alpha=10) 22.793119 6 LinearRegression 22.770784 7 Ridge(alpha=0.001) 22.770117 8 Ridge(alpha=0.01) 22.764254 9 Lasso(alpha=0.001) 22.753017 10 Ridge(alpha=0.1) 22.718126 11 Ridge(alpha=1) 22.690411 12 Lasso(alpha=0.01) 22.635614 coefficients 값 확인 123456789# alpha = 0.01lasso_01 = Lasso(alpha=0.01)lasso_01.fit(x_train, y_train)lasso_pred_01 = lasso_01.predict(x_test)# alpha = 100lasso_100 = Lasso(alpha=100)lasso_100.fit(x_train, y_train)lasso_pred_100 = lasso_100.predict(x_test) [alpha = 0.01] 1x_train.columns Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='object') 1lasso_01.coef_ array([ -0.09427142, 0.04759954, 0.01255668, 3.08256139, -15.36800113, 4.07373679, -0.00100439, -1.40819927, 0.27152905, -0.0097157 , -0.84377679, 0.01249204, -0.52790174]) 1plot_coef(x_train.columns, lasso_01.coef_) [alpha = 100] 1x_train.columns Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='object') 1lasso_100.coef_ array([-0. , 0. , -0. , 0. , -0. , 0. , -0. , 0. , -0. , -0.02078349, -0. , 0.00644409, -0. ]) 1plot_coef(x_train.columns, lasso_100.coef_) &gt;&gt; ElasticNet [Document] 1from sklearn.linear_model import ElasticNet 예측 결과 확인 1ratios = [0.2, 0.5, 0.8] 1234567# alpha = 0.5 로 고정for ratio in ratios: elasticnet = ElasticNet(alpha=0.1, l1_ratio=ratio) elasticnet.fit(x_train, y_train) elas_pred = elasticnet.predict(x_test) mse_eval('ElasticNet(l1_ratio={})'.format(ratio), y_test, elas_pred) model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Lasso(alpha=1) 27.493672 3 Ridge(alpha=100) 23.487453 4 Lasso(alpha=0.1) 22.979708 5 Ridge(alpha=10) 22.793119 6 LinearRegression 22.770784 7 Ridge(alpha=0.001) 22.770117 8 Ridge(alpha=0.01) 22.764254 9 Lasso(alpha=0.001) 22.753017 10 ElasticNet(l1_ratio=0.2) 22.749018 11 Ridge(alpha=0.1) 22.718126 12 Ridge(alpha=1) 22.690411 13 Lasso(alpha=0.01) 22.635614 model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Lasso(alpha=1) 27.493672 3 Ridge(alpha=100) 23.487453 4 Lasso(alpha=0.1) 22.979708 5 Ridge(alpha=10) 22.793119 6 ElasticNet(l1_ratio=0.5) 22.787269 7 LinearRegression 22.770784 8 Ridge(alpha=0.001) 22.770117 9 Ridge(alpha=0.01) 22.764254 10 Lasso(alpha=0.001) 22.753017 11 ElasticNet(l1_ratio=0.2) 22.749018 12 Ridge(alpha=0.1) 22.718126 13 Ridge(alpha=1) 22.690411 14 Lasso(alpha=0.01) 22.635614 model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Lasso(alpha=1) 27.493672 3 Ridge(alpha=100) 23.487453 4 Lasso(alpha=0.1) 22.979708 5 ElasticNet(l1_ratio=0.8) 22.865628 6 Ridge(alpha=10) 22.793119 7 ElasticNet(l1_ratio=0.5) 22.787269 8 LinearRegression 22.770784 9 Ridge(alpha=0.001) 22.770117 10 Ridge(alpha=0.01) 22.764254 11 Lasso(alpha=0.001) 22.753017 12 ElasticNet(l1_ratio=0.2) 22.749018 13 Ridge(alpha=0.1) 22.718126 14 Ridge(alpha=1) 22.690411 15 Lasso(alpha=0.01) 22.635614 coefficients 값 확인 123456789# ㅣ1_ratio = 0.2elasticnet_2 = ElasticNet(alpha = 0.1, l1_ratio = 0.2)elasticnet_2.fit(x_train, y_train)elast_pred_2 = elasticnet_2.predict(x_test)# l1_ratio = 0.8elasticnet_8 = ElasticNet(alpha=0.1, l1_ratio = 0.8)elasticnet_8.fit(x_train, y_train)elast_pred_8 = elasticnet_8.predict(x_test) [ l1_ratio = 0.2 ] 1x_train.columns Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='object') 1elasticnet_2.coef_ array([-0.09297585, 0.05293361, -0.03950412, 1.30126199, -0.41996826, 3.15838796, -0.00644646, -1.15290012, 0.25973467, -0.01231233, -0.77186571, 0.01201684, -0.60780037]) 1plot_coef(x_train.columns, elasticnet_2.coef_) [ l1_ratio = 0.8 ] 1x_train.columns Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='object') 1elasticnet_8.coef_ array([-0.08797633, 0.05035601, -0.03058513, 1.51071961, -0. , 3.70247373, -0.01017259, -1.12431077, 0.24389841, -0.01189981, -0.73481448, 0.01259147, -0.573733 ]) 1plot_coef(x_train.columns, elasticnet_8.coef_) 4. Scaling 4-1. Scaler 소개 StandardScaler MinMaxScaler RobustScaler 1from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler 1x_train.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT count 379.000000 379.000000 379.000000 379.000000 379.000000 379.000000 379.000000 379.000000 379.000000 379.000000 379.000000 379.000000 379.000000 mean 3.512192 11.779683 10.995013 0.076517 0.548712 6.266953 67.223483 3.917811 9.282322 404.680739 18.448549 357.048100 12.633773 std 8.338717 23.492842 6.792065 0.266175 0.115006 0.681796 28.563787 2.084167 8.583051 166.813256 2.154917 92.745266 7.259213 min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 188.000000 12.600000 2.520000 1.730000 25% 0.078910 0.000000 5.190000 0.000000 0.445000 5.876500 42.250000 2.150900 4.000000 278.000000 17.150000 375.425000 6.910000 50% 0.228760 0.000000 9.690000 0.000000 0.532000 6.208000 74.400000 3.414500 5.000000 330.000000 19.000000 392.110000 11.380000 75% 2.756855 19.000000 18.100000 0.000000 0.624000 6.611000 93.850000 5.400900 8.000000 666.000000 20.200000 396.260000 16.580000 max 73.534100 100.000000 27.740000 1.000000 0.871000 8.398000 100.000000 10.585700 24.000000 711.000000 22.000000 396.900000 37.970000 &gt;&gt; StandardScaler 평균(mean)을 0, 표준편차(std)를 1로 만들어 주는 scaler 123std_scaler = StandardScaler()std_scaled = std_scaler.fit_transform(x_train)round(pd.DataFrame(std_scaled).describe(), 2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 10 11 12 count 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 mean -0.00 0.00 0.00 -0.00 -0.00 -0.00 -0.00 0.00 -0.00 0.00 0.00 0.00 0.00 std 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 min -0.42 -0.50 -1.55 -0.29 -1.43 -3.97 -2.25 -1.34 -0.97 -1.30 -2.72 -3.83 -1.50 25% -0.41 -0.50 -0.86 -0.29 -0.90 -0.57 -0.88 -0.85 -0.62 -0.76 -0.60 0.20 -0.79 50% -0.39 -0.50 -0.19 -0.29 -0.15 -0.09 0.25 -0.24 -0.50 -0.45 0.26 0.38 -0.17 75% -0.09 0.31 1.05 -0.29 0.66 0.51 0.93 0.71 -0.15 1.57 0.81 0.42 0.54 max 8.41 3.76 2.47 3.47 2.81 3.13 1.15 3.20 1.72 1.84 1.65 0.43 3.49 &gt;&gt; MinMaxScaler min값과 max값을 0~1사이로 정규화 (Normalize) 123minmax_scaler = MinMaxScaler()minmax_scaled = minmax_scaler.fit_transform(x_train)round(pd.DataFrame(minmax_scaled).describe(), 2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 10 11 12 count 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 379.00 mean 0.05 0.12 0.39 0.08 0.34 0.56 0.66 0.29 0.36 0.41 0.62 0.90 0.30 std 0.11 0.23 0.25 0.27 0.24 0.14 0.29 0.22 0.37 0.32 0.23 0.24 0.20 min 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 25% 0.00 0.00 0.17 0.00 0.12 0.48 0.41 0.11 0.13 0.17 0.48 0.95 0.14 50% 0.00 0.00 0.34 0.00 0.30 0.55 0.74 0.24 0.17 0.27 0.68 0.99 0.27 75% 0.04 0.19 0.65 0.00 0.49 0.63 0.94 0.45 0.30 0.91 0.81 1.00 0.41 max 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 &gt;&gt; RobustScaler 중앙값(median)이 0, IQR(interquartile rage)이 1이 되도록 변환 outlier 처리에 유용 123robust_scaler = RobustScaler()robust_scaled = robust_scaler.fit_transform(x_train)round(pd.DataFrame(robust_scaled).median(), 2) 0 0.0 1 0.0 2 0.0 3 0.0 4 0.0 5 0.0 6 0.0 7 0.0 8 0.0 9 0.0 10 0.0 11 0.0 12 0.0 dtype: float64 4-2. Scaling 후 모델 학습 – 파이프라인 활용 1from sklearn.pipeline import make_pipeline 1234567891011121314# elasticnet(alpha=0.1, l1_ratio=0.2) &lt; without standard scaling &gt;elasticnet_no_scale = ElasticNet(alpha=0.1, l1_ratio=0.2)no_scale_pred = elasticnet_no_scale.fit(x_train, y_train).predict(x_test)mse_eval('No Standard ElasticNet', y_test, no_scale_pred)# elasticnet(alpha=0.1, l1_ratio=0.2) &lt; with standard scaling &gt;elasticnet_pipeline = make_pipeline( StandardScaler(), ElasticNet(alpha=0.1, l1_ratio=0.2))with_scale_pred = elasticnet_pipeline.fit(x_train, y_train).predict(x_test)mse_eval('With Standard ElasticNet', y_test, with_scale_pred) model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Lasso(alpha=1) 27.493672 3 Ridge(alpha=100) 23.487453 4 Lasso(alpha=0.1) 22.979708 5 ElasticNet(l1_ratio=0.8) 22.865628 6 Ridge(alpha=10) 22.793119 7 ElasticNet(l1_ratio=0.5) 22.787269 8 LinearRegression 22.770784 9 Ridge(alpha=0.001) 22.770117 10 Ridge(alpha=0.01) 22.764254 11 Lasso(alpha=0.001) 22.753017 12 ElasticNet(l1_ratio=0.2) 22.749018 13 No Standard ElasticNet 22.749018 14 Ridge(alpha=0.1) 22.718126 15 Ridge(alpha=1) 22.690411 16 Lasso(alpha=0.01) 22.635614 model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Lasso(alpha=1) 27.493672 3 Ridge(alpha=100) 23.487453 4 With Standard ElasticNet 23.230164 5 Lasso(alpha=0.1) 22.979708 6 ElasticNet(l1_ratio=0.8) 22.865628 7 Ridge(alpha=10) 22.793119 8 ElasticNet(l1_ratio=0.5) 22.787269 9 LinearRegression 22.770784 10 Ridge(alpha=0.001) 22.770117 11 Ridge(alpha=0.01) 22.764254 12 Lasso(alpha=0.001) 22.753017 13 ElasticNet(l1_ratio=0.2) 22.749018 14 No Standard ElasticNet 22.749018 15 Ridge(alpha=0.1) 22.718126 16 Ridge(alpha=1) 22.690411 17 Lasso(alpha=0.01) 22.635614 5. Polynomial Features [Document] 다항식의 계수간 상호작용을 통해 새로운 feature를 생성한다. 예를 들면, [a, b] 2개의 feature가 존재한다고 가정하고, degree=2로 설정한다면, polynomial features 는 [1, a, b, a^2, ab, b^2]가 돤다 1from sklearn.preprocessing import PolynomialFeatures Polynomial Features 생성 1poly = PolynomialFeatures(degree=2, include_bias=False) 12poly_features = poly.fit_transform(x_train)[0]poly_features array([ 0.12329 , 0. , 10.01 , 0. , 0.547 , 5.913 , 92.9 , 2.3534 , 6. , 432. , 17.8 , 394.95 , 16.21 , 0.01520042, 0. , 1.2341329 , 0. , 0.06743963, 0.72901377, 11.453641 , 0.29015069, 0.73974 , 53.26128 , 2.194562 , 48.6933855 , 1.9985309 , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 100.2001 , 0. , 5.47547 , 59.18913 , 929.929 , 23.557534 , 60.06 , 4324.32 , 178.178 , 3953.4495 , 162.2621 , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.299209 , 3.234411 , 50.8163 , 1.2873098 , 3.282 , 236.304 , 9.7366 , 216.03765 , 8.86687 , 34.963569 , 549.3177 , 13.9156542 , 35.478 , 2554.416 , 105.2514 , 2335.33935 , 95.84973 , 8630.41 , 218.63086 , 557.4 , 40132.8 , 1653.62 , 36690.855 , 1505.909 , 5.53849156, 14.1204 , 1016.6688 , 41.89052 , 929.47533 , 38.148614 , 36. , 2592. , 106.8 , 2369.7 , 97.26 , 186624. , 7689.6 , 170618.4 , 7002.72 , 316.84 , 7030.11 , 288.538 , 155985.5025 , 6402.1395 , 262.7641 ]) 1x_train.iloc[0] CRIM 0.12329 ZN 0.00000 INDUS 10.01000 CHAS 0.00000 NOX 0.54700 RM 5.91300 AGE 92.90000 DIS 2.35340 RAD 6.00000 TAX 432.00000 PTRATIO 17.80000 B 394.95000 LSTAT 16.21000 Name: 112, dtype: float64 Polynomial Features + Standard Scaling 후 모델 학습 12345poly_pipeline = make_pipeline( PolynomialFeatures(degree=2, include_bias=False), StandardScaler(), ElasticNet(alpha=0.1, l1_ratio=0.2)) 1poly_pred = poly_pipeline.fit(x_train, y_train).predict(x_test) D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 32.61172784964583, tolerance: 3.2374824854881266 positive) 1mse_eval('Poly ElasticNet', y_test, poly_pred) model mse 0 Lasso(alpha=100) 63.348818 1 Lasso(alpha=10) 42.436622 2 Lasso(alpha=1) 27.493672 3 Ridge(alpha=100) 23.487453 4 With Standard ElasticNet 23.230164 5 Lasso(alpha=0.1) 22.979708 6 ElasticNet(l1_ratio=0.8) 22.865628 7 Ridge(alpha=10) 22.793119 8 ElasticNet(l1_ratio=0.5) 22.787269 9 LinearRegression 22.770784 10 Ridge(alpha=0.001) 22.770117 11 Ridge(alpha=0.01) 22.764254 12 Lasso(alpha=0.001) 22.753017 13 ElasticNet(l1_ratio=0.2) 22.749018 14 No Standard ElasticNet 22.749018 15 Ridge(alpha=0.1) 22.718126 16 Ridge(alpha=1) 22.690411 17 Lasso(alpha=0.01) 22.635614 18 Poly ElasticNet 17.526214 2차 Polynomial Features 추가 후 학습된 모델의 성능이 많이 향상 된것을 확인할 수 있다 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - Machine Learning","slug":"【STUDY-Python】/Python-Machine-Learning","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-Machine-Learning/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"sklearn","slug":"sklearn","permalink":"https://hyemin-kim.github.io/tags/sklearn/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://hyemin-kim.github.io/tags/Machine-Learning/"},{"name":"회귀","slug":"회귀","permalink":"https://hyemin-kim.github.io/tags/%ED%9A%8C%EA%B7%80/"}]},{"title":"Python >> sklearn - (2) 분류 (Classification)","slug":"S-Python-sklearn2","date":"2020-07-26T11:23:49.000Z","updated":"2020-11-06T05:20:32.608Z","comments":true,"path":"2020/07/26/S-Python-sklearn2/","link":"","permalink":"https://hyemin-kim.github.io/2020/07/26/S-Python-sklearn2/","excerpt":"","text":"분류 (Classification) 0. 데이터 셋 0-1. iris 데이터 셋 0-2. 데이터프레임 만들기 0-3. 시각화로 데이터셋 파악하기 1. training set / validation set 나누기 2. 하이퍼 파라미터 (hyper-parameter) 튜닝 3. 분류 알고리즘 3-1. Logistic Regression 3-2. SGD (SGDClassifier) 3-3. KNN (KNeighborsClassifier) 3-4. SVM (SVC) 3-5. Decision Tree (DecisionTreeClassifier) 1. Decision Tree (의사 결정 나무): 나무 가지치기를 통해 소그룹으로 나누어 판별하는것 2. Decision Tree 분류 결과 시각화 3. 가지 치기 (pruning) 4. 모델 성능 평가 지표 4-1. 오차 행렬 (Confusion Matrix) 4-2. 정확도 (Accuracy) 4-3. 정밀도 (Precision) 4-4. 민감도 (Sensitivity) / 재현율 (Recall) 4-5. 특이도 (Specificity) 4-6. F1 Score 12import warningswarnings.filterwarnings('ignore') # 불필요한 경고 출력을 방지함 1import pandas as pd 0. 데이터 셋 sklearn.dataset 에서 제공해주는 다양한 샘플 데이터를 활용한다 여기서는 iris 데이터 셋을 활용한다 0-1. iris 데이터 셋 Mission: 꽃 종류 분류하기 iris 데이터 셋 1from sklearn.datasets import load_iris 12# iris 데이터 셋 로드iris = load_iris() iris 데이터 셋 구성 (key values): DESCR: 데이터 셋의 정보를 보여줌 data: feature data feature_names: feature data의 컬럼 이름 target: label data (수치형) target_names: label data의 value 이름 (문자형) 12# 데이터 셋 정보 확인하기print(iris['DESCR']) .. _iris_dataset: Iris plants dataset -------------------- **Data Set Characteristics:** :Number of Instances: 150 (50 in each of three classes) :Number of Attributes: 4 numeric, predictive attributes and the class :Attribute Information: - sepal length in cm - sepal width in cm - petal length in cm - petal width in cm - class: - Iris-Setosa - Iris-Versicolour - Iris-Virginica :Summary Statistics: ============== ==== ==== ======= ===== ==================== Min Max Mean SD Class Correlation ============== ==== ==== ======= ===== ==================== sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) ============== ==== ==== ======= ===== ==================== :Missing Attribute Values: None :Class Distribution: 33.3% for each of 3 classes. :Creator: R.A. Fisher :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) :Date: July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher's paper. Note that it's the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda &amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. .. topic:: References - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley &amp; Sons. ISBN 0-471-22361-1. See page 218. - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. - See also: 1988 MLC Proceedings, 54-64. Cheeseman et al\"s AUTOCLASS II conceptual clustering system finds 3 classes in the data. - Many, many more ... 123# data 불러오기data = iris['data']data[:5] array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2]]) 123# feature names 확인하기feature_names = iris['feature_names']feature_names ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] [해석] sepal: 꽃 받침; petal: 꽃잎 123# label data 확인하기target = iris['target']target[:5] array([0, 0, 0, 0, 0]) 12# target names 확인하기iris['target_names'] array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10') 0-2. 데이터프레임 만들기 123# feature data 먼저 생성하기df_iris = pd.DataFrame(data, columns = feature_names)df_iris.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 123# target column 추가하기df_iris['target'] = targetdf_iris.head() # 최종 dataframe .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 0-3. 시각화로 데이터셋 파악하기 12import matplotlib.pyplot as pltimport seaborn as sns 1. Sepal data로 보는 꽃 종류 1df_iris.columns Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)', 'target'], dtype='object') 123sns.scatterplot('sepal width (cm)', 'sepal length (cm)', hue='target', palette='muted', data=df_iris)plt.title('Sepal')plt.show() 2. petal data로 보는 꽃 종류 123sns.scatterplot('petal width (cm)', 'petal length (cm)', hue='target', palette='muted', data=df_iris)plt.title('Petal')plt.show() 3. 3D plot로 보는 꽃 종류 (PCA 이용) 1234567891011121314151617from mpl_toolkits.mplot3d import Axes3Dfrom sklearn.decomposition import PCAfig = plt.figure(figsize=(8, 6))ax = Axes3D(fig, elev=-150, azim=110)X_reduced = PCA(n_components=3).fit_transform(df_iris.drop('target', 1))ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=df_iris['target'], cmap=plt.cm.Set1, edgecolor='k', s=40)ax.set_title(\"Iris 3D\")ax.set_xlabel(\"x\")ax.w_xaxis.set_ticklabels([])ax.set_ylabel(\"y\")ax.w_yaxis.set_ticklabels([])ax.set_zlabel(\"z\")ax.w_zaxis.set_ticklabels([])plt.show() 1. training set / validation set 나누기 1from sklearn.model_selection import train_test_split 1x_train, x_valid, y_train, y_valid = train_test_split(df_iris.drop('target', 1), df_iris['target']) 1x_train.shape, y_train.shape ((112, 4), (112,)) 1x_valid.shape, y_valid.shape ((38, 4), (38,)) 1sns.countplot(y_train) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1cb7aaaeec8&gt; 'target’값이 0, 1, 2인 데이터가 Original dataset으로 부터 랜덤으로 뽑히기 때문에 비율의 차이가 존재할 수 있다. 따라서 기계학습할 때 sample size가 큰 데이터 위주로 학습하여 모델의 예측성능이 떨어질 수 있다. (위 상황에서, 학습된 머신러닝 모델이 sample size가 큰 target=1인 경우를 좀 더 잘 예측하고, target=2에 대한 예측도가 떨어질 수 있다) 이를 방지하기 위해 우리는 stratify옵션을 이용하여 label의 class 분포를 균등하게 배분한다. 1x_train, x_valid, y_train, y_valid = train_test_split(df_iris.drop('target', 1), df_iris['target'], stratify=df_iris['target']) 1sns.countplot(y_train) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1cb7b17b508&gt; 1x_train.shape, y_train.shape ((112, 4), (112,)) 1x_valid.shape, y_valid.shape ((38, 4), (38,)) 2. 하이퍼 파라미터 (hyper-parameter) 튜닝 모델 학습할 때 설정 한 옵션들은 **하이퍼 파라미터 (hyper-parameter)**라고 한다. 설정한 값에 따라 모델 성능도 달라질 수 있다. 각 알고리즘 별, hyper-parameter의 종류가 매우 다양하다. 다음 두 가지 parameter는 기본적으로 설정해주는 것이 좋다: random_state: sampling seed 설정 (항상 동일하게 sampling 하기) n_jobs=-1: CPU를 모두 사용 (학습속도가 빠름) 3. 분류 알고리즘 3-1. Logistic Regression [sklearn.linear_model.LogisticRegression] Document Logistic Regression, SVM(Support Vector Machine)과 같은 알고리즘은 이진(Binary Class) 분류만 가능한다. (2개의 클래스 판별만 가능한다.) 하지만, 3개 이상의 클래스에 대한 판별 **[다중 클래스(Multi-Class) 분류]**을 진행하는 경우, 다음과 같은 전략으로 판별한다. one-vs-one (OvO): K 개의 클래스가 존재할 때, 이 중 2개의 클래스 조합을 선택하여 K(K−1)/2 개의 이진 클래스 분류 문제를 풀고 이진판별을 통해 가장 많은 판별값을 얻은 클래스를 선택하는 방법이다. one-vs-rest (OvR): K 개의 클래스가 존재할 때, 클래스들을 “k번째 클래스(one)” &amp; \"나머지(rest)\"로 나누어서 K개의 개별 이진 분류 문제를 푼다. 즉, 각각의 클래스에 대해 표본이 속하는지(y=1) 속하지 않는지(y=0)의 이진 분류 문제를 푸는 것이다. OvO와 달리 클래스 수만큼의 이진 분류 문제를 풀면 된다. 대부분 OvsR 전략을 선호합니다. 1from sklearn.linear_model import LogisticRegression step 1: 모델 선언 1lr = LogisticRegression(random_state=0) step 2: 모델 학습 1lr.fit(x_train, y_train) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=0, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) step 3: 예측 1prediction = lr.predict(x_valid) 1prediction[:5] array([0, 1, 2, 2, 0]) step 4: 평가 1(prediction == y_valid).mean() # 정확도 0.9473684210526315 3-2. SGD (SGDClassifier) [sklearn.linear_model.SGDClassifier] Document stochastic gradient descent (SGD): 확률적 경사 하강법 1from IPython.display import Image 12# 출처: https://machinelearningnotepad.wordpress.com/Image('https://machinelearningnotepad.files.wordpress.com/2018/04/yk1mk.png', width=500) 1from sklearn.linear_model import SGDClassifier step 1: 모델 선언 1sgd = SGDClassifier(random_state=0) step 2: 모델 학습 1sgd.fit(x_train, y_train) SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5, random_state=0, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) step 3: 예측 1prediction = sgd.predict(x_valid) step 4: 평가 1(prediction == y_valid).mean() 0.9473684210526315 Change hyper-parameter values: e.g.: penalty = ‘l1’, random_state = 1, n_jobs = -1 1sgd2 = SGDClassifier(penalty='l1', random_state=1, n_jobs=-1) 1sgd2.fit(x_train, y_train) SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l1', power_t=0.5, random_state=1, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) 1prediction2 = sgd2.predict(x_valid) 1(prediction2 == y_valid).mean() 1.0 3-3. KNN (KNeighborsClassifier) [sklearn.neighbors.KNeighborsClassifier] Document KNN (K Nearest Neighbors): K 최근접 이웃 알고리즘 새로운 데이터의 분류 결과가 K 개 최근접 이웃의 클래스에 의해서 결정되며, 데이터는 가장 많이 할당되는 클래스로 분류하게 된다. 12# 출처: 데이터 캠프Image('https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final_a1mrv9.png') 1from sklearn.neighbors import KNeighborsClassifier 12# 1. 모델 선언knn = KNeighborsClassifier() 12# 2. 모델 학습knn.fit(x_train, y_train) # default: n_neighbors=5 KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform') 12# 3. 예측prediction = knn.predict(x_valid) 12# 4. 평가(prediction == y_valid).mean() 0.9210526315789473 n_neighnors를 9개로 설정하여 다시 예측해본다: 123knn2 = KNeighborsClassifier(n_neighbors=9)knn2.fit(x_train, y_train)knn2_pred = knn2.predict(x_valid) 1(knn2_pred == y_valid).mean() 0.9473684210526315 3-4. SVM (SVC) [sklearn.svm.SVC] Document 새로운 데이터가 어느 카테고리에 속할지 판단하는 비확률적 이진 선형 분류 모델을 만듦. 경계로 표현되는 데이터들 중 가장 큰 폭을 가진 경계를 찾는 알고리즘. 1Image('https://csstudy.files.wordpress.com/2011/03/screen-shot-2011-02-28-at-5-53-26-pm.png') SVM은 Logistic Regression과 같이 이진 분류만 가능하다. (2개의 클래스 판별만 가능) 3개 이상의 클래스인 경우: OvsR 전략 사용 1from sklearn.svm import SVC # SVC: Support Vector Classification 123svc = SVC(random_state=0)svc.fit(x_train, y_train)svc_pred = svc.predict(x_valid) 1svc # hyper-parameter 확인 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf', max_iter=-1, probability=False, random_state=0, shrinking=True, tol=0.001, verbose=False) 1(svc_pred == y_valid).mean() 0.9473684210526315 각 클래스 별 확률값을 return해주는 decision_function() 1svc.decision_function(x_valid)[:5] array([[ 2.22273426, 1.18194657, -0.25426485], [-0.22060229, 2.23192595, 0.91725911], [-0.23638817, 1.18969144, 2.17593611], [-0.23457057, 1.07146337, 2.22588253], [ 2.22808358, 1.16872302, -0.25381783]]) 1svc_pred[:5] array([0, 1, 2, 2, 0]) 확률값이 제일 높은 클래스로 분류(예측) 된 것을 확인하실 수 있다 3-5. Decision Tree (DecisionTreeClassifier) [sklearn.tree.DecisionTreeClassifier] Document 1. Decision Tree (의사 결정 나무): 나무 가지치기를 통해 소그룹으로 나누어 판별하는것 1Image('https://www.researchgate.net/profile/Ludmila_Aleksejeva/publication/293194222/figure/fig1/AS:669028842487827@1536520314657/Decision-tree-for-Iris-dataset.png', width=500) 1from sklearn.tree import DecisionTreeClassifier 1dt = DecisionTreeClassifier(random_state=0) 1dt.fit(x_train, y_train) DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=0, splitter='best') 1dt_pred = dt.predict(x_valid) 1(dt_pred == y_valid).mean() 0.9210526315789473 2. Decision Tree 분류 결과 시각화 123from sklearn.tree import export_graphvizfrom IPython.display import Imageimport numpy as np 방법 1: pydot을 사용하여 \"dot 파일\"을 \"png 이미지\"로 전환 (참고) 1pip install pydot Collecting pydotNote: you may need to restart the kernel to use updated packages. Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB) Requirement already satisfied: pyparsing&gt;=2.1.4 in d:\\anaconda\\lib\\site-packages (from pydot) (2.4.6) Installing collected packages: pydot Successfully installed pydot-1.4.1 ​ 12345678910111213# 참고: https://niceman.tistory.com/169import pydot# .dot결과 생성export_graphviz(dt, out_file='tree.dot', feature_names=feature_names, class_names=np.unique(iris['target_names']))# Encoding(graph,) = pydot.graph_from_dot_file('tree.dot', encoding='utf8')# .dot파일을 .png이미지로 저장graph.write_png('tree.png')Image(filename = 'tree.png', width=600) 방법 2: graphviz.Source이용 (참고) 1pip install -U graphviz Requirement already up-to-date: graphviz in d:\\anaconda\\lib\\site-packages (0.14.1) Note: you may need to restart the kernel to use updated packages. 1import graphviz 123456# 참고: https://www.kaggle.com/vaishvik25/titanic-eda-fe-3-model-decision-tree-vizfrom sklearn.tree import DecisionTreeClassifier, export_graphviztree_dot = export_graphviz(dt,out_file=None, feature_names=feature_names, class_names=np.unique(iris['target_names']))tree = graphviz.Source(tree_dot)tree gini계수: 불순도를 의미함. gini계수가 높을 수록 엔트로피(Entropy)가 큼. 즉, 클래스가 혼잡하게 섞여 있음. 3. 가지 치기 (pruning) Overfitting을 방지하기 위해 적당히 가지 치기를 진행한다. 1234# 수동으로 max_depth 설정dt2 = DecisionTreeClassifier(max_depth=2)dt2.fit(x_train, y_train)dt2_pred = dt2.predict(x_valid) 1(dt2_pred == y_valid).mean() 0.9210526315789473 123tree2_dot = export_graphviz(dt2,out_file=None, feature_names=feature_names, class_names=np.unique(iris['target_names']))tree2 = graphviz.Source(tree2_dot)tree2 4. 모델 성능 평가 지표 참고자료: 분류성능평가지표 - Precision(정밀도), Recall(재현율) and Accuracy(정확도) 4-1. 오차 행렬 (Confusion Matrix) 4-2. 정확도 (Accuracy) 정확도 (Accuracy): 모델이 샘플을 올바르게 예측하는 비율 Accuracy=TP+TNTP+FP+TN+FNAccuracy = \\frac{TP+TN}{TP+FP+TN+FN} Accuracy=TP+FP+TN+FNTP+TN​ !!정확도의 함정!! 정확도는 모델의 성능을 가장 지관적으로 나타낼 수 있는 평가 지표다. 하지만, 만약 Actual positive sample과 Actual negative sample의 비율이 차이가 많이 나면 정확도의 함정에 빠질 수 있다. 즉, 모두 positive / negative로 예측 했을 때 모델의 정확도가 매우 높은 경우다. 이 경우에 예측 정확도가 높지만, 모델의 예측 성능이 좋다라고 말할 수는 없다. 유방암 환자 데이터셋을 이용하여 한번 이해해 볼게요. 123from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitimport numpy as np 1cancer = load_breast_cancer(유방암 환자 데이터셋) 1print(cancer['DESCR']) # describe .. _breast_cancer_dataset: Breast cancer wisconsin (diagnostic) dataset -------------------------------------------- **Data Set Characteristics:** :Number of Instances: 569 :Number of Attributes: 30 numeric, predictive attributes and the class :Attribute Information: - radius (mean of distances from center to points on the perimeter) - texture (standard deviation of gray-scale values) - perimeter - area - smoothness (local variation in radius lengths) - compactness (perimeter^2 / area - 1.0) - concavity (severity of concave portions of the contour) - concave points (number of concave portions of the contour) - symmetry - fractal dimension (\"coastline approximation\" - 1) The mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius. - class: - WDBC-Malignant - WDBC-Benign :Summary Statistics: ===================================== ====== ====== Min Max ===================================== ====== ====== radius (mean): 6.981 28.11 texture (mean): 9.71 39.28 perimeter (mean): 43.79 188.5 area (mean): 143.5 2501.0 smoothness (mean): 0.053 0.163 compactness (mean): 0.019 0.345 concavity (mean): 0.0 0.427 concave points (mean): 0.0 0.201 symmetry (mean): 0.106 0.304 fractal dimension (mean): 0.05 0.097 radius (standard error): 0.112 2.873 texture (standard error): 0.36 4.885 perimeter (standard error): 0.757 21.98 area (standard error): 6.802 542.2 smoothness (standard error): 0.002 0.031 compactness (standard error): 0.002 0.135 concavity (standard error): 0.0 0.396 concave points (standard error): 0.0 0.053 symmetry (standard error): 0.008 0.079 fractal dimension (standard error): 0.001 0.03 radius (worst): 7.93 36.04 texture (worst): 12.02 49.54 perimeter (worst): 50.41 251.2 area (worst): 185.2 4254.0 smoothness (worst): 0.071 0.223 compactness (worst): 0.027 1.058 concavity (worst): 0.0 1.252 concave points (worst): 0.0 0.291 symmetry (worst): 0.156 0.664 fractal dimension (worst): 0.055 0.208 ===================================== ====== ====== :Missing Attribute Values: None :Class Distribution: 212 - Malignant, 357 - Benign :Creator: Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian :Donor: Nick Street :Date: November, 1995 This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. https://goo.gl/U2Uwz2 Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes. The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34]. This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/ .. topic:: References - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43(4), pages 570-577, July-August 1995. - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171. 123data = cancer['data']target = cancer['target']feature_names = cancer['feature_names'] 123# 데이터 프레임 생성df = pd.DataFrame(data = data, columns = feature_names)df['target'] = target 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension target 0 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 ... 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 0 1 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 ... 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 0 2 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 ... 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 0 3 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 ... 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 0 4 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 ... 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 0 5 rows × 31 columns target: 0: Malignant (악성종양); 1: Benign (양성종양) 12pos = df.loc[df['target'] == 1] # 앙성 sampleneg = df.loc[df['target'] == 0] # 음성 sample 1pos.shape, neg.shape ((357, 31), (212, 31)) 시범용 sample data를 생성: 양성 환자 357 + 음성 환자 5 1sample = pd.concat([pos, neg[:5]], sort=True) 1x_train, x_test, y_train, y_test = train_test_split(sample.drop('target',1), sample['target'], random_state=42) 1x_train.shape, y_train.shape ((271, 30), (271,)) 1x_test.shape, y_test.shape ((91, 30), (91,)) 1234# 모델 정의 및 학습model = LogisticRegression()model.fit(x_train, y_train)model_pred = model.predict(x_test) Confusion Matrix 1from sklearn.metrics import confusion_matrix 1confusion_matrix(y_test, model_pred) array([[ 1, 0], [ 2, 88]], dtype=int64) 1234sns.heatmap(confusion_matrix(y_test, model_pred), annot=True, cmap='Reds')plt.xlabel('Predict')plt.ylabel('Actual')plt.show() 정확도 (Accuracy) 12# logistic 모델 정확도(model_pred == y_test).mean() 0.978021978021978 12345# 모두 양성으로 예측한 경우my_pred = np.ones(shape=y_test.shape)# 정확도(my_pred == y_test).mean() 0.989010989010989 정확도만 놓고 본다면, 무조건 양성 환자로 예측하는 분류기가 성능이 더 좋다. 하지만 무조건 양성 환자로 예측해서 예측율이 98.9%로 말하는 의사는 당영히 자질이 좋은 의사라고 볼 수 없다 정확도(Accuracy)만 보고 분류기의 성능을 판별하는 것은 위와 같은 오류에 빠질 수 있다. 이를 보완하기 위해 다음과 같은 지표들도 같이 활용하게 된다 4-3. 정밀도 (Precision) 정밀도 (Precision): 양성 예측의 정확도. 즉, Positive Prediction 중에서 올바르게 예측되는 비율 Precision=TPTP+FPPrecision=\\frac{TP}{TP+FP} Precision=TP+FPTP​ 1from sklearn.metrics import precision_score 1precision_score(y_test, model_pred) 1.0 4-4. 민감도 (Sensitivity) / 재현율 (Recall) 민감도 (Sensitivity) / 재현율 (Recall): 분류기가 양성 샘플에 대한 식별력을 나타남. 즉, Positive Condition 중에서 올바르게 예측되는 비율. True Positive Rate (TPR) 이라고도 불린다. Sensitivity/Recall=TPTP+FNSensitivity / Recall = \\frac{TP}{TP+FN} Sensitivity/Recall=TP+FNTP​ 1from sklearn.metrics import recall_score 1recall_score(y_test, model_pred) 0.9777777777777777 4-5. 특이도 (Specificity) 특이도 (Specificity): 분류기가 음성 샘플에 대한 식별력을 나타남. 즉, Negative Condition 중에서 올바르게 예측되는 비율. True Negative Rate (TNR) 이라고도 불린다. Specificity=TNTN+FPSpecificity = \\frac{TN}{TN+FP} Specificity=TN+FPTN​ 4-6. F1 Score F1 Score: 정밀도(Precision)와 재현율(Recall)의 조화 평균을 나타나는 지표임. 데이터 label이 불균형 구조일 때, 모델의 성능을 정확하게 평가할 수 있으며, 성능을 하나의 숫자로 표현할 수 있다. F1&nbsp;Score=2∗Precision∗RecallPrecision+Recall=TPTP+FN+FP2F1\\ Score = 2*\\frac{Precision * Recall}{Precision + Recall}=\\frac{TP}{TP+\\frac{FN+FP}{2}} F1&nbsp;Score=2∗Precision+RecallPrecision∗Recall​=TP+2FN+FP​TP​ 1from sklearn.metrics import f1_score 1f1_score(y_test, model_pred) 0.9887640449438202 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - Machine Learning","slug":"【STUDY-Python】/Python-Machine-Learning","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-Machine-Learning/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"sklearn","slug":"sklearn","permalink":"https://hyemin-kim.github.io/tags/sklearn/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://hyemin-kim.github.io/tags/Machine-Learning/"},{"name":"분류","slug":"분류","permalink":"https://hyemin-kim.github.io/tags/%EB%B6%84%EB%A5%98/"}]},{"title":"Python >> sklearn - (1) 전처리","slug":"S-Python-sklearn1","date":"2020-07-17T07:37:50.000Z","updated":"2020-11-06T05:20:28.982Z","comments":true,"path":"2020/07/17/S-Python-sklearn1/","link":"","permalink":"https://hyemin-kim.github.io/2020/07/17/S-Python-sklearn1/","excerpt":"","text":"전처리 (Pre-Processing) 개요 1. 전처리의 정의 2. 전처리의 종류 실습 – Titanic 0. 데이터 셋 파악 1. train / validation 셋 나누기 2. 결측치 처리 2-0. 결측치 확인 2-1. Numerical Column의 결측치 처리 2-2. Categorical Column의 결측치 처리 3. Label Encoding: 문자(categorivcal)를 수치(numerical)로 변환 4. 원 핫 인코딩 (One Hot Encoding) 5. Normalize (정규화) 6. Standard Scaling (표준화) 개요 1. 전처리의 정의 데이터 전처리는 데이터 분석에 적합하게 데이터를 가공/ 변경/ 처리/ 클리닝하는 과정이다 2. 전처리의 종류 결측치 - Imputer 이상치 정규화 (Normalization) 0~1사이의 분포로 조정 xnew=x−xminxmax−xminx_{new} = \\frac{x-x_{min}}{x_{max}-x_{min}}xnew​=xmax​−xmin​x−xmin​​ 표준화 (Standardization) 평균을 0, 표준편차를 1로 맞춤 xnew=x−μσx_{new} = \\frac{x-\\mu}{\\sigma}xnew​=σx−μ​ 샘플링 (over/under sampling) 피처 공학 (Feature Engineering) feature 생성/ 연산 구간 생성, 스케일 변경 실습 – Titanic 12import numpy as npimport pandas as pd 12train = pd.read_csv('train.csv')test = pd.read_csv('test.csv') 0. 데이터 셋 파악 1train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S PassengerId: 승객 아이디 Survived: 생존 여부, 1: 생존, 0: 사망 Pclass: 등급 Name: 성함 Sex: 성별 Age: 나이 SibSp: 형제, 자매, 배우자 수 Parch: 부모, 자식 수 Ticket: 티켓번호 Fare: 요즘 Cabin: 좌석번호 Embarked: 탑승 항구 1. train / validation 셋 나누기 STEP 1. feature &amp; label 정의하기 123feature = [ 'Pclass', 'Sex', 'Age', 'Fare'] 123label = [ 'Survived'] 1train[feature].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Pclass Sex Age Fare 0 3 male 22.0 7.2500 1 1 female 38.0 71.2833 2 3 female 26.0 7.9250 3 1 female 35.0 53.1000 4 3 male 35.0 8.0500 1train[label].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Survived 0 0 1 1 2 1 3 1 4 0 STEP 2. 적절한 비율로 train / validation set 나누기 1from sklearn.model_selection import train_test_split reference: &lt; train_test_split &gt; Document train_test_split ( X, y, test_size=…, random_state=…, shuffle=True ) test_size: validation set에 할당할 비율 (20% -&gt; 0.2) random_state: random seed 설정 shuffle: 기본 True: shuffle the data before splitting 1x_train, x_valid, y_train, y_valid = train_test_split(train[feature], train[label], test_size=0.2, random_state=30, shuffle=True) 1x_train.shape, y_train.shape ((712, 4), (712, 1)) 1x_valid.shape, y_valid.shape ((179, 4), (179, 1)) 2. 결측치 처리 2-0. 결측치 확인 방법 1. pandas의 info() 1train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB 방법 2. pandas의 isnull() 합계를 구하는 sum()을 통해 한 눈에 확인할 수 있다 1train.isnull().sum() PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 개별 column의 결측치 확인하기 1train['Age'].isnull().sum() 177 2-1. Numerical Column의 결측치 처리 1train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 1train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB 1. Pandas의 \"fillna()\"를 사용: 1개의 column을 처리할 때 a. 숫자\"0\"으로 채우기 1train['Age'].fillna(0).describe() count 891.000000 mean 23.799293 std 17.596074 min 0.000000 25% 6.000000 50% 24.000000 75% 35.000000 max 80.000000 Name: Age, dtype: float64 b. 통계값(평균)으로 채우기 1train['Age'].fillna(train['Age'].mean()).describe() count 891.000000 mean 29.699118 std 13.002015 min 0.420000 25% 22.000000 50% 29.699118 75% 35.000000 max 80.000000 Name: Age, dtype: float64 2. sklearn의 \"SimpleImputer\"를 사용: 2개 이상의 column을 한 번에 처리할 때 reference: Impute 도큐먼트 SimplrImputer 도큐먼트 SimpleImputer( *, missing_values=nan, strategy=‘mean’, fill_value=None, verbose=0, copy=True, add_indicator=False ) strategy: “mean” / “median” / “most_frequent” / “constant” 1from sklearn.impute import SimpleImputer a. 숫자\"0\"으로 채우기 12# STEP 1. imputer 만들기imputer = SimpleImputer(strategy='constant', fill_value=0) 12# STEP 2. fit() 을 통해 결측치에 대한 학습을 진행하기imputer.fit(train[['Age', 'Pclass']]) SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy='constant', verbose=0) 123# STEP 3. transform() 을 통해 실제 결측치에 대해 처리하기result = imputer.transform(train[['Age', 'Pclass']])result array([[22., 3.], [38., 1.], [26., 3.], ..., [ 0., 3.], [26., 1.], [32., 3.]]) 12# STEP 4. 처리 결과를 original data에 대입train[['Age', 'Pclass']] = result 1train[['Age', 'Pclass']].isnull().sum() Age 0 Pclass 0 dtype: int64 fit_transform() 은 fit()과 transform()을 한 번에 해주는 합수다. 1train = pd.read_csv('train.csv') 1train[['Age', 'Pclass']].isnull().sum() Age 177 Pclass 0 dtype: int64 12# STEP 1. imputer 만들기imputer = SimpleImputer(strategy='constant', fill_value=0) 12# STEP 2. fit and transformresult = imputer.fit_transform(train[['Age', 'Pclass']]) 12# STEP 3. 결과 대입train[['Age', 'Pclass']] = result 1train[['Age', 'Pclass']].isnull().sum() Age 0 Pclass 0 dtype: int64 1train[['Age', 'Pclass']].describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Pclass count 891.000000 891.000000 mean 23.799293 2.308642 std 17.596074 0.836071 min 0.000000 1.000000 25% 6.000000 2.000000 50% 24.000000 3.000000 75% 35.000000 3.000000 max 80.000000 3.000000 b. 통계값(평균)으로 채우기 1train = pd.read_csv('train.csv') 1train[['Age', 'Pclass']].isnull().sum() Age 177 Pclass 0 dtype: int64 123imputer = SimpleImputer(strategy='mean')result = imputer.fit_transform(train[['Age', 'Pclass']])train[['Age', 'Pclass']] = result 1train[['Age', 'Pclass']].isnull().sum() Age 0 Pclass 0 dtype: int64 1train[['Age', 'Pclass']].describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Pclass count 891.000000 891.000000 mean 29.699118 2.308642 std 13.002015 0.836071 min 0.420000 1.000000 25% 22.000000 2.000000 50% 29.699118 3.000000 75% 35.000000 3.000000 max 80.000000 3.000000 2-2. Categorical Column의 결측치 처리 1train = pd.read_csv('train.csv') 1train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 1train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB 1. Pandas의 \"fillna()\"를 사용: 1개의 column을 처리할 때 1train['Embarked'].fillna('S') 0 S 1 C 2 S 3 S 4 S .. 886 S 887 S 888 S 889 C 890 Q Name: Embarked, Length: 891, dtype: object 2. sklearn의 \"SimpleImputer\"를 사용: 2개 이상의 column을 한 번에 처리할 때 123imputer = SimpleImputer(strategy = 'most_frequent')result = imputer.fit_transform(train[['Embarked', 'Cabin']])train[['Embarked', 'Cabin']] = result 1train[['Embarked', 'Cabin']].isnull().sum() Embarked 0 Cabin 0 dtype: int64 3. Label Encoding: 문자(categorivcal)를 수치(numerical)로 변환 기계학습을 위해서 모든 문자로된 데이터는 수치로 변환해야 한다 12train = pd.read_csv('train.csv')train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB 1train['Sex'] 0 male 1 female 2 female 3 female 4 male ... 886 male 887 female 888 female 889 male 890 male Name: Sex, Length: 891, dtype: object 방법 1: convert함수를 직접 정의하기 1train['Sex'].value_counts() male 577 female 314 Name: Sex, dtype: int64 123456# STEP 1. 함수 정의def convert(data): if data == 'female': return 1 elif data == 'male': return 0 12# STEP 2. 함수 applytrain['Sex'].apply(convert) 0 0 1 1 2 1 3 1 4 0 .. 886 0 887 1 888 1 889 0 890 0 Name: Sex, Length: 891, dtype: int64 방법 2: sklearn의 “LabelEncoder” 사용 변환 규칙: value name의 alphabet 순서대로 0, 1, 2… 숫자를 부여 1from sklearn.preprocessing import LabelEncoder 1train['Sex'].value_counts() male 577 female 314 Name: Sex, dtype: int64 1le = LabelEncoder() 1train['Sex_num'] = le.fit_transform(train['Sex']) 1train['Sex_num'].value_counts() 1 577 0 314 Name: Sex_num, dtype: int64 12# class 확인le.classes_ array(['female', 'male'], dtype=object) 12# 숫자 -&gt; 문자le.inverse_transform([0, 1, 1, 0, 0, 1, 1]) array(['female', 'male', 'male', 'female', 'female', 'male', 'male'], dtype=object) NaN 값이 포함되어 있으면, LabeEncoder가 정상 동작하지 않음 1train['Embarked'] 0 S 1 C 2 S 3 S 4 S .. 886 S 887 S 888 S 889 C 890 Q Name: Embarked, Length: 891, dtype: object 1train['Embarked'].value_counts() S 644 C 168 Q 77 Name: Embarked, dtype: int64 1le.fit_transform(train['Embarked']) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) D:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py in _encode(values, uniques, encode, check_unknown) 111 try: --&gt; 112 res = _encode_python(values, uniques, encode) 113 except TypeError: D:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py in _encode_python(values, uniques, encode) 59 if uniques is None: ---&gt; 60 uniques = sorted(set(values)) 61 uniques = np.array(uniques, dtype=values.dtype) TypeError: '&lt;' not supported between instances of 'float' and 'str' ​ During handling of the above exception, another exception occurred: TypeError Traceback (most recent call last) &lt;ipython-input-38-86525b1fc929&gt; in &lt;module&gt; ----&gt; 1 le.fit_transform(train['Embarked']) D:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py in fit_transform(self, y) 250 \"\"\" 251 y = column_or_1d(y, warn=True) --&gt; 252 self.classes_, y = _encode(y, encode=True) 253 return y 254 D:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py in _encode(values, uniques, encode, check_unknown) 112 res = _encode_python(values, uniques, encode) 113 except TypeError: --&gt; 114 raise TypeError(\"argument must be a string or number\") 115 return res 116 else: TypeError: argument must be a string or number 1train['Embarked'] = train['Embarked'].fillna('S') 1train['Embarked'] = le.fit_transform(train['Embarked']) 1train['Embarked'] 0 2 1 0 2 2 3 2 4 2 .. 886 2 887 2 888 2 889 0 890 1 Name: Embarked, Length: 891, dtype: int32 1train['Embarked'].value_counts() 2 646 0 168 1 77 Name: Embarked, dtype: int64 4. 원 핫 인코딩 (One Hot Encoding) pd.get_dummies ( df_name [ ‘col_name’ ] ) 1train = pd.read_csv('train.csv') 1train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 1train.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB \"Embarked\"를 살펴보기 12# Unique Value 확인하기train['Embarked'].value_counts() S 644 C 168 Q 77 Name: Embarked, dtype: int64 123# NA 채우기train['Embarked'] = train['Embarked'].fillna('S')train['Embarked'].value_counts() S 646 C 168 Q 77 Name: Embarked, dtype: int64 123# Label Encoding (문자 to 숫자)train['Embarked_num'] = LabelEncoder().fit_transform(train['Embarked'])train['Embarked_num'].value_counts() 2 646 0 168 1 77 Name: Embarked_num, dtype: int64 Embarked는 탑승 항구의 이니셜을 나타낸다. 우리는 LabelEncoder를 통해서 값을 수치형으로 변환해주었다, 하지만 이대로 데이터를 기계학습 시키면, 기계는 데이터 안에서 관계를 학습한다. 예를 들면, ‘S’= 2, ‘Q’= 1 이라고 되어 있는데, Q+Q=S가 된다라고 학습해버린다 그렇기 때문에, 우리는 각 unique value를 별도의 column으로 분리하고, 값에 해당하는 column는 True (1), 나머지 column는 False (0) 를 갖게 한다.이것이 바로 원 핫 인코딩 이다. 1train['Embarked'][:6] 0 S 1 C 2 S 3 S 4 S 5 Q Name: Embarked, dtype: object 1train['Embarked_num'][:6] 0 2 1 0 2 2 3 2 4 2 5 1 Name: Embarked_num, dtype: int32 12one_hot = pd.get_dummies(train['Embarked_num'][:6])one_hot .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 0 0 1 1 1 0 0 2 0 0 1 3 0 0 1 4 0 0 1 5 0 1 0 12one_hot.columns = ['C', 'Q', 'S']one_hot .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C Q S 0 0 0 1 1 1 0 0 2 0 0 1 3 0 0 1 4 0 0 1 5 0 1 0 원핫인코딩은 카테고리의 특성을(계절, 항구, 성별, 종류…) 가지는 column에 대해서 적용한다 5. Normalize (정규화) 정규화: column간에 다른 min,max 값을 가지는 경우, 정규화를 통해 min / max 의 척도를 맞추어 주는 작업이다 sklearn.preprocessing --&gt; MinMaxScaler() 예: 영화평점 네이버 영화평점 (0점 ~ 10점): [2, 4, 6, 8, 10] 넷플릭스 영화평점 (0점 ~ 5점): [1, 2, 3, 4, 5] 123movie = {'naver': [2, 4, 6, 8, 10], 'netflix': [1, 2, 3, 4, 5] } 12movie = pd.DataFrame(data=movie)movie .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } naver netflix 0 2 1 1 4 2 2 6 3 3 8 4 4 10 5 1from sklearn.preprocessing import MinMaxScaler 1min_max_scaler = MinMaxScaler() 1min_max_movie = min_max_scaler.fit_transform(movie) 1pd.DataFrame(min_max_movie, columns = ['naver', 'netfllix']) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } naver netfllix 0 0.00 0.00 1 0.25 0.25 2 0.50 0.50 3 0.75 0.75 4 1.00 1.00 6. Standard Scaling (표준화) 표준화: 평균이 0, 표준편차가 1이 되도록 변환해주는 작업 sklearn.preprocessing --&gt; StandardScaler() 12from sklearn.preprocessing import StandardScalerstandard_scaler = StandardScaler() 123# 샘플데이터 생성x = np.arange(10)x[9] = 1000 # oulier 추가 1x.mean(), x.std() (103.6, 298.8100399919654) 12# 원본 데이터 표준화하기scaled = standard_scaler.fit_transform(x.reshape(-1, 1)) 1scaled.mean(), scaled.std() (4.4408920985006264e-17, 1.0) 1round(scaled.mean(), 2), scaled.std() # mean값 반올림 (0.0, 1.0) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - Machine Learning","slug":"【STUDY-Python】/Python-Machine-Learning","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-Machine-Learning/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"sklearn","slug":"sklearn","permalink":"https://hyemin-kim.github.io/tags/sklearn/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://hyemin-kim.github.io/tags/Machine-Learning/"},{"name":"전처리","slug":"전처리","permalink":"https://hyemin-kim.github.io/tags/%EC%A0%84%EC%B2%98%EB%A6%AC/"}]},{"title":"Python >> sklearn -(0) sklearn 개요","slug":"S-Python-sklearn0","date":"2020-07-17T07:36:59.000Z","updated":"2020-11-06T05:20:25.235Z","comments":true,"path":"2020/07/17/S-Python-sklearn0/","link":"","permalink":"https://hyemin-kim.github.io/2020/07/17/S-Python-sklearn0/","excerpt":"","text":"scikit-learn 개요 Install Package Import Functions from Sub-packages 3 Steps to Fit Model and Do Prediction &lt; scikit-learn &gt; Homepage scikit-learn 패키지는 지도학습, 비지도학습 등 대부분의 머신러닝 알고리즘을 제공하고 있으며, Python에서 머신러닝을 수행할 때 굉장히 많이 쓰이는 패키지 중의 하나다 Install Package 1pip install -U scikit-learn # -U: Update Note: you may need to restart the kernel to use updated packages. ​ Usage: D:\\Anaconda\\python.exe -m pip install [options] &lt;requirement specifier&gt; [package-index-options] ... D:\\Anaconda\\python.exe -m pip install [options] -r &lt;requirements file&gt; [package-index-options] ... D:\\Anaconda\\python.exe -m pip install [options] [-e] &lt;vcs project url&gt; ... D:\\Anaconda\\python.exe -m pip install [options] [-e] &lt;local project path&gt; ... D:\\Anaconda\\python.exe -m pip install [options] &lt;archive url/path&gt; ... no such option: -: Import Functions from Sub-packages 12from sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_split 3 Steps to Fit Model and Do Prediction STEP 1. 모델 정의 12from sklearn.linear_model import LinearRegressionmodel = LinearRegression() STEP 2. 학습 (Fit in Training set) 명령어: model_name .fit 1model.fit(x_train, y_train) STEP 3. 예측 (Predict in Test set) 명령어: model_name .predict 1prediction = model.predict(x_test) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - Machine Learning","slug":"【STUDY-Python】/Python-Machine-Learning","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-Machine-Learning/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"sklearn","slug":"sklearn","permalink":"https://hyemin-kim.github.io/tags/sklearn/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://hyemin-kim.github.io/tags/Machine-Learning/"}]},{"title":"Python >> Seaborn - (2) 통계 기반의 시각화","slug":"S-Python-Seaborn2","date":"2020-07-03T10:18:20.000Z","updated":"2020-11-27T05:28:16.974Z","comments":true,"path":"2020/07/03/S-Python-Seaborn2/","link":"","permalink":"https://hyemin-kim.github.io/2020/07/03/S-Python-Seaborn2/","excerpt":"","text":"통계 기반의 시각화 0. 통계 기반의 시각화를 제공해주는 Seaborn 1. countplot 1-1. 세로로 그리기 1-2. 가로로 그리기 1-3. 색상 팔레트 설정 2. distplot 2-1. 기본 displot 2-2. 데이터가 Series일 경우 2-3. rugplot 2-4. kde (kernel density) 2-5. 가로로 표현하기 2-6. 컬러 바꾸기 3. heatmap 3-1. 기본 heatmap 3-2. pivot table을 활용하여 그리기 3-3. correlation(상관관계)를 시각화 4. pairplot 4-1. 기본 pairplot 그리기 4-2. hue 옵션으로 특성 구분 4-3. 컬러 팔레트 적용 4-4. 사이즈 적용 5. violinplot 5-1. 기본 violinplot 그리기 5-2. 비교 분포 확인 5-3. 가로로 뉘인 violinplot 5-4. hue 옵션으로 분포 비교 6. lmplot 6-1. 기본 lmplot 6-2. hue 옵션으로 다중 선형관계 그리기 6-3. col 옵션을 추가하여 그래프를 별도로 그려볼 수 있다 7. relplot 7-1. 기본 relplot 7-2. col 옵션으로 그래프 분할 7-3. row와 column에 표기할 데이터 column 선택 7-4. 컬러 팔레트 적용 8. jointplot 8-1. 기본 jointplot 그리기 8-2. 선형관계를 표현하는 regression 라인 그리기 8-3. hex 밀도 보기 8-4. 등고선 모양으로 밀집도 확인하기 1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns 12plt.rcParams[\"figure.figsize\"] = (9, 6) # figure size 설정plt.rcParams[\"font.size\"] = 14 # fontsize 설정 0. 통계 기반의 시각화를 제공해주는 Seaborn reference: Seaborn 공식 도큐먼트 seaborn 라이브러리가 매력적인 이유는 바로 통계 차트다. 이번 실습에서는 seaborn의 다양한 통계 차트 중 대표적인 차트 몇 개를 뽑아서 다뤄볼 예정이다. 그럼 먼저 실습에 사용되는 Dataset을 한번 살펴볼게요. Dataset — \"Titanic\" 12titanic = sns.load_dataset('titanic')titanic .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone 0 0 3 male 22.0 1 0 7.2500 S Third man True NaN Southampton no False 1 1 1 female 38.0 1 0 71.2833 C First woman False C Cherbourg yes False 2 1 3 female 26.0 0 0 7.9250 S Third woman False NaN Southampton yes True 3 1 1 female 35.0 1 0 53.1000 S First woman False C Southampton yes False 4 0 3 male 35.0 0 0 8.0500 S Third man True NaN Southampton no True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 886 0 2 male 27.0 0 0 13.0000 S Second man True NaN Southampton no True 887 1 1 female 19.0 0 0 30.0000 S First woman False B Southampton yes True 888 0 3 female NaN 1 2 23.4500 S Third woman False NaN Southampton no False 889 1 1 male 26.0 0 0 30.0000 C First man True C Cherbourg yes True 890 0 3 male 32.0 0 0 7.7500 Q Third man True NaN Queenstown no True 891 rows × 15 columns survived: 생존여부 pclass: 좌석등급 (숫자) sex: 성별 age: 나이 sibsp: 형제자매 + 배우자 숫자 parch: 부모 + 자식 숫자 fare: 요금 embarked: 탑승 항구 class: 좌석등급 (영문) who: 사람 구분 deck: 데크 embark_town: 탑승 항구 (영문) alive: 생존여부 (영문) alone: 혼자인지 여부 Dataset — \"tips\" 12tips = sns.load_dataset('tips')tips .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 ... ... ... ... ... ... ... ... 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 244 rows × 7 columns total_bill: 총 합계 요금표 tip: 팁 sex: 성별 smoker: 흡연자 여부 day: 요일 time: 식사 시간 size: 식사 인원 12# 배경 설정sns.set(style='darkgrid') 1. countplot 항목별 갯수를 세어주는 countplot 해당 column을 구성하고 있는 value들을 자동으로 구분하여 보여준다 reference: &lt;sns.countplot&gt; Document sns.countplot ( x=None, y=None, hue=None, data=None, color=None, palette=None ) 1-1. 세로로 그리기 12sns.countplot(x='class', hue='who', data=titanic)plt.show() 1-2. 가로로 그리기 12sns.countplot(y='class', hue='who', data=titanic)plt.show() 1-3. 색상 팔레트 설정 12sns.countplot(x='class', hue='who', palette='copper', data= titanic)plt.show() 2. distplot matplotlib의 hist그래프와 kdeplot을 통합한 그래프다. 분포와 밀도를 확인할 수 있음 reference: &lt;sns.distplot&gt; Document sns.displot ( a, hist=True, kde=True, rug=False, vertical=False, color=None ) hist: histogram kde: kernel density estimate rug: rugplot vertical: If True, observed values are on y-axis 123# 샘플 데이터 생성x = np.random.randn(100)x array([-3.39765920e-01, -1.48664049e+00, -5.57926444e-01, 3.25206560e-01, -7.46665762e-01, -3.10926812e-01, -2.14536012e+00, 1.25905620e+00, -2.07806423e-01, 5.56377038e-01, -2.20574498e+00, -1.15138577e-01, -3.32417471e-01, 1.13927613e-01, -7.29559442e-01, -1.31243715e+00, -8.27477111e-01, -1.24455099e+00, -5.44035731e-02, -1.85399773e+00, -1.62571613e+00, 3.89312791e-01, 1.26815698e+00, -7.43355761e-01, -1.34113997e+00, 2.67291801e-02, -4.74142344e-01, -1.07662894e+00, -2.35607451e+00, 1.90337236e-01, -1.18577255e+00, -1.23238300e+00, 9.39298755e-01, -2.69078751e-01, -3.50418097e-01, 1.92109121e+00, -1.46520490e-01, 3.90810577e-01, -6.60511307e-01, -1.46288431e+00, 1.26314685e+00, 2.38384651e-01, 8.03730080e-01, 2.83340226e-01, -1.24219159e+00, -1.50458389e+00, -1.60213592e-01, 3.97086657e-01, 1.27321390e-01, -1.13722876e+00, -1.48448425e+00, 1.36136226e+00, -2.34669327e-01, -1.32679409e+00, 1.59032718e+00, 7.53779845e-01, -7.48815568e-01, 7.34822673e-03, 5.57358372e-01, 1.78429993e+00, -1.50510591e+00, -3.87983571e-01, -7.57372493e-01, 6.25354827e-01, 1.44857563e-01, 7.78608476e-01, -6.61441801e-02, -1.24836018e+00, 1.77522984e+00, 1.60497019e-01, -1.18893624e+00, 1.93951152e+00, -9.34504796e-01, 1.82000588e+00, -1.91594654e+00, -1.13118210e+00, -4.13371342e-01, -5.07021131e-01, 1.57792370e+00, -2.52509848e+00, 1.86695906e-01, -1.18412859e+00, 1.49572473e-01, -3.53669860e-01, 1.38877682e+00, 2.53212949e-02, 7.79387552e-01, -7.41508306e-01, 4.10007279e-01, 1.96517288e-02, -5.69215198e-01, 1.45113980e+00, -8.80722624e-01, 1.35468793e+00, -1.67677998e-03, -1.14952039e+00, 8.90718244e-01, -4.10411520e-01, 6.17620908e-01, 2.96993057e-01]) 2-1. 기본 displot 12sns.distplot(x) # x: numpy arrayplt.show() 2-2. 데이터가 Series일 경우 12x = pd.Series(x, name='x variable')x 0 -0.339766 1 -1.486640 2 -0.557926 3 0.325207 4 -0.746666 ... 95 -1.149520 96 0.890718 97 -0.410412 98 0.617621 99 0.296993 Name: x variable, Length: 100, dtype: float64 12sns.distplot(x) # x: Seriesplt.show() x가 Seires일 때는: 그래프에서 x label이 자동으로 Series 이름(column name) 으로 나타남 2-3. rugplot 데이터 위치를 x축 위에 작은 선분(rug)으로 나타내어 데이터들의 위치 및 분포를 보여준다 12sns.distplot(x, rug=True, hist=False, kde=True)plt.show() 2-4. kde (kernel density) kde 는 histogram보다 부드러운 형태의 분포 곡선을 보여주는 방법 12sns.distplot(x, rug=False, hist=False, kde=True)plt.show() 2-5. 가로로 표현하기 12sns.distplot(x, vertical=True)plt.show() 2-6. 컬러 바꾸기 12sns.distplot(x, color='r')plt.show() 3. heatmap 색상으로 표현할 수 있는 다양한 정보를 일정한 이미지위에 열분포 형태의 비쥬얼한 그래픽으로 출력하는 것이 특정이다 주로 활용되는 경우: pivot table의 데이터를 시각화할 때 데이터의 상관관계를 살펴볼 때 reference: &lt;sns.heatmap&gt; Document sns.heatmap ( data, annot=None, cmap=None ) annot: If True, write the data value in each cell 3-1. 기본 heatmap 123uniform_data = np.random.rand(10, 12)sns.heatmap(uniform_data, annot=True)plt.show() 컬러가 진할수록 숫자가 0에 가깝고, 연할수록 1에 가깝다 3-2. pivot table을 활용하여 그리기 1tips .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 ... ... ... ... ... ... ... ... 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 244 rows × 7 columns 12pivot = tips.pivot_table(index='day', columns='size', values='tip')pivot .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } size 1 2 3 4 5 6 day Thur 1.83 2.442500 2.692500 4.218000 5.000000 5.3 Fri 1.92 2.644375 3.000000 4.730000 NaN NaN Sat 1.00 2.517547 3.797778 4.123846 3.000000 NaN Sun NaN 2.816923 3.120667 4.087778 4.046667 5.0 12sns.heatmap(pivot, cmap='Blues', annot=True)plt.show() 3-3. correlation(상관관계)를 시각화 corr() 함수는 데이터의 상관관계를 보여줌 1titanic.corr() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } survived pclass age sibsp parch fare adult_male alone survived 1.000000 -0.338481 -0.077221 -0.035322 0.081629 0.257307 -0.557080 -0.203367 pclass -0.338481 1.000000 -0.369226 0.083081 0.018443 -0.549500 0.094035 0.135207 age -0.077221 -0.369226 1.000000 -0.308247 -0.189119 0.096067 0.280328 0.198270 sibsp -0.035322 0.083081 -0.308247 1.000000 0.414838 0.159651 -0.253586 -0.584471 parch 0.081629 0.018443 -0.189119 0.414838 1.000000 0.216225 -0.349943 -0.583398 fare 0.257307 -0.549500 0.096067 0.159651 0.216225 1.000000 -0.182024 -0.271832 adult_male -0.557080 0.094035 0.280328 -0.253586 -0.349943 -0.182024 1.000000 0.404744 alone -0.203367 0.135207 0.198270 -0.584471 -0.583398 -0.271832 0.404744 1.000000 12sns.heatmap(titanic.corr(), annot=True, cmap='YlGnBu')plt.show() 4. pairplot pairplot은 grid 형태로 각 집합의 조합에 대해 히스토그램과 분포도를 그린다. (숫자형 column에 대해서만 그려줌) reference: &lt;sns.pairplot&gt; Document sns.pairplot ( data, hue=None, palette=None, height=2.5 ) 1tips.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 4-1. 기본 pairplot 그리기 12sns.pairplot(tips)plt.show() 4-2. hue 옵션으로 특성 구분 12sns.pairplot(tips, hue='size')plt.show() 4-3. 컬러 팔레트 적용 12sns.pairplot(tips, hue='size', palette='rainbow')plt.show() 4-4. 사이즈 적용 12sns.pairplot(tips, hue='size', palette='rainbow', height=4)plt.show() 5. violinplot 마이올린처럼 생긴 violinplot다. column에 대한 데이터의 비교 분포도를 확인할 수 있다. 곡선형 부분 (뚱뚱한 부분)은 데이터의 분포를 나타냄 양쪽 끝 뾰족한 부분은 데이터의 최소값과 최대값을 나타냄 reference: &lt;sns.violinplot&gt; Document sns.violinplot ( x=None. y=None, hue=None, data=None, split=False ) split: When using hue nesting with a variable that takes two levels, setting split to True will draw half of a violin for each level. This can make it easier to directly compare the distributions. 1tips.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 5-1. 기본 violinplot 그리기 12sns.violinplot(x=tips['total_bill'])plt.show() 5-2. 비교 분포 확인 x, y축을 지정해줌으로써 바이올린을 분할하여 비교 분포를 볼 수 있다 12sns.violinplot(x='day', y='total_bill', data=tips)plt.show() 5-3. 가로로 뉘인 violinplot x축, y축 변경 12sns.violinplot(y='day', x='total_bill', data=tips)plt.show() 5-4. hue 옵션으로 분포 비교 사실 hue옵션을 사용하지 않으면 바이올린이 대칭이기 때문에 분포의 큰 의미는 없다. 하지만, hue옵션을 주면, 단일 column에 대한 바이올린 모양의 비교를 할 수 있다. 12sns.violinplot(x='day', y='total_bill', hue='smoker', data=tips, palette='muted')plt.show() split 옵션으로 바이올린을 합쳐서 볼 수 있다 12sns.violinplot(x='day', y='total_bill', hue='smoker', data=tips, palette='muted', split=True)plt.show() violinplot은 이런 경우에 많이 활용된다 6. lmplot lmport (initial: 소문자 L) 은 column간의 선형관계를 확인하기에 용이한 차트임. 또한, outlier도 같이 짐작해 볼 수 있다. reference: &lt;sns.lmplot&gt; Document sns.lmplot ( x, y, data, hue=None, col=None, col_wrap=None, row=None, height=5 ) 1tips.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 6-1. 기본 lmplot 12sns.lmplot(x='total_bill', y='tip', data=tips, height=6)plt.show() 6-2. hue 옵션으로 다중 선형관계 그리기 아래의 그래프를 통하여 비흡연자가, 흡연자 대비 좀 더 가파른 선형관계를 가지는 것을 볼 수 있다 12sns.lmplot(x='total_bill', y='tip', hue='smoker', data=tips, height=6)plt.show() 6-3. col 옵션을 추가하여 그래프를 별도로 그려볼 수 있다 또한, col_wrap으로 한 줄에 표기할 column의 갯수를 명시할 수 있다 12sns.lmplot(x='total_bill', y='tip', hue='smoker', col='day', col_wrap=2, data=tips, height=6)plt.show() 7. relplot 두 column간 상관관계를 보지만 lmport처럼 선형관계를 따로 그려주지 않다 reference: &lt;sns.replot&gt; Document sns.relplot ( x, y, data, hue=None, col=None, row=None, height=5, palette=None ) 1tips.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 7-1. 기본 relplot 12sns.relplot(x='total_bill', y='tip', hue='day', data=tips)plt.show() 7-2. col 옵션으로 그래프 분할 12sns.relplot(x='total_bill', y='tip', hue='day', col='time', data=tips)plt.show() 7-3. row와 column에 표기할 데이터 column 선택 12sns.relplot(x='total_bill', y='tip',hue='day', col='time', row='sex', data=tips)plt.show() 7-4. 컬러 팔레트 적용 12sns.relplot(x='total_bill', y='tip', hue='day', col='time', row='sex', data=tips, palette='CMRmap_r')plt.show() 8. jointplot jointplot은 scatter(산점도)와 histogram(분포)을 동시에 그려줌.(숫자형 데이터만) reference: &lt;sns.jointplot&gt; Document sns.jointplot ( x, y, data=None, kind=‘scatter’, height=6 ) kind: kind of plot to draw. {‘scatter’, ‘reg’, ‘resid’, ‘kde’, ‘hex’} 1tips.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 8-1. 기본 jointplot 그리기 default 로 \"scatter plot\"을 그린다 (kind=‘scatter’) 12sns.jointplot(x='total_bill', y='tip', data=tips)plt.show() 8-2. 선형관계를 표현하는 regression 라인 그리기 옵션: kind='reg’ 12sns.jointplot('total_bill', 'tip', data=tips, kind='reg')plt.show() 8-3. hex 밀도 보기 옵션: kind='hex’ 12sns.jointplot('total_bill', 'tip', data=tips, kind='hex')plt.show() 8-4. 등고선 모양으로 밀집도 확인하기 kind=‘kde’ 옵션으로 데이터의 밀집도를 보다 부드러운 선으로 확인할 수 있다 12iris = sns.load_dataset('iris')iris .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa ... ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica 150 rows × 5 columns 12sns.jointplot('sepal_width', 'petal_length', data=iris, kind='kde', color='g')plt.show() document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 4. Seaborn","slug":"【STUDY-Python】/Python-4-Seaborn","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-4-Seaborn/"},{"name":"Python - 시각화","slug":"【STUDY-Python】/Python-시각화","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%8B%9C%EA%B0%81%ED%99%94/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"시각화","slug":"시각화","permalink":"https://hyemin-kim.github.io/tags/%EC%8B%9C%EA%B0%81%ED%99%94/"},{"name":"Seaborn","slug":"Seaborn","permalink":"https://hyemin-kim.github.io/tags/Seaborn/"}]},{"title":"Python >> Seaborn - (1) Seaborn을 활용한 다양한 그래프 그리기","slug":"S-Python-Seaborn1","date":"2020-07-03T10:14:58.000Z","updated":"2020-11-06T05:20:12.566Z","comments":true,"path":"2020/07/03/S-Python-Seaborn1/","link":"","permalink":"https://hyemin-kim.github.io/2020/07/03/S-Python-Seaborn1/","excerpt":"","text":"Seaborn을 활용한 다양한 그래프 그리기 0. Seaborn 개요 0-1. seaborn 에서만 제공되는 통계 기반 plot 0-2. 아름다운 스타일링 0-3. 컬러 팔레트 0-4. pandas 데이터프레임과 높은 호환성 1. Scatterplot 1-1. x, y, color, area 설정하기 1-2. cmap과 alpha 2. Barplot, Barhplot 2-1. 기본 Barplot 그리기 2-2. 기본 Barhplot 그리기 2-3. Barplot에서 비교 그래프 그리기 3. Line Plot 3-1. 기본 lineplot 그리기 3-2. 2개 이상의 그래프 그리기 3-3. 마커 스타일링 3-4. 라인 스타일 변경하기 4. Areaplot (Filled Area) 5.Histogram 5-1. 기본 Histogram 그리기 5-2. 다중 Histogram 그리기 6. Pie Chart 7. Box Plot 7-1. 기본 박스플롯 생성 7-2. 다중 박스플롯 생성 7-3. Box Plot 축 바꾸기 7-4. Outlier 마커 심볼과 컬러 변경 reference: pyplot 공식 도튜먼트 살펴보기 seaborn 공식 도큐먼트 살펴보기 1234567import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom IPython.display import Image# seabornimport seaborn as sns 12plt.rcParams[\"figure.figsize\"] = (9, 6) # figure size 설정plt.rcParams[\"font.size\"] = 14 # fontsize 설정 0. Seaborn 개요 seaborn은 matplotlib을 더 사용하게 쉽게 해주는 라이브러리다. matplotlib으로 대부분의 시각화는 가능하지만, 다음과 같은 이유로 많은 사람들이 seaborn을 선호한다. 비교: matplotlib을 활용한 다양한 그래프 그리기 0-1. seaborn 에서만 제공되는 통계 기반 plot 1tips = sns.load_dataset(\"tips\") (1) violinplot 123sns.violinplot(x=\"day\", y=\"total_bill\", data=tips)plt.title('violin plot')plt.show() (2) countplot 123sns.countplot(tips['day'])plt.title('countplot')plt.show() (3) relplot 123sns.relplot(x='tip', y='total_bill', data=tips)plt.title('relplot')plt.show() (4) lmplot 123sns.lmplot(x='tip', y='total_bill', data=tips)plt.title('lmplot')plt.show() (5) heatmap 123plt.title('heatmap')sns.heatmap(tips.corr(), annot=True, linewidths=1)plt.show() 0-2. 아름다운 스타일링 (1) default color의 예쁜 조합 seaborn의 최대 장점 중의 하나가 아름다운 컬러팔레트다. 스타일링에 크게 신경 쓰지 않아도 default 컬러가 예쁘게 조합해준다. matplotlib VS seaborn 12plt.bar(tips['day'], tips['total_bill'])plt.show() 12sns.barplot(x=\"day\", y=\"total_bill\", data=tips, palette=\"colorblind\")plt.show() (2) 그래프 배경 설정 그래프의 배경 (grid 스타일)을 설정할 수 있음. sns.set_style(’…’) whitegrid: white background + grid darkgrid: dark background + grid white: white background (without grid) dark: dark background (without grid) 123sns.set_style('darkgrid')sns.barplot(x=\"day\", y=\"total_bill\", data=tips, palette=\"colorblind\")plt.show() 123sns.set_style('white')sns.barplot(x=\"day\", y=\"total_bill\", data=tips, palette=\"colorblind\")plt.show() 0-3. 컬러 팔레트 자세한 컬러팔레트는 공식 도큐먼트를 참고 123456sns.palplot(sns.light_palette((210, 90, 60), input=\"husl\"))sns.palplot(sns.dark_palette(\"muted purple\", input=\"xkcd\"))sns.palplot(sns.color_palette(\"BrBG\", 10))sns.palplot(sns.color_palette(\"BrBG_r\", 10))sns.palplot(sns.color_palette(\"coolwarm\", 10))sns.palplot(sns.diverging_palette(255, 133, l=60, n=10, center=\"dark\")) 1sns.barplot(x=\"tip\", y=\"total_bill\", data=tips, palette='coolwarm') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba5bf62888&gt; 1sns.barplot(x=\"tip\", y=\"total_bill\", data=tips, palette='Reds') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba59e40988&gt; 0-4. pandas 데이터프레임과 높은 호환성 1tips .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 ... ... ... ... ... ... ... ... 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 244 rows × 7 columns 1234sns.catplot(x=\"sex\", y=\"total_bill\", data=tips, kind=\"bar\")plt.show() hue옵션: bar를 새로운 기준으로 분할 12345sns.catplot(x=\"sex\", y=\"total_bill\", hue=\"smoker\", data=tips, kind=\"bar\")plt.show() col / row 옵션: 그래프 자체를 새로운 기준으로 분할 123456sns.catplot(x=\"sex\", y=\"total_bill\", hue=\"smoker\", col=\"time\", data=tips, kind=\"bar\")plt.show() xtick, ytick, xlabel, ylabel을 알아서 생성해 줌 legend까지 자동으로 생성해 줌 뿐만 아니라, 신뢰 구간도 알아서 계산하여 생성함 1. Scatterplot reference: &lt;sns.scatterplot&gt; Document sns.scatterplot ( x, y, size=None, sizes=None, hue=None, palette=None, color=‘auto’, alpha=‘auto’… ) sizes 옵션: size의 선택범위를 설정. (사아즈의 min, max를 설정) hue 옵션: 컬러의 구별 기준이 되는 grouping variable 설정 color 옵션: cmap에 컬러를 지정하면, 컬러 값을 모두 같게 가겨갈 수 있음 alpha 옵션: 투명도 (0~1) 1sns.set_style('darkgrid') 1-1. x, y, color, area 설정하기 12345# 데이터 생성x = np.random.rand(50)y = np.random.rand(50)colors = np.arange(50)area = x * y * 1000 (1) matplotlib 12plt.scatter(x, y, s=area, c=colors)plt.show() (2) seaborn 12sns.scatterplot(x, y, size=area, sizes=(area.min(), area.max()), hue=area, palette='coolwarm')plt.show() [Tip] Palette 이름이 생각안나면: palette 값을 임의로 주고 실행하여 오류 경고창에 정확한 palette 이름을 보여줌 12sns.scatterplot(x, y, size=area, sizes=(area.min(), area.max()), hue=area, palette='coolwarm111')plt.show() --------------------------------------------------------------------------- ValueError Traceback (most recent call last) D:\\Anaconda\\lib\\site-packages\\seaborn\\relational.py in numeric_to_palette(self, data, order, palette, norm) 248 try: --&gt; 249 cmap = mpl.cm.get_cmap(palette) 250 except (ValueError, TypeError): D:\\Anaconda\\lib\\site-packages\\matplotlib\\cm.py in get_cmap(name, lut) 182 \"Colormap %s is not recognized. Possible values are: %s\" --&gt; 183 % (name, ', '.join(sorted(cmap_d)))) 184 ValueError: Colormap coolwarm111 is not recognized. Possible values are: Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples, Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, Set3_r, Spectral, Spectral_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r, afmhot, afmhot_r, autumn, autumn_r, binary, binary_r, bone, bone_r, brg, brg_r, bwr, bwr_r, cividis, cividis_r, cool, cool_r, coolwarm, coolwarm_r, copper, copper_r, cubehelix, cubehelix_r, flag, flag_r, gist_earth, gist_earth_r, gist_gray, gist_gray_r, gist_heat, gist_heat_r, gist_ncar, gist_ncar_r, gist_rainbow, gist_rainbow_r, gist_stern, gist_stern_r, gist_yarg, gist_yarg_r, gnuplot, gnuplot2, gnuplot2_r, gnuplot_r, gray, gray_r, hot, hot_r, hsv, hsv_r, icefire, icefire_r, inferno, inferno_r, jet, jet_r, magma, magma_r, mako, mako_r, nipy_spectral, nipy_spectral_r, ocean, ocean_r, pink, pink_r, plasma, plasma_r, prism, prism_r, rainbow, rainbow_r, rocket, rocket_r, seismic, seismic_r, spring, spring_r, summer, summer_r, tab10, tab10_r, tab20, tab20_r, tab20b, tab20b_r, tab20c, tab20c_r, terrain, terrain_r, twilight, twilight_r, twilight_shifted, twilight_shifted_r, viridis, viridis_r, vlag, vlag_r, winter, winter_r 1-2. cmap과 alpha (1) matplotlib 12345678910111213plt.figure(figsize=(12, 6))plt.subplot(131)plt.scatter(x, y, s=area, c='blue', alpha=0.1)plt.title('alpha=0.1')plt.subplot(132)plt.title('alpha=0.5')plt.scatter(x, y, s=area, c='red', alpha=0.5)plt.subplot(133)plt.title('alpha=1.0')plt.scatter(x, y, s=area, c='green', alpha=1.0)plt.show() (2) seaborn 123456789101112131415plt.figure(figsize=(12, 6))plt.subplot(131)sns.scatterplot(x, y, size=area, sizes=(area.min(), area.max()), color='blue', alpha=0.1)plt.title('alpha=0.1')plt.subplot(132)plt.title('alpha=0.5')sns.scatterplot(x, y, size=area, sizes=(area.min(), area.max()), color='red', alpha=0.5)plt.subplot(133)plt.title('alpha=1.0')sns.scatterplot(x, y, size=area, sizes=(area.min(), area.max()), color='green', alpha=0.9)plt.show() 2. Barplot, Barhplot reference: &lt;sns.barplot&gt; Document sns.boxplot ( x, y, hue=None, data=None, alpha=‘auto’, palette=None / color=None ) 2-1. 기본 Barplot 그리기 (1) matplotlib 12345678910111213x = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']y = [90, 60, 80, 50, 70, 40]plt.figure(figsize = (7,4))plt.bar(x, y, alpha = 0.7, color = 'red')plt.title('Subjects')plt.xticks(rotation=20)plt.ylabel('Grades')plt.show() (2) seaborn 12345678910111213x = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']y = [90, 60, 80, 50, 70, 40]plt.figure(figsize = (7,4))sns.barplot(x, y, alpha=0.8, palette='YlGnBu')plt.title('Subjects')plt.xticks(rotation=20)plt.ylabel('Grades')plt.show() 2-2. 기본 Barhplot 그리기 (1) matplotlib plt.barh 함수 사용 bar 함수에서 xticks / ylabel 로 설정했던 부분이 barh 함수에서 yticks / xlabel 로 변경함 12345678910111213x = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']y = [90, 60, 80, 50, 70, 40]plt.figure(figsize = (7,5))plt.barh(x, y, alpha = 0.7, color = 'red')plt.title('Subjects')plt.yticks(x)plt.xlabel('Grades')plt.show() (2) seaborn sns.barplot 함수를 그대로 사용 barplot함수 안에 x와 y의 위치를 교환 xticks설정이 변경 불필요; 하지만 ylabel설정은 xlable로 변경 필요 1234567891011x = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']y = [90, 60, 80, 50, 70, 40]plt.figure(figsize = (7,5))sns.barplot(y, x, alpha=0.9, palette=\"YlOrRd\")plt.xlabel('Grades')plt.title('Subjects')plt.show() 2-3. Barplot에서 비교 그래프 그리기 (1) matplotlib 12345678910111213141516171819202122232425262728x_label = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']x = np.arange(len(x_label)) # x = [0, 1, 2, 3, 4, 5]y_1 = [90, 60, 80, 50, 70, 40]y_2 = [80, 40, 90, 60, 50, 70]# 넓이 지정width = 0.35# subplots 생성fig, axes = plt.subplots()# 넓이 설정axes.bar(x - width/2, y_1, width, alpha = 0.5)axes.bar(x + width/2, y_2, width, alpha = 0.8)# ticks &amp; label 설정plt.xticks(x)axes.set_xticklabels(x_label)plt.ylabel('Grades')# titleplt.title('Subjects')# legendplt.legend(['John', 'Peter'])plt.show() 123456789101112131415161718192021222324252627x_label = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']x = np.arange(len(x_label)) # x = [0, 1, 2, 3, 4, 5]y_1 = [90, 60, 80, 50, 70, 40]y_2 = [80, 40, 90, 60, 50, 70]# 넓이 지정width = 0.35# subplots 생성fig, axes = plt.subplots()# 넓이 설정axes.barh(x - width/2, y_1, width, alpha = 0.5, color = \"green\")axes.barh(x + width/2, y_2, width, alpha = 0.5, color = \"blue\")# ticks &amp; label 설정plt.yticks(x)axes.set_yticklabels(x_label)plt.xlabel('Grades')# titleplt.title('Subjects')# legendplt.legend(['John', 'Peter'])plt.show() (2) seaborn Seaborn에서는 위의 matplotlib과 조금 다른 방식을 취한다. seaborn에서 hue옵션으로 매우 쉽게 비교 barplot을 그릴 수 있음. sns.barplot ( x, y, hue=…, data=…, palette=… ) 실전 tip. 그래프를 임의로 그려야 하는 경우 -&gt; matplotlib DataFrame을 가지고 그리는 경우 -&gt; seaborn 12titanic = sns.load_dataset('titanic')titanic.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone 0 0 3 male 22.0 1 0 7.2500 S Third man True NaN Southampton no False 1 1 1 female 38.0 1 0 71.2833 C First woman False C Cherbourg yes False 2 1 3 female 26.0 0 0 7.9250 S Third woman False NaN Southampton yes True 3 1 1 female 35.0 1 0 53.1000 S First woman False C Southampton yes False 4 0 3 male 35.0 0 0 8.0500 S Third man True NaN Southampton no True 12sns.barplot(x='sex', y='survived', hue='pclass', data=titanic, palette='muted')plt.show() 3. Line Plot reference: &lt;sns.lineplot&gt; Document sns.lineplot ( x, y, label=…, color=None, alpha=‘auto’, marker=None, linestyle=None ) 기본 옵션은 matplotlib의 plt.plot과 비슷 함수만 plt.plot에서 sns.lineplot로 바꾸면 됨 plt.legend() 명령어 따로 쓸 필요없음 배경이 whitegrid / darkgrid 로 설정되어 있을 시 plt.grid() 명령어 불필요 3-1. 기본 lineplot 그리기 (1) matplotlib 12345678910x = np.arange(0, 10, 0.1)y = 1 + np.sin(x)plt.plot(x, y)plt.xlabel('x value')plt.ylabel('y value')plt.title('sin graph', fontsize=16)plt.show() (2) seaborn 1234567sns.lineplot(x, y) # 함수만 변경하면 됨 (plt.plot -&gt; sns.lineplot)plt.xlabel('x value')plt.ylabel('y value')plt.title('sin graph', fontsize=16)plt.show() 3-2. 2개 이상의 그래프 그리기 12345678910111213x = np.arange(0, 10, 0.1)y_1 = 1 + np.sin(x)y_2 = 1 + np.cos(x)sns.lineplot(x, y_1,label='1+sin', color='blue', alpha = 0.3) # label 설정값을 legend에 나타날 수 있음sns.lineplot(x, y_2, label='1+cos', color='red', alpha = 0.7)plt.xlabel(\"x value\")plt.ylabel(\"y value\")plt.title(\"sin and cos graph\", fontsize = 18)plt.show() 3-3. 마커 스타일링 marker: 마커 옵션 12345678910111213x = np.arange(0, 10, 0.1)y_1 = 1 + np.sin(x)y_2 = 1+ np.cos(x)sns.lineplot(x, y_1, label='1+sin', color='blue', alpha=0.3, marker='o')sns.lineplot(x, y_2, label='1+cos', color='red', alpha=0.7, marker='+')plt.xlabel('x value')plt.ylabel('y value')plt.title('sin and cos graph', fontsize = 18)plt.show() 3-4. 라인 스타일 변경하기 linestyle: 라인 스타일 변경하기 12345678910111213x = np.arange(0, 10, 0.1)y_1 = 1 + np.sin(x)y_2 = 1+ np.cos(x)sns.lineplot(x, y_1, label='1+sin', color='blue', linestyle=':')sns.lineplot(x, y_2, label='1+cos', color='red', linestyle='-.')plt.xlabel('x value')plt.ylabel('y value')plt.title('sin and cos graph', fontsize = 18)plt.show() 4. Areaplot (Filled Area) Seaborn에서는 areaplot을 지원하지 않음 matplotlib을 활용하여 구현해야 함 5.Histogram reference: &lt;sns.distplot&gt; Document sns.distplot ( x, bins=None, hist=True, kde=True, vertical=False ) bins: hist bins 갯수 설정 hist: Whether to plot a (normed) histogram kde: Whether to plot a gaussian kernel density estimate vertical: If True, observed values are on y-axis 5-1. 기본 Histogram 그리기 (1) matplotlib 12345678N = 100000bins = 30x = np.random.randn(N)plt.hist(x, bins=bins)plt.show() (2) seaborn Histogram + Density Function (default) 123456N = 100000bins = 30x = np.random.randn(N)sns.distplot(x, bins=bins) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba5cc800c8&gt; Histogram Only 1sns.distplot(x, bins=bins, hist=True, kde=False, color='g') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba5cd09788&gt; Density Function Only 1sns.distplot(x, bins=bins, hist=False, kde=True, color='g') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba5c7cc208&gt; 수평 그래프 1sns.distplot(x, bins=bins, vertical=True, color='r') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba5c250108&gt; 5-2. 다중 Histogram 그리기 matplotlib 에서의 방법을 사용 12345678910111213141516N = 100000bins = 30x = np.random.randn(N)fig, axes = plt.subplots(1, 3, sharey = True, tight_layout = True)fig.set_size_inches(12, 5)axes[0].hist(x, bins = bins)axes[1].hist(x, bins = bins*2)axes[2].hist(x, bins = bins*4)plt.show() 6. Pie Chart Seaborn에서는 pie plot을 지원하지 않음 matplotlib을 활용하여 구현해야 함 7. Box Plot reference: &lt;sns.boxplot&gt; Document sns.baxplot ( x=None, y=None, hue=None, data=None, orient=None, width=0.8 ) hue: 비교 그래프를 그릴 때 나눔 기준이 되는 Variable 설정 orient: “v” / “h”. Orientation of the plot (vertical or horizontal) width: box의 넓이 7-1. 기본 박스플롯 생성 샘플 데이터 생성 123456# DGPspread = np.random.rand(50) * 100center = np.ones(25) * 50flier_high = np.random.rand(10) * 100 + 100flier_low = np.random.rand(10) * -100data = np.concatenate((spread, center, flier_high, flier_low)) (1) matplotlib 12plt.boxplot(data)plt.show() (2) seaborn 12sns.boxplot(data, orient='v', width=0.2)plt.show() 7-2. 다중 박스플롯 생성 seaborn에서는 hue옵션으로 매우 쉽게 비교 boxplot을 그릴 수 있으며 주로 DataFrame을 가지고 그릴 때 활용한다. barplot과 마찬가지로, 용도에 따라 적절한 library를 사용한다 실전 Tip. 그래프를 임의로 그려야 하는 경우 -&gt; matplotlit DataFrame을 가지고 그리는 경우 -&gt; seaborn (1) matplotlib 1234567891011121314151617# DGPspread1 = np.random.rand(50) * 100center1 = np.ones(25) * 50flier_high1 = np.random.rand(10) * 100 + 100flier_low1 = np.random.rand(10) * -100data1 = np.concatenate((spread1, center1, flier_high1, flier_low1))spread2 = np.random.rand(50) * 100center2 = np.ones(25) * 40flier_high2 = np.random.rand(10) * 100 + 100flier_low2 = np.random.rand(10) * -100data2 = np.concatenate((spread2, center2, flier_high2, flier_low2))data1.shape = (-1, 1)data2.shape = (-1, 1)data = [data1, data2, data2[::2, 0]] 12plt.boxplot(data)plt.show() (2) seaborn 12titanic = sns.load_dataset('titanic')titanic.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone 0 0 3 male 22.0 1 0 7.2500 S Third man True NaN Southampton no False 1 1 1 female 38.0 1 0 71.2833 C First woman False C Cherbourg yes False 2 1 3 female 26.0 0 0 7.9250 S Third woman False NaN Southampton yes True 3 1 1 female 35.0 1 0 53.1000 S First woman False C Southampton yes False 4 0 3 male 35.0 0 0 8.0500 S Third man True NaN Southampton no True 12sns.boxplot(x='pclass', y='age', hue='survived', data=titanic)plt.show() 7-3. Box Plot 축 바꾸기 (1) 단일 boxplot orient옵션: orient = \"h\"로 설정 123456# DGPspread = np.random.rand(50) * 100center = np.ones(25) * 50flier_high = np.random.rand(10) * 100 + 100flier_low = np.random.rand(10) * -100data = np.concatenate((spread, center, flier_high, flier_low)) 1sns.boxplot(data, orient='h', width=0.3) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba5e866188&gt; (2) 다중 boxplot x, y 변수 교환 orient = “h” 12sns.boxplot(y='pclass', x='age', hue='survived', data=titanic, orient='h')plt.show() 7-4. Outlier 마커 심볼과 컬러 변경 flierprops = … 옵션 사용 (matplotlib과 동일) 123456outlier_marker = dict(markerfacecolor='r', marker='D')plt.title('Changed Outlier Symbols', fontsize=15)sns.boxplot(data, orient='v', width=0.2, flierprops=outlier_marker)plt.show() document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 4. Seaborn","slug":"【STUDY-Python】/Python-4-Seaborn","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-4-Seaborn/"},{"name":"Python - 시각화","slug":"【STUDY-Python】/Python-시각화","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%8B%9C%EA%B0%81%ED%99%94/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"시각화","slug":"시각화","permalink":"https://hyemin-kim.github.io/tags/%EC%8B%9C%EA%B0%81%ED%99%94/"},{"name":"Seaborn","slug":"Seaborn","permalink":"https://hyemin-kim.github.io/tags/Seaborn/"}]},{"title":"Python >> Matplotlib - (2) 다양한 그래프 그리기","slug":"S-Python-Matplotlib2","date":"2020-06-28T05:12:32.000Z","updated":"2020-11-06T05:18:29.244Z","comments":true,"path":"2020/06/28/S-Python-Matplotlib2/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/28/S-Python-Matplotlib2/","excerpt":"","text":"matplotlib을 활용한 다양한 그래프 그리기 1. Scatterplot 1-1. x, y, colors, area 설정하기 1-2. cmap과 alpha 2. Barplot, Barhplot 2-1. 기본 barplot 그리기 2-2. 기본 Barhplot 그리기 2-3. Barplot에서 비교 그래프 그리기 3. Line Plot 3-1. 기본 lineplot 그리기 3-2. 2개 이상의 그래프 그리기 3-3. 마커 스타일링 3-4. 라인 스타일링 4. Areaplot (Filled Area) 4-1. 기본 areaplot 그리기 4-2. 경계선을 굵게 그리고 area는 옅게 그리는 효과 적용 4-3. 여러 그래프를 겹쳐서 표현 5. Histogram 5-1. 기본 Histogram 그리기 5-2. 다중 Histogram 그리기 5-3. Y축에 Density 표기 6. Pie Chart 7. Box Plot 7-1. 기본 박스플롯 생성 7-2. 다중 박스플롯 생성 7-3. Box Plot 축 바꾸기 7-4. Outlier 마커 심볼과 컬러 변경 8. 3D 그래프 그리기 8-1. 밑그림 그리기 (canvas) 8-2. 3D plot 그리기 8-3. 3d-scatter 그리기 8-4. contour3D 그리기 (등고선) 9. imshow 123import matplotlib.pyplot as pltimport pandas as pdimport numpy as np 12plt.rcParams[\"figure.figsize\"] = (9, 6) # figure size 설정plt.rcParams[\"font.size\"] = 14 # fontsize 설정 1. Scatterplot reference: &lt;plt.scatter&gt; Document plt.scatter( x, y, s=None, c=None, cmap=None, alpha=None ) s: marker size c: color cmap: colormap alpha: between 0 and 1 Data 생성 12# 0~1 사이의 random value 50 개 생성np.random.rand(50) array([0.65532609, 0.19008877, 0.72343673, 0.63981883, 0.07531076, 0.67080518, 0.93282479, 0.04750706, 0.81240348, 0.40032198, 0.59662026, 0.25797641, 0.37315105, 0.6266855 , 0.50732916, 0.55803591, 0.63610033, 0.88673444, 0.99751021, 0.03723629, 0.07695327, 0.44247 , 0.5245731 , 0.41263818, 0.8009583 , 0.57238283, 0.58647938, 0.9882001 , 0.88993497, 0.5396632 , 0.24683042, 0.0838774 , 0.0826096 , 0.89701004, 0.78305308, 0.21027637, 0.93441558, 0.05756907, 0.6299839 , 0.05833447, 0.24247082, 0.9057054 , 0.1585265 , 0.45569918, 0.85597115, 0.43875418, 0.96962923, 0.17476189, 0.68713067, 0.832518 ]) 12# 0 부터 50 개의 value 생성np.arange(50) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]) 1-1. x, y, colors, area 설정하기 plt.scatter ( x, y, s = , c = ) s: 점의 넓이. area 값이 커지면 넓이도 커진다 c: 임의 값을 color 값으로 변환 1234567x = np.random.rand(50)y = np.random.rand(50)colors = np.arange(50)area = x * y * 1000plt.scatter(x, y, s = area, c = colors)plt.show() 1-2. cmap과 alpha cmap에 컬러를 지정하면, 컬러 값을 모두 같게 가져갈 수도 있다 alpha값은 투명도를 나타내며 0~1 사이의 값을 지정해 둘 수 있으며, 0에 가까울 수록 투명한 값을 가진다 123456789101112131415plt.figure(figsize=(12 ,6))plt.subplot(131)plt.scatter(x, y, s = area, cmap = 'blue', alpha = 0.1)plt.title('alpha = 0.1') plt.subplot(132)plt.scatter(x, y, s = area, cmap = 'blue', alpha = 0.5)plt.title('alpha = 0.5') plt.subplot(133)plt.scatter(x, y, s = area, cmap = 'blue', alpha = 1.0)plt.title('alpha = 1.0')plt.show() 2. Barplot, Barhplot reference: &lt;plt.bar&gt; Document plt.bar(x, height, width = 0.8, align = ‘center’, alpha = …, color = … ) x: The x coordinates of the bars height: The height(s) of the bars width: The width(s) of the bars (default: 0.8) align: Alignment of the bars to the x coordinates: {‘center’, ‘edge’} 2-1. 기본 barplot 그리기 12345678910111213141516x = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']y = [90, 60, 80, 50, 70, 40]# figure sizeplt.figure(figsize = (7,4))# 수직 barplotplt.bar(x, y, alpha = 0.7, color = 'red')# titleplt.title('Subjects')# y labelplt.ylabel('Grades')plt.show() 문자열이 겹히는 현상 발생했다. 이를 해결하는 방법은 2가지다: 문자열 화전: plt.xtick(rotation = …) barh(수평바 그래프) 사용 12345678910111213141516171819x = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']y = [90, 60, 80, 50, 70, 40]# figure sizeplt.figure(figsize = (7,4))# 수직 barplotplt.bar(x, y, alpha = 0.7, color = 'red')# titleplt.title('Subjects')# x ticksplt.xticks(rotation = 20)# y labelplt.ylabel('Grades')plt.show() 2-2. 기본 Barhplot 그리기 barh 함수에서는 xticks / ylabel 로 설정했던 부분을 yticks / xlabel 로 변경함 12345678910111213141516171819x = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']y = [90, 60, 80, 50, 70, 40]# figure sizeplt.figure(figsize = (7,4))# 수직 barplotplt.barh(x, y, alpha = 0.7, color = 'green')# titleplt.title('Subjects')# y ticks# plt.yticks(x)# x labelplt.xlabel('Grades')plt.show() 2-3. Barplot에서 비교 그래프 그리기 reference: Grouped bar chart with labels (1) barplot 12345678910111213141516171819202122232425262728x_label = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']x = np.arange(len(x_label)) # x = [0, 1, 2, 3, 4, 5]y_1 = [90, 60, 80, 50, 70, 40]y_2 = [80, 40, 90, 60, 50, 70]# 넓이 지정width = 0.35# subplots 생성fig, axes = plt.subplots()# 넓이 설정axes.bar(x - width/2, y_1, width, alpha = 0.5)axes.bar(x + width/2, y_2, width, alpha = 0.8)# ticks &amp; label 설정plt.xticks(x)axes.set_xticklabels(x_label)plt.ylabel('Grades')# titleplt.title('Subjects')# legendplt.legend(['John', 'Peter'])plt.show() (2) barhplot 123456789101112131415161718192021222324252627x_label = ['Math', 'Programming', 'Data Science', 'Art', 'English', 'Physics']x = np.arange(len(x_label)) # x = [0, 1, 2, 3, 4, 5]y_1 = [90, 60, 80, 50, 70, 40]y_2 = [80, 40, 90, 60, 50, 70]# 넓이 지정width = 0.35# subplots 생성fig, axes = plt.subplots()# 넓이 설정axes.barh(x - width/2, y_1, width, alpha = 0.5, color = \"green\")axes.barh(x + width/2, y_2, width, alpha = 0.5, color = \"blue\")# ticks &amp; label 설정plt.yticks(x)axes.set_yticklabels(x_label)plt.xlabel('Grades')# titleplt.title('Subjects')# legendplt.legend(['John', 'Peter'])plt.show() 3. Line Plot plt.plot ( x, y, label=…, color=…, alpha=…, marker=…, linestyle=…) 3-1. 기본 lineplot 그리기 123456789101112x = np.arange(0, 10, 0.1)y = 1 + np.sin(x)plt.plot(x, y)plt.xlabel('x value')plt.ylabel('y value')plt.title('sin graph', fontsize = 16)plt.grid()plt.show() 3-2. 2개 이상의 그래프 그리기 label: line 이름 (legend에 나타남) color: 컬러 옵션 alpha: 투명도 옵션 123456789101112131415x = np.arange(0, 10, 0.1)y_1 = 1 + np.sin(x)y_2 = 1 + np.cos(x)plt.plot(x, y_1,label='1+sin', color='blue', alpha = 0.3) # label 설정값을 legend에 나타날 수 있음plt.plot(x, y_2, label='1+cos', color='red', alpha = 0.7)plt.xlabel(\"x value\")plt.ylabel(\"y value\")plt.title(\"sin and cos graph\", fontsize = 18)plt.grid()plt.legend()plt.show() 3-3. 마커 스타일링 marker: 마커 옵션 123456789101112131415x = np.arange(0, 10, 0.1)y_1 = 1 + np.sin(x)y_2 = 1+ np.cos(x)plt.plot(x, y_1, label='1+sin', color='blue', alpha=0.3, marker='o')plt.plot(x, y_2, label='1+cos', color='red', alpha=0.7, marker='+')plt.xlabel('x value')plt.ylabel('y value')plt.title('sin and cos graph', fontsize = 18)plt.grid()plt.legend()plt.show() 3-4. 라인 스타일링 linestyle: 라인 스타일 변경 옵션 123456789101112131415x = np.arange(0, 10, 0.1)y_1 = 1 + np.sin(x)y_2 = 1+ np.cos(x)plt.plot(x, y_1, label='1+sin', color='blue', linestyle=':')plt.plot(x, y_2, label='1+cos', color='red', linestyle='-.')plt.xlabel('x value')plt.ylabel('y value')plt.title('sin and cos graph', fontsize = 18)plt.grid()plt.legend()plt.show() 4. Areaplot (Filled Area) reference: &lt;plt.fill_between&gt; Document plt.fill_between (x, y, color=…, alpha=…) 4-1. 기본 areaplot 그리기 12y = np.random.randint(low=5, high=10, size=20)y array([8, 8, 7, 6, 5, 8, 6, 9, 8, 8, 5, 5, 6, 6, 5, 5, 6, 8, 9, 5]) 1234567x = np.arange(1,21)y = np.random.randint(low=5, high=10, size=20)# fill_between으로 색칠하기plt.fill_between(x, y, color = \"green\", alpha = 0.6)plt.show() 4-2. 경계선을 굵게 그리고 area는 옅게 그리는 효과 적용 1234plt.fill_between(x, y, color='green', alpha=0.3)plt.plot(x, y, color='green', alpha=0.7)plt.show() 4-3. 여러 그래프를 겹쳐서 표현 123456789101112x = np.arange(0, 10, 0.05)y_1 = np.cos(x) + 1y_2 = np.sin(x) + 1y_3 = y_1 * y_2 / np.piplt.fill_between(x, y_1, label='1+cos', color='green', alpha=0.1)plt.fill_between(x, y_2, label='1+sin', color='blue', alpha=0.2)plt.fill_between(x, y_3, label='sin*cos/pi', color='red', alpha=0.3)plt.legend()plt.show() 많이 겹치는 부분이 어디인지 확인하고 싶을 때 많이 활용됨 5. Histogram reference: &lt;plt.hist&gt; Document plt.hist (x, bins = …) 5-1. 기본 Histogram 그리기 12345678N = 100000bins = 30x = np.random.randn(N)plt.hist(x, bins = bins)plt.show() 5-2. 다중 Histogram 그리기 fig, axs = plt.subplots (row, column, sharey = True, tight_layout = True) axes[i].hist ( x, bins = …) sharey: 다중 그래프가 같은 y축을 share tight_layout: graph의 패딩을 자동으로 조절해주어 fit한 graph를 생성 12345678910111213141516N = 100000bins = 30x = np.random.randn(N)fig, axes = plt.subplots(1, 3, sharey = True, tight_layout = True)fig.set_size_inches(12, 5)axes[0].hist(x, bins = bins)axes[1].hist(x, bins = bins*2)axes[2].hist(x, bins = bins*4)plt.show() 5-3. Y축에 Density 표기 pdf(확률 밀도 함수): density = True cdf(누적 확률 함수): density = True, cumulatice = True 1234567891011121314N = 100000bins = 30x = np.random.randn(N)fig, axes = plt.subplots(1, 2, tight_layout = True)fig.set_size_inches(12, 4)# density=True 값을 통하여 Y축에 density를 표기할 수 있다axes[0].hist(x, bins = bins, density = True, cumulative = True) #cdf: 누적확률함수axes[1].hist(x, bins = bins, density = True) # pdf: 확률밀도함수plt.show() 6. Pie Chart reference: &lt;plt.pie&gt; Document plt.pie( x, explode=None, labels=None, colors=None, autopct=None, shadow=False, startangle=None,…) pie chart 옵션 explode: 파이에서 툭 튀어져 나온 비율 autopct: 퍼센트 자동으로 표기 shadow: 그림자 표시 startangle: 파이를 그리기 시작할 각도 리턴을 받는 인자 texts: label에 대한 텍스트 효과 autotexts: 파이 위에 그려지는 텍스트 효과 12345678910111213141516171819202122232425labels = ['Samsung', 'Huawei', 'Apple', 'Xiaomi', 'Oppo', 'Etc']sizes = [20.4, 15.8, 10.5, 9, 7.6, 36.7]explode = (0.3, 0, 0, 0, 0, 0)# text, autotext 인자를 활용하여 텍스트 스타일링을 적용한다patches, texts, autotexts = plt.pie(sizes, explode = explode, labels = labels, autopct = \"%1.1f%%\", shadow = True, startangle=90)plt.title('Smartphone Pie', fontsize=15)# label 텍스트에 대한 스타일 적용for t in texts: t.set_fontsize(12) t.set_color('gray') # pie 위의 텍스트에 대한 스타일 적용for t in autotexts: t.set_fontsize(18) t.set_color('white') plt.show() 7. Box Plot reference: &lt;plt.boxplot&gt; Document plt.boxplot (data, vert=True, flierprops = …) vert: boxplot 축 바꾸기 (If True: 수직 boxplot; If not: 수평 boxplot) flierprops: oulier marker 설정 (Symbol &amp; Color) 샘플 데이터 생성 123456# Data Generation Process (DGP)spread = np.random.rand(50) * 100center = np.ones(25) * 50flier_high = np.random.rand(10) * 100 + 100flier_low = np.random.rand(10) * -100data = np.concatenate((spread, center, flier_high, flier_low)) 7-1. 기본 박스플롯 생성 123plt.boxplot(data)plt.tight_layoutplt.show() 7-2. 다중 박스플롯 생성 다중 그래프 생성을 위해서는 data 자체가 2차원으로 구성되어 있어야 한다 row와 column으로 구성된 DataFrame에서 Column은 x축에 Row는 Y축에 구성되어 있음 1234567891011121314151617# DGPspread1 = np.random.rand(50) * 100center1 = np.ones(25) * 50flier_high1 = np.random.rand(10) * 100 + 100flier_low1 = np.random.rand(10) * -100data1 = np.concatenate((spread1, center1, flier_high1, flier_low1))spread2 = np.random.rand(50) * 100center2 = np.ones(25) * 40flier_high2 = np.random.rand(10) * 100 + 100flier_low2 = np.random.rand(10) * -100data2 = np.concatenate((spread2, center2, flier_high2, flier_low2))data1.shape = (-1, 1)data2.shape = (-1, 1)data = [data1, data2, data2[::2, 0]] 12plt.boxplot(data)plt.show() 7-3. Box Plot 축 바꾸기 vert = False 옵션을 사용 1234plt.boxplot(data, vert = False)plt.title('Horizontal Box Plot', fontsize = 16)plt.show() 7-4. Outlier 마커 심볼과 컬러 변경 flierprops = … 옵션 사용 123456outlier_marker = dict(markerfacecolor = 'r', marker = 'D') # red diamondplt.boxplot(data, flierprops = outlier_marker)plt.title('Change Outlier Symbols', fontsize = 16)plt.show() 8. 3D 그래프 그리기 reference: mplot3d tutorial 3D 로 그래프를 그리기 위해서는 mplot3d를 추가로 import 해야 함 1from mpl_toolkits import mplot3d 8-1. 밑그림 그리기 (canvas) 12fig = plt.figure()ax = plt.axes(projection = '3d') 8-2. 3D plot 그리기 Axes = plt.axes(projection = ‘3d’) Axes .plot (x, y, z, color=…, alpha=…, marker=…) Axes .plot3D (x, y, z, color=…, alpha=…, marker=…) 12345678910# projection = 3d로 설정ax = plt.axes(projection = '3d')# x, y, z 데이터 생성z = np.linspace(0, 15, 1000)x = np.sin(z)y = np.cos(z)ax.plot(x, y, z, 'gray')plt.show() 12345678910111213# projection = 3d로 설정ax = plt.axes(projection = '3d')# x, y, z 데이터 생성sample_size = 100x = np.cumsum(np.random.normal(0, 1, sample_size)) # cumsum: 누적 합y = np.cumsum(np.random.normal(0, 1, sample_size))z = np.cumsum(np.random.normal(0, 1, sample_size))ax.plot3D(x, y, z, alpha=0.6, marker='o')plt.title('ax.plot')plt.show() 8-3. 3d-scatter 그리기 reference: &lt;Axes.scatter&gt; Document Axes = fig.add_subplot(111, projection=‘3d’) # Axe3D object Axes .scatter( x, y, z, s=None, c=None, marker=None, cmap=None, alpha=None, …) s: marker size c: marker color 12345678910111213fig = plt.figure(figsize=(10, 5))ax = fig.add_subplot(111, projection='3d') # Axe3D objectsample_size = 500x = np.cumsum(np.random.normal(0, 5, sample_size))y = np.cumsum(np.random.normal(0, 5, sample_size))z = np.cumsum(np.random.normal(0, 5, sample_size))ax.scatter(x, y, z, c=z, s=20, alpha=0.5, cmap='Greens')plt.title('ax.scatter')plt.show() 컬러가 찐한 부분에 데이터가 더 많이 몰려있음 8-4. contour3D 그리기 (등고선) Axes = plt.axes(projection=‘3d’) Axes .contour3D (x, y, z ) 12345678910111213x = np.linspace(-6, 6, 30)y = np.linspace(-6, 6, 30)x, y = np.meshgrid(x, y)z = np.sin(np.sqrt(x**2 + y**2))fig = plt.figure(figsize=(12, 6))ax = plt.axes(projection='3d')ax.contour3D(x, y, z, 20, cmap='Reds')plt.title(\"ax.contour3D\")plt.show() 9. imshow 이미지 데이터가 numpy array에서는 숫자형식으로 표현됨 명령어imshow는 이 컬러숫자들을 이미지로 변환하여 보여줌 예제: sklearn.datasets안의 load_digits데이터 load_digits 는 0~16 값을 가지는 array로 이루어져 있다 1개의 array는 8 X 8 배열 안에 표현되어 있다 숫자는 0~9까지 이루어져있다 12345from sklearn.datasets import load_digitsdigits = load_digits()X = digits.images[:10] # 앞에 10개 image를 뽑아서 저장함X[0] # 첫번째 image의 컬러숫자를 살펴보자 array([[ 0., 0., 5., 13., 9., 1., 0., 0.], [ 0., 0., 13., 15., 10., 15., 5., 0.], [ 0., 3., 15., 2., 0., 11., 8., 0.], [ 0., 4., 12., 0., 0., 8., 8., 0.], [ 0., 5., 8., 0., 0., 9., 8., 0.], [ 0., 4., 11., 0., 1., 12., 7., 0.], [ 0., 2., 14., 5., 10., 12., 0., 0.], [ 0., 0., 6., 13., 10., 0., 0., 0.]]) 지금 한 위치에 숫자 하나밖에 없어서 컬러는 흑백으로 나옴. 숫자가 클수록 black에 가깝고, 작을수록 white에 가까움 12345678fig, axes = plt.subplots(nrows=2, ncols=5, sharex=True, figsize=(12, 6), sharey=True)for i in range(10): axes[i//5][i%5].imshow(X[i], cmap='Blues') axes[i//5][i%5].set_title(str(i), fontsize=20) plt.tight_layout()plt.show() document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 3. Matplotlib","slug":"【STUDY-Python】/Python-3-Matplotlib","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-3-Matplotlib/"},{"name":"Python - 시각화","slug":"【STUDY-Python】/Python-시각화","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%8B%9C%EA%B0%81%ED%99%94/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Matplotlib","slug":"Matplotlib","permalink":"https://hyemin-kim.github.io/tags/Matplotlib/"},{"name":"시각화","slug":"시각화","permalink":"https://hyemin-kim.github.io/tags/%EC%8B%9C%EA%B0%81%ED%99%94/"}]},{"title":"Python >> Matplotlib - (1) 기본 canvas 그리기 및 스타일링","slug":"S-Python-Matplotlib1","date":"2020-06-28T05:12:24.000Z","updated":"2020-11-06T05:18:22.753Z","comments":true,"path":"2020/06/28/S-Python-Matplotlib1/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/28/S-Python-Matplotlib1/","excerpt":"","text":"기본적인 canvas 그리기 및 스타일링 1. 밑 그림 그리기 1-1. 단일 그래프 (single graph) 1-2. 다중 그래프 (multiple graphs) 1-3. 그래프 배열 (subplot / subplots) 2. 주요 스타일 옵션 2-1. 타이틀 2-2. X, Y축 Label 설정 2-3. X, Y축 Tick 조정 (rotation) 2-4. 범례 (Legend) 설정 2-5. X와 Y의 한계점(Limit) 설정 2-6. 스타일 세부 설정 - 마커, 라인, 컬러 2-7. 그리드 (grid) 설정 reference: pyplot 공식 Document 살펴보기 123import matplotlib.pyplot as pltimport pandas as pdimport numpy as np 1# plt.rcParams[\"figure.figsize\"] = (12, 9) # figure size 설정 1. 밑 그림 그리기 1-1. 단일 그래프 (single graph) plt.plot(df_name) plt.show() 12345678## data 생성data = np.arange(1, 100)## plotplt.plot(data)## 그래프만 보여주는 코드 (타 실행 결과 생략하고 그래프만 보여줌)plt.show() 1-2. 다중 그래프 (multiple graphs) 여러 그래프를 같은 canvas 안에 그리기: 명령어 plt.plot(df_name) 를 연속 사용 새 그래프를 새로운 canvas 안에 그리기: 세 그래프를 그리기 전에 plt.figure()명령어를 추가 (1) 1개의 canvas 안에 다중 그래프 그리기 123456789data1 = np.arange(1, 51)data2 = np.arange(51, 101)plt.plot(data1)plt.plot(data2)plt.plot(data2 + 50)plt.show() (2) 새로운 canvas에서 새 그래프를 그리기 figure()는 새로운 그래프 canvas를 생성한다 12345678910data1 = np.arange(100, 201)data2 = np.arange(200, 301)plt.plot(data)plt.figure() # figure() 명령어를 추가plt.plot(data2)plt.plot(data2 + 50)plt.show() 1-3. 그래프 배열 (subplot / subplots) 여러 개 plot을 지정된 행,열수에 따라 배열해주기: plt.subplot(row, column, index) # 각 plot의 좌표 설정 plt.subplots(행의 갯수, 열의 갯수) # 행,열수 설정 (1) subplot (plot의 좌표를 설정하기) 이 방법은 그래프마다 설정해줘야 함 plt.subplot(row, column, index) # 콤마를 제거해도 됨 123456789data1 = np.arange(100, 201)plt.subplot(2, 1, 1)plt.plot(data1)data2 = np.arange(200, 301)plt.subplot(2, 1, 2)plt.plot(data2)plt.show() 위의 코드와 동일하나, \"콤마\"를 제거한 상태 123456789data1 = np.arange(100, 201)plt.subplot(211) # 콤마를 생략함: 211 -&gt; row : 2, col: 1, index : 1plt.plot(data1)data2 = np.arange(200, 301)plt.subplot(212) # 콤마를 생략함plt.plot(data2)plt.show() 12345678910111213data1 = np.arange(100, 201)plt.subplot(1, 3, 1)plt.plot(data1)data2 = np.arange(200, 301)plt.subplot(1, 3, 2)plt.plot(data2)data3 = np.arange(300, 401)plt.subplot(1, 3, 3)plt.plot(data3)plt.show() (2) subplots (배열 기준인 행,열수를 지정하기) subplot와 다르게 subplots()명령어는 한번만 설정해주면 됨 plt.subplots(행의 갯수, 열의 갯수) 123456789101112131415data = np.arange(1, 51)# 밑 그림fig, axes = plt.subplots(2, 3)# plotaxes[0, 0].plot(data)axes[0, 1].plot(data * data)axes[0, 2].plot(data ** 3) # data^3axes[1, 0].plot(data % 10)axes[1, 1].plot(-data)axes[1, 2].plot(data // 20)plt.tight_layout()plt.show() 2. 주요 스타일 옵션 1234from IPython.display import Image# 출처: matplotlib.orgImage('https://matplotlib.org/_images/anatomy.png') 2-1. 타이틀 타이틀 추가: plt.title(\"…\") 타이틀 fontsize 설정: plt.title(\"…\", fontsize = … ) 1234plt.plot([1,2,3], [3,6,9])plt.plot([1,2,3], [2,4,9])plt.title(\"이것은 타이틀 입니다\") Text(0.5, 1.0, '이것은 타이틀 입니다') 1234plt.plot([1,2,3], [3,6,9])plt.plot([1,2,3], [2,4,9])plt.title(\"타이틀 fontsize를 키웁니다\", fontsize = 20) Text(0.5, 1.0, '타이틀 fontsize를 키웁니다') 2-2. X, Y축 Label 설정 plt.xlabel ( “x_name”, fontsize = …) plt.ylabel ( “y_name”, fontsize = …) 123456789plt.plot([1,2,3], [3,6,9])plt.plot([1,2,3], [2,4,9])# 타이틀 설정plt.title(\"Label 설정 예제\", fontsize = 16)# X축 &amp; Y축 Label 설정plt.xlabel(\"X축\", fontsize = 16)plt.ylabel(\"Y축\", fontsize = 16) Text(0, 0.5, 'Y축') 2-3. X, Y축 Tick 조정 (rotation) Tick은 X, Y축에 위치한 눈금을 말한다 rotation 명령어를 통해 Tick의 각도를 조절할 수 있다 plt.xticks ( rotation = … ) plt.yticks ( rotation = … ) Rotation 각도는 역시개방향 회전각도를 말한다 1234567891011121314plt.plot(np.arange(10), np.arange(10)*2)plt.plot(np.arange(10), np.arange(10)**2)plt.plot(np.arange(10), np.log(np.arange(10)))# titleplt.title(\"X, Y축 Tick 조정\", fontsize = 16)# X축, Y축 Label 설정plt.xlabel(\"X축\", fontsize = 16)plt.ylabel(\"Y축\", fontsize = 16)# X tick, Y tick rotation 조정plt.xticks(rotation = 90)plt.yticks(rotation = 30) D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log This is separate from the ipykernel package so we can avoid doing imports until (array([-10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90.]), &lt;a list of 11 Text yticklabel objects&gt;) 2-4. 범례 (Legend) 설정 plt.legend ( [ “name1” , “name2” , … ], fontsize = …) 1234567891011121314151617plt.plot(np.arange(10), np.arange(10)*2)plt.plot(np.arange(10), np.arange(10)**2)plt.plot(np.arange(10), np.log(np.arange(10)))# titleplt.title(\"범례(Legend) 설정\", fontsize = 16)# X축, Y축 Label 설정plt.xlabel(\"X축\", fontsize = 16)plt.ylabel(\"Y축\", fontsize = 16)# X tick, Y tick rotation 조정plt.xticks(rotation = 90)plt.yticks(rotation = 30)# legend 설정plt.legend([\"2x\", \"x^2\", \"logx\"], fontsize = 14) D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log This is separate from the ipykernel package so we can avoid doing imports until &lt;matplotlib.legend.Legend at 0x173a5712888&gt; 2-5. X와 Y의 한계점(Limit) 설정 plt.xlim ( a, b ) plt.ylim ( c, d ) 123456789101112131415161718192021plt.plot(np.arange(10), np.arange(10)*2)plt.plot(np.arange(10), np.arange(10)**2)plt.plot(np.arange(10), np.log(np.arange(10)))# titleplt.title(\"X축, Y축 Limit 설정\", fontsize = 16)# X축, Y축 Label 설정plt.xlabel(\"X축\", fontsize = 16)plt.ylabel(\"Y축\", fontsize = 16)# X tick, Y tick rotation 조정plt.xticks(rotation = 90)plt.yticks(rotation = 30)# legend 설정plt.legend([\"2x\", \"x^2\", \"logx\"], fontsize = 14)# x, y limit 설정plt.xlim(0, 5)plt.ylim(0, 20) D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log This is separate from the ipykernel package so we can avoid doing imports until (0, 20) 2-6. 스타일 세부 설정 - 마커, 라인, 컬러 reference: 세부 Document 확인하기 스타일 세부 설정은 마커, 선의 동류 설정, 드리고 컬러가 있으며, 문다열로 세부설정을 하게 된다 (1) marker의 종류 ‘.’ point marker ‘,’ pixel marker ‘o’ circle marker ‘v’ triangle_down marker ‘^’ triangle_up marker ‘&lt;’ triangle_left marker ‘&gt;’ triangle_right marker ‘1’ tri_down marker ‘2’ tri_up marker ‘3’ tri_left marker ‘4’ tri_right marker 's ’ square marker ‘p’ pentagon marker ‘*’ star marker ‘h’ hexagon1 marker ‘H’ hexagon2 marker ‘+’ plus marker ‘x’ x marker ‘D’ diamond marker ‘d’ thin_diamond marker ‘|’ vline marker ‘_’ hline marker 123456789101112# marker 스타일 설정plt.plot(np.arange(10), np.arange(10)*2, marker='o', markersize=5)plt.plot(np.arange(10), np.arange(10)*2 - 10, marker='v', markersize=10)plt.plot(np.arange(10), np.arange(10)*2 - 20, marker='+', markersize=15)plt.plot(np.arange(10), np.arange(10)*2 - 30, marker='*', markersize=20)# 타이틀 &amp; font 설정plt.title('마커 스타일 예제', fontsize=20)# X축 &amp; Y축 Label 설정plt.xlabel('X축', fontsize=20)plt.ylabel('Y축', fontsize=20) Text(0, 0.5, 'Y축') (2) line의 종류 ‘-’ solid line style ‘–’ dashed line style ‘-.’ dash-dot line style ‘:’ dotted line style 12345678910111213# line 스타일 설정plt.plot(np.arange(10), np.arange(10)*2, marker='o', linestyle='')plt.plot(np.arange(10), np.arange(10)*2 - 10, marker='o', linestyle='-')plt.plot(np.arange(10), np.arange(10)*2 - 20, marker='v', linestyle='--')plt.plot(np.arange(10), np.arange(10)*2 - 30, marker='+', linestyle='-.')plt.plot(np.arange(10), np.arange(10)*2 - 40, marker='*', linestyle=':')# 타이틀 &amp; font 설정plt.title('다양한 선의 종류 예제', fontsize=20)# X축 &amp; Y축 Label 설정plt.xlabel('X축', fontsize=20)plt.ylabel('Y축', fontsize=20) Text(0, 0.5, 'Y축') (3) color의 종류 ‘b’ blue ‘g’ green ‘r’ red ‘c’ cyan ‘m’ magenta ‘y’ yellow ‘k’ black ‘w’ white more choices: matplotlib.colors (e.g. “purple”, “#008000”) 123456789101112# color 설정plt.plot(np.arange(10), np.arange(10)*2, marker='o', linestyle='-', color='b')plt.plot(np.arange(10), np.arange(10)*2 - 10, marker='v', linestyle='--', color='c')plt.plot(np.arange(10), np.arange(10)*2 - 20, marker='+', linestyle='-.', color='y')plt.plot(np.arange(10), np.arange(10)*2 - 30, marker='*', linestyle=':', color='r')# 타이틀 &amp; font 설정plt.title('색상 설정 예제', fontsize=20)# X축 &amp; Y축 Label 설정plt.xlabel('X축', fontsize=20)plt.ylabel('Y축', fontsize=20) Text(0, 0.5, 'Y축') (4) Format: '[marker][line][color]' example: ‘b’ # blue markers with default shape ‘or’ # red circles ‘-g’ # green solid line ‘–’ # dashed line with default color ‘^k:’ # black triangle_up markers connected by a dotted line Each of them is optional. If not provided, the value from the style cycle is used. Exception: If line is given, but no marker, the data will be a line without markers. 123456789101112# \"marker + line + color\" format 설정plt.plot(np.arange(10), np.arange(10)*2, \"o-r\")plt.plot(np.arange(10), np.arange(10)*2 - 10, 'v--b')plt.plot(np.arange(10), np.arange(10)*2 - 20, '+y')plt.plot(np.arange(10), np.arange(10)*2 - 30, ':k')# 타이틀 &amp; font 설정plt.title('marker/line + color 설정 예제', fontsize=20)# X축 &amp; Y축 Label 설정plt.xlabel('X축', fontsize=20)plt.ylabel('Y축', fontsize=20) Text(0, 0.5, 'Y축') (5) 색상 투명도 설정 alpha = … (0.0 ~ 1.0) 123456789101112# color 투명도 설정plt.plot(np.arange(10), np.arange(10)*2, color='b', alpha=0.1)plt.plot(np.arange(10), np.arange(10)*2 - 10, color='b', alpha=0.3)plt.plot(np.arange(10), np.arange(10)*2 - 20, color='b', alpha=0.6)plt.plot(np.arange(10), np.arange(10)*2 - 30, color='b', alpha=1.0)# 타이틀 &amp; font 설정plt.title('투명도 (alpha) 설정 예제', fontsize=20)# X축 &amp; Y축 Label 설정plt.xlabel('X축', fontsize=20)plt.ylabel('Y축', fontsize=20) Text(0, 0.5, 'Y축') 2-7. 그리드 (grid) 설정 그리드 (grid) 추가: plt.grid() 1234567891011121314plt.plot(np.arange(10), np.arange(10)*2, marker='o', linestyle='-', color='b')plt.plot(np.arange(10), np.arange(10)*2 - 10, marker='v', linestyle='--', color='c')plt.plot(np.arange(10), np.arange(10)*2 - 20, marker='+', linestyle='-.', color='y')plt.plot(np.arange(10), np.arange(10)*2 - 30, marker='*', linestyle=':', color='r')# 타이틀 &amp; font 설정plt.title('그리드 설정 예제', fontsize=20)# X축 &amp; Y축 Label 설정plt.xlabel('X축', fontsize=20)plt.ylabel('Y축', fontsize=20)# grid 옵션 추가plt.grid() document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 3. Matplotlib","slug":"【STUDY-Python】/Python-3-Matplotlib","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-3-Matplotlib/"},{"name":"Python - 시각화","slug":"【STUDY-Python】/Python-시각화","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%8B%9C%EA%B0%81%ED%99%94/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Matplotlib","slug":"Matplotlib","permalink":"https://hyemin-kim.github.io/tags/Matplotlib/"},{"name":"시각화","slug":"시각화","permalink":"https://hyemin-kim.github.io/tags/%EC%8B%9C%EA%B0%81%ED%99%94/"}]},{"title":"Python >> Matplotlib 개요","slug":"S-Python-Matplotlib0","date":"2020-06-28T05:11:58.000Z","updated":"2020-11-06T05:18:15.675Z","comments":true,"path":"2020/06/28/S-Python-Matplotlib0/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/28/S-Python-Matplotlib0/","excerpt":"","text":"시각화 library – [matplotlib] 개요 matplotlib: 파이썬 기반 시각화 라이브러리 1. 불러오기 2. matplotlib 주요 장점 3. matplotlib 주요 단점 4. matplotlib 웹사이트 matplotlib: 파이썬 기반 시각화 라이브러리 1. 불러오기 1import matplotlib.pyplot 1import matplotlib.pyplot as plt # alias 설정 pandas도 matplotlib을 내장 2. matplotlib 주요 장점 파이썬 표준 시각화 도구라고 불릴 만큼 다양한 기능 지원 세부 옵션을 통하여 아름다운 스타일링 가능 보다 다양한 그래프를 그릴 수 있음 pandas와 연동이 용이함 3. matplotlib 주요 단점 한글에 대한 완벽한 지원 X 한글 사용시 추가설정 필요 (설정방법은 [Python &gt;&gt; Pandas 시각화] 안의 [0.준비 - 한글폰트 깨짐현상 해결]을 참조) 세부 기능이 많으나, 사용성이 복잡하다고 느낄 수 있음 4. matplotlib 웹사이트 http://matplotlib.org/ 여거시 matplotlib의 Documents, Samples 들을 볼 수 있음 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 3. Matplotlib","slug":"【STUDY-Python】/Python-3-Matplotlib","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-3-Matplotlib/"},{"name":"Python - 시각화","slug":"【STUDY-Python】/Python-시각화","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%8B%9C%EA%B0%81%ED%99%94/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Matplotlib","slug":"Matplotlib","permalink":"https://hyemin-kim.github.io/tags/Matplotlib/"},{"name":"사각화","slug":"사각화","permalink":"https://hyemin-kim.github.io/tags/%EC%82%AC%EA%B0%81%ED%99%94/"}]},{"title":"Python >> Pandas 시각화","slug":"S-Python-Pandas-visual","date":"2020-06-25T05:09:37.000Z","updated":"2020-11-06T05:19:36.293Z","comments":true,"path":"2020/06/25/S-Python-Pandas-visual/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/25/S-Python-Pandas-visual/","excerpt":"","text":"Pandas - 데이터 시각화 0. 준비 – 한글폰트 깨짐현상 해결 1. Plot 그래프 line 그래프 bar 그래프 히스토그램 (hist) 커널 밀도 그래프 (kde) 고밀도 산점도 그래프 (hexbin) 박스 플롯 (box) area plot 파이 그래프 (pie plot) 산점도 그래프 (scatter plot) 1import pandas as pd 1df = pd.read_csv(\"house_price_clean.csv\") 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역 규모 연도 월 분양가 0 서울 60㎡이하 2015 10 5652 1 서울 60㎡초과 85㎡이하 2015 10 5882 2 서울 85㎡초과 102㎡이하 2015 10 5721 3 서울 102㎡초과 2015 10 5879 4 인천 60㎡이하 2015 10 3488 ... ... ... ... ... ... 3288 경남 60㎡초과 85㎡이하 2020 2 3065 3289 경남 85㎡초과 102㎡이하 2020 2 3247 3290 제주 60㎡이하 2020 2 4039 3291 제주 60㎡초과 85㎡이하 2020 2 3962 3292 제주 102㎡초과 2020 2 3601 3293 rows × 5 columns 0. 준비 – 한글폰트 깨짐현상 해결 reference: 주피터 노트북(Jupyter notebook) - Matplotlib 한글 깨짐 현상 해결 matplotlib/seaborn으로 시각화할 때 한글 폰트 깨짐현상 해결방법 Jupyter Notebook에서 그래프를 그릴 때 한글 깨짐 현상이 발생한다 1df.plot() &lt;matplotlib.axes._subplots.AxesSubplot at 0x179eb070ac8&gt; 우리는 설정 파일을 수정하여 한글 폰트를 영구 등록함으로써 이 문제를 해결할 수 있다 (1) 설정 파일 위치 찾기 1234import matplotlib as mpl#font 설정 파일 위치 출력mpl.matplotlib_fname() 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\matplotlibrc' (2) 설정 파일 수정하기 맨 마지막 matplotlibrc 는 우리가 수정해야할 파일의 이름이다 step 1. 한글 폰트 적용 수정전: # font.family : sans-serif 수정후: font.family : Malgun Gothic step 2. minus 깨짐 방지 수정전: # axes.unicode_minus : True ## use unicode for the minus symbol 수정후: axes.unicode_minus : False ## use unicode for the minus symbol (3) Tip: 전역으로 시각화 figsize 조절 12import matplotlib.pyplot as pltplt.rcParams['figure.figsize'] = (8, 5) 설정을 완료한 후 jupyter notebook의 kernel을 리셋하고 다시 그래프를 그리면, 한글폰트가 깨지지 않고 잘 출력되는 것을 확인하실 수 있다. 1df.plot() &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f01b0c48&gt; 1. Plot 그래프 df_name [ col_name ] .plot ( kind = ‘…’ ) plot은 일반 선그래프를 나타난다 kind 옵션을 통해 원하는 그패프를 그릴 수 있다 kind 옵션: line: 선 그래프 bar: 바 그래프 barh: 수평 바 프래프 hist: 히스토르램 kde: 커널 밀도 그래프 hexbin: 고밀도 산점도 그래프 box: 박스 플롯 area: 면적 그래프 pie: 파이 그래프 scatter: 산점도 그래프 line 그래프 line 그래프는 데이터가 연속적인 경우 사용하기 적절하다. (예를 들면, 주가 데이터) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역 규모 연도 월 분양가 0 서울 60㎡이하 2015 10 5652 1 서울 60㎡초과 85㎡이하 2015 10 5882 2 서울 85㎡초과 102㎡이하 2015 10 5721 3 서울 102㎡초과 2015 10 5879 4 인천 60㎡이하 2015 10 3488 ... ... ... ... ... ... 3288 경남 60㎡초과 85㎡이하 2020 2 3065 3289 경남 85㎡초과 102㎡이하 2020 2 3247 3290 제주 60㎡이하 2020 2 4039 3291 제주 60㎡초과 85㎡이하 2020 2 3962 3292 제주 102㎡초과 2020 2 3601 3293 rows × 5 columns (1) 모든 observation의 분양가 살펴보기 12# index - 분양가df[\"분양가\"].plot(kind = 'line') &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f01a0bc8&gt; (2) 연도에 따른 서울 분양가 변화 추세 123# select \"서울\" datadf_seoul = df.loc[df[\"지역\"] == \"서울\"]df_seoul .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역 규모 연도 월 분양가 0 서울 60㎡이하 2015 10 5652 1 서울 60㎡초과 85㎡이하 2015 10 5882 2 서울 85㎡초과 102㎡이하 2015 10 5721 3 서울 102㎡초과 2015 10 5879 64 서울 60㎡이하 2015 11 6320 ... ... ... ... ... ... 3178 서울 102㎡초과 2020 1 8779 3234 서울 60㎡이하 2020 2 8193 3235 서울 60㎡초과 85㎡이하 2020 2 8140 3236 서울 85㎡초과 102㎡이하 2020 2 13835 3237 서울 102㎡초과 2020 2 9039 212 rows × 5 columns 123# group by \"year\" df_seoul_year = df_seoul.groupby('연도').mean()df_seoul_year .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 월 분양가 연도 2015 11.0 6201.000000 2016 6.5 6674.520833 2017 6.5 6658.729167 2018 6.5 7054.687500 2019 6.5 8735.083333 2020 1.5 9647.375000 12# line plotdf_seoul_year[\"분양가\"].plot(kind = 'line') &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f028b5c8&gt; bar 그래프 bar 그패프는 그룹별로 비교할 때 유용하다 지역별 평균 분양가 살펴보기 1df.groupby(\"지역\")[\"분양가\"].mean() 지역 강원 2448.156863 경기 4133.952830 경남 2858.932367 경북 2570.465000 광주 3055.043750 대구 3679.620690 대전 3176.127389 부산 3691.981132 서울 7308.943396 세종 2983.543147 울산 2990.373913 인천 3684.302885 전남 2326.250000 전북 2381.416268 제주 3472.677966 충남 2534.950000 충북 2348.183962 Name: 분양가, dtype: float64 12# 수직 바 그래프df.groupby(\"지역\")[\"분양가\"].mean().plot(kind = 'bar') &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f028b548&gt; 12# 수평 바 그래프df.groupby(\"지역\")[\"분양가\"].mean().plot(kind = 'barh') &lt;matplotlib.axes._subplots.AxesSubplot at 0x179edd9d4c8&gt; 히스토그램 (hist) 히스토그램은 분포-빈도 를 시각화하여 보여준다. 가로축에는 분포를, 세로축에는 빈도가 시각화되어 보여짐. 1df[\"분양가\"].plot(kind = \"hist\") &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f021cc88&gt; 커널 밀도 그래프 (kde) 히스토그램과 유사하게 밀도를 보여주는 그래프다 히스토그램과 유사한 모양새를 각추고 있다 하지만 히스토그램과 다르게 부드러운 라인을 가지고 있다 1df[\"분양가\"].plot(kind = \"kde\") &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f043d608&gt; 고밀도 산점도 그래프 (hexbin) hexbin은 고밀고 산점도 그래프다 x와 y 키 값을 넣어 주어야 한다 x, y 값 모두 numeric value 이어야한다 데이터의 밀도를 추정한다 1df.plot(kind = \"hexbin\", x = \"분양가\", y = \"연도\", gridsize = 20) &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f028a9c8&gt; 박스 플롯 (box) 1df_seoul = df.loc[df[\"지역\"] == \"서울\"] 1df_seoul[\"분양가\"].plot(kind = \"box\") &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f226d748&gt; box plot 해석 IQR (Inter Quantile Range) = 3Q - 1Q Upper fence = 75th Percentile + 1.5*IQR Lower fence = 25th Percentile - 1.5*IQR box plot은 데이터 outlier 감지할 때 가장 많이 활용되며, 25%, median, 75% 분위값을 활용하는 용도로도 많이 활용된다 area plot area plot은 line 그래프에서 아래 area를 모두 색칠해 주는 것이 특징이다. 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역 규모 연도 월 분양가 0 서울 60㎡이하 2015 10 5652 1 서울 60㎡초과 85㎡이하 2015 10 5882 2 서울 85㎡초과 102㎡이하 2015 10 5721 3 서울 102㎡초과 2015 10 5879 4 인천 60㎡이하 2015 10 3488 ... ... ... ... ... ... 3288 경남 60㎡초과 85㎡이하 2020 2 3065 3289 경남 85㎡초과 102㎡이하 2020 2 3247 3290 제주 60㎡이하 2020 2 4039 3291 제주 60㎡초과 85㎡이하 2020 2 3962 3292 제주 102㎡초과 2020 2 3601 3293 rows × 5 columns 1df.groupby(\"월\")[\"분양가\"].count().plot(kind = \"line\") &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f22a6688&gt; 1df.groupby(\"월\")[\"분양가\"].count().plot(kind = \"area\") &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f2267588&gt; 파이 그래프 (pie plot) pie는 대표적으로 데이터의 점유율을 보유줄 때 유용하다 연도별 분양가 데이터 점유율 1df.groupby(\"연도\")[\"분양가\"].count().plot(kind = 'pie') &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f224fec8&gt; 산점도 그래프 (scatter plot) 점으로 데이터를 표기해준다 x, y값을 넣어주어야한다 (hexbin과 유사) x축과 y축을 지정해주면 그에 맞는 데이터 분포를 볼 수 있다 역시 numeric column 만 지정할 수 있다 1df.plot(x = \"월\", y = \"분양가\", kind = \"scatter\") &lt;matplotlib.axes._subplots.AxesSubplot at 0x179f23372c8&gt; document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"},{"name":"Python - 시각화","slug":"【STUDY-Python】/Python-시각화","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%8B%9C%EA%B0%81%ED%99%94/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"시각화","slug":"시각화","permalink":"https://hyemin-kim.github.io/tags/%EC%8B%9C%EA%B0%81%ED%99%94/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"}]},{"title":"【실습】 Python >> Pandas 전처리 -- 부동산 데이터","slug":"E-Python-Pandas-Pre-1","date":"2020-06-22T10:14:57.000Z","updated":"2020-10-28T06:49:55.723Z","comments":true,"path":"2020/06/22/E-Python-Pandas-Pre-1/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/22/E-Python-Pandas-Pre-1/","excerpt":"","text":"&lt;Pandas 전처리&gt; 실습 – 부동산 데이터 0. 샘플데이터 1. column 이름 제정의 (rename) 2. Data Overview 2-1. Data Shape 확인하기 2-2. 걸측값과 Data Type 확인하기 2-3. 통계값 확인하기 3. 데이터 타입 변환 3-1. str.strip()을 활용하여 공백이 있는 데이터의 공백 없애기 3-2. 빈 공백에 0을 넣어주기 3-3. NaN 값은 fillna로 채워주기 3-4. str.replace() 를 활용하여 콤마를 제거하기 3-5. str.replace()를 활용하여 “-” 제거하기 3-6. 규모구분 column에 불필요한 “전용면적” 제거하기 4. 전처리 내용 복습하기 5. 지역별 분양가격을 확인해보기 5-1. 지역별 평균 분양가격 확인해보기 5-2. 분양가격이 100보다 작은 행을 제거해보기 5-3. 지역별 “분양가격” 데이터의 갯수를 확인해보기 5-4. 지역별 제일 비싼 분양가를 확인해보기 6. 연도별 평균 분양가격을 확인해보기 7. 피벗테이블 활용하기 8. 연도별, 규모별 가격을 알아보기 1import pandas as pd 0. 샘플데이터 공공데이터포털 에서 제공하는 공공데이터 “민간 아파트 가격동향” 를 활용한다. 1df = pd.read_csv(\"seoul_house_price.csv\") 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격(㎡) 0 서울 전체 2015 10 5841 1 서울 전용면적 60㎡이하 2015 10 5652 2 서울 전용면적 60㎡초과 85㎡이하 2015 10 5882 3 서울 전용면적 85㎡초과 102㎡이하 2015 10 5721 4 서울 전용면적 102㎡초과 2015 10 5879 ... ... ... ... ... ... 4500 제주 전체 2020 2 3955 4501 제주 전용면적 60㎡이하 2020 2 4039 4502 제주 전용면적 60㎡초과 85㎡이하 2020 2 3962 4503 제주 전용면적 85㎡초과 102㎡이하 2020 2 NaN 4504 제주 전용면적 102㎡초과 2020 2 3601 4505 rows × 5 columns 1. column 이름 제정의 (rename) [목표] 분양가격 column의 이름을 재정의: “분양가격(m2)​” --&gt; “분양가격” 1df = df.rename(columns = {\"분양가격(㎡)\" : \"분양가격\"}) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 0 서울 전체 2015 10 5841 1 서울 전용면적 60㎡이하 2015 10 5652 2 서울 전용면적 60㎡초과 85㎡이하 2015 10 5882 3 서울 전용면적 85㎡초과 102㎡이하 2015 10 5721 4 서울 전용면적 102㎡초과 2015 10 5879 ... ... ... ... ... ... 4500 제주 전체 2020 2 3955 4501 제주 전용면적 60㎡이하 2020 2 4039 4502 제주 전용면적 60㎡초과 85㎡이하 2020 2 3962 4503 제주 전용면적 85㎡초과 102㎡이하 2020 2 NaN 4504 제주 전용면적 102㎡초과 2020 2 3601 4505 rows × 5 columns 2. Data Overview 2-1. Data Shape 확인하기 1df.shape (4505, 5) 2-2. 걸측값과 Data Type 확인하기 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 4505 entries, 0 to 4504 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 지역명 4505 non-null object 1 규모구분 4505 non-null object 2 연도 4505 non-null int64 3 월 4505 non-null int64 4 분양가격 4210 non-null object dtypes: int64(2), object(3) memory usage: 176.1+ KB 2-3. 통계값 확인하기 1df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 연도 월 count 4505.000000 4505.000000 mean 2017.452830 6.566038 std 1.311432 3.595519 min 2015.000000 1.000000 25% 2016.000000 3.000000 50% 2017.000000 7.000000 75% 2019.000000 10.000000 max 2020.000000 12.000000 3. 데이터 타입 변환 [목표] &lt;object 타입&gt;으로 되어있는 \"분양가격\"을 &lt;int 타입&gt;으로 변환하기 1df[\"분양가격\"].astype(int) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-193-5870dcdf031c&gt; in &lt;module&gt; ----&gt; 1 df[\"분양가격\"].astype(int) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py in astype(self, dtype, copy, errors) 5696 else: 5697 # else, only a single dtype is given -&gt; 5698 new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors) 5699 return self._constructor(new_data).__finalize__(self) 5700 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in astype(self, dtype, copy, errors) 580 581 def astype(self, dtype, copy: bool = False, errors: str = \"raise\"): --&gt; 582 return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors) 583 584 def convert(self, **kwargs): D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in apply(self, f, filter, **kwargs) 440 applied = b.apply(f, **kwargs) 441 else: --&gt; 442 applied = getattr(b, f)(**kwargs) 443 result_blocks = _extend_blocks(applied, result_blocks) 444 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py in astype(self, dtype, copy, errors) 623 vals1d = values.ravel() 624 try: --&gt; 625 values = astype_nansafe(vals1d, dtype, copy=True) 626 except (ValueError, TypeError): 627 # e.g. astype_nansafe can fail on object-dtype of strings D:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py in astype_nansafe(arr, dtype, copy, skipna) 872 # work around NumPy brokenness, #1987 873 if np.issubdtype(dtype.type, np.integer): --&gt; 874 return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape) 875 876 # if we have a datetime/timedelta array of objects pandas\\_libs\\lib.pyx in pandas._libs.lib.astype_intsafe() ValueError: invalid literal for int() with base 10: ' ' !! “분양가격” column에 “2칸 공백” 값이 있어서 Error가 납니다 3-1. str.strip()을 활용하여 공백이 있는 데이터의 공백 없애기 df_name [ “col_name” ] .str.strip() 1df.loc[df[\"분양가격\"] == ' '] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 28 광주 전용면적 85㎡초과 102㎡이하 2015 10 29 광주 전용면적 102㎡초과 2015 10 34 대전 전용면적 102㎡초과 2015 10 81 제주 전용면적 60㎡이하 2015 10 113 광주 전용면적 85㎡초과 102㎡이하 2015 11 114 광주 전용면적 102㎡초과 2015 11 119 대전 전용면적 102㎡초과 2015 11 166 제주 전용면적 60㎡이하 2015 11 198 광주 전용면적 85㎡초과 102㎡이하 2015 12 199 광주 전용면적 102㎡초과 2015 12 204 대전 전용면적 102㎡초과 2015 12 251 제주 전용면적 60㎡이하 2015 12 283 광주 전용면적 85㎡초과 102㎡이하 2016 1 284 광주 전용면적 102㎡초과 2016 1 289 대전 전용면적 102㎡초과 2016 1 336 제주 전용면적 60㎡이하 2016 1 1df[\"분양가격\"] = df[\"분양가격\"].str.strip(' ') 1df.loc[df[\"분양가격\"] == \" \"] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 3-2. 빈 공백에 0을 넣어주기 1df.loc[df[\"분양가격\"] == '', \"분양가격\"] = 0 1df[\"분양가격\"].astype(int) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-198-5870dcdf031c&gt; in &lt;module&gt; ----&gt; 1 df[\"분양가격\"].astype(int) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py in astype(self, dtype, copy, errors) 5696 else: 5697 # else, only a single dtype is given -&gt; 5698 new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors) 5699 return self._constructor(new_data).__finalize__(self) 5700 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in astype(self, dtype, copy, errors) 580 581 def astype(self, dtype, copy: bool = False, errors: str = \"raise\"): --&gt; 582 return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors) 583 584 def convert(self, **kwargs): D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in apply(self, f, filter, **kwargs) 440 applied = b.apply(f, **kwargs) 441 else: --&gt; 442 applied = getattr(b, f)(**kwargs) 443 result_blocks = _extend_blocks(applied, result_blocks) 444 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py in astype(self, dtype, copy, errors) 623 vals1d = values.ravel() 624 try: --&gt; 625 values = astype_nansafe(vals1d, dtype, copy=True) 626 except (ValueError, TypeError): 627 # e.g. astype_nansafe can fail on object-dtype of strings D:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py in astype_nansafe(arr, dtype, copy, skipna) 872 # work around NumPy brokenness, #1987 873 if np.issubdtype(dtype.type, np.integer): --&gt; 874 return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape) 875 876 # if we have a datetime/timedelta array of objects pandas\\_libs\\lib.pyx in pandas._libs.lib.astype_intsafe() ValueError: cannot convert float NaN to integer !! “분양가격” column에 “NaN” 값이 있어서 Error가 또 납니다 ㅠㅠ 3-3. NaN 값은 fillna로 채워주기 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 0 서울 전체 2015 10 5841 1 서울 전용면적 60㎡이하 2015 10 5652 2 서울 전용면적 60㎡초과 85㎡이하 2015 10 5882 3 서울 전용면적 85㎡초과 102㎡이하 2015 10 5721 4 서울 전용면적 102㎡초과 2015 10 5879 ... ... ... ... ... ... 4500 제주 전체 2020 2 3955 4501 제주 전용면적 60㎡이하 2020 2 4039 4502 제주 전용면적 60㎡초과 85㎡이하 2020 2 3962 4503 제주 전용면적 85㎡초과 102㎡이하 2020 2 NaN 4504 제주 전용면적 102㎡초과 2020 2 3601 4505 rows × 5 columns 1df.loc[df[\"분양가격\"].isna()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 368 광주 전용면적 85㎡초과 102㎡이하 2016 2 NaN 369 광주 전용면적 102㎡초과 2016 2 NaN 374 대전 전용면적 102㎡초과 2016 2 NaN 388 강원 전용면적 85㎡초과 102㎡이하 2016 2 NaN 421 제주 전용면적 60㎡이하 2016 2 NaN ... ... ... ... ... ... 4461 세종 전용면적 60㎡이하 2020 2 NaN 4488 전남 전용면적 85㎡초과 102㎡이하 2020 2 NaN 4493 경북 전용면적 85㎡초과 102㎡이하 2020 2 NaN 4499 경남 전용면적 102㎡초과 2020 2 NaN 4503 제주 전용면적 85㎡초과 102㎡이하 2020 2 NaN 295 rows × 5 columns 1df[\"분양가격\"] = df[\"분양가격\"].fillna(0) 1df.loc[df[\"분양가격\"].isna()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 1df[\"분양가격\"].astype(int) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-203-5870dcdf031c&gt; in &lt;module&gt; ----&gt; 1 df[\"분양가격\"].astype(int) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py in astype(self, dtype, copy, errors) 5696 else: 5697 # else, only a single dtype is given -&gt; 5698 new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors) 5699 return self._constructor(new_data).__finalize__(self) 5700 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in astype(self, dtype, copy, errors) 580 581 def astype(self, dtype, copy: bool = False, errors: str = \"raise\"): --&gt; 582 return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors) 583 584 def convert(self, **kwargs): D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in apply(self, f, filter, **kwargs) 440 applied = b.apply(f, **kwargs) 441 else: --&gt; 442 applied = getattr(b, f)(**kwargs) 443 result_blocks = _extend_blocks(applied, result_blocks) 444 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py in astype(self, dtype, copy, errors) 623 vals1d = values.ravel() 624 try: --&gt; 625 values = astype_nansafe(vals1d, dtype, copy=True) 626 except (ValueError, TypeError): 627 # e.g. astype_nansafe can fail on object-dtype of strings D:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py in astype_nansafe(arr, dtype, copy, skipna) 872 # work around NumPy brokenness, #1987 873 if np.issubdtype(dtype.type, np.integer): --&gt; 874 return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape) 875 876 # if we have a datetime/timedelta array of objects pandas\\_libs\\lib.pyx in pandas._libs.lib.astype_intsafe() ValueError: invalid literal for int() with base 10: '6,657' !! 이번에는 \",\"가 들어간 데이터가 문제네요… 3-4. str.replace() 를 활용하여 콤마를 제거하기 1df.loc[df[\"분양가격\"] == \"6,657\"] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 2125 서울 전체 2017 11 6,657 1df[\"분양가격\"] = df[\"분양가격\"].str.replace(',', '') 1df[\"분양가격\"].astype(int) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-206-5870dcdf031c&gt; in &lt;module&gt; ----&gt; 1 df[\"분양가격\"].astype(int) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py in astype(self, dtype, copy, errors) 5696 else: 5697 # else, only a single dtype is given -&gt; 5698 new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors) 5699 return self._constructor(new_data).__finalize__(self) 5700 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in astype(self, dtype, copy, errors) 580 581 def astype(self, dtype, copy: bool = False, errors: str = \"raise\"): --&gt; 582 return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors) 583 584 def convert(self, **kwargs): D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in apply(self, f, filter, **kwargs) 440 applied = b.apply(f, **kwargs) 441 else: --&gt; 442 applied = getattr(b, f)(**kwargs) 443 result_blocks = _extend_blocks(applied, result_blocks) 444 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py in astype(self, dtype, copy, errors) 623 vals1d = values.ravel() 624 try: --&gt; 625 values = astype_nansafe(vals1d, dtype, copy=True) 626 except (ValueError, TypeError): 627 # e.g. astype_nansafe can fail on object-dtype of strings D:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py in astype_nansafe(arr, dtype, copy, skipna) 872 # work around NumPy brokenness, #1987 873 if np.issubdtype(dtype.type, np.integer): --&gt; 874 return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape) 875 876 # if we have a datetime/timedelta array of objects pandas\\_libs\\lib.pyx in pandas._libs.lib.astype_intsafe() ValueError: cannot convert float NaN to integer !! 다시 NaN값이 생겨서 fillna로 채워줍니다. 1df[\"분양가격\"] = df[\"분양가격\"].fillna(0) 1df[\"분양가격\"].astype(int) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-208-5870dcdf031c&gt; in &lt;module&gt; ----&gt; 1 df[\"분양가격\"].astype(int) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py in astype(self, dtype, copy, errors) 5696 else: 5697 # else, only a single dtype is given -&gt; 5698 new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors) 5699 return self._constructor(new_data).__finalize__(self) 5700 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in astype(self, dtype, copy, errors) 580 581 def astype(self, dtype, copy: bool = False, errors: str = \"raise\"): --&gt; 582 return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors) 583 584 def convert(self, **kwargs): D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in apply(self, f, filter, **kwargs) 440 applied = b.apply(f, **kwargs) 441 else: --&gt; 442 applied = getattr(b, f)(**kwargs) 443 result_blocks = _extend_blocks(applied, result_blocks) 444 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py in astype(self, dtype, copy, errors) 623 vals1d = values.ravel() 624 try: --&gt; 625 values = astype_nansafe(vals1d, dtype, copy=True) 626 except (ValueError, TypeError): 627 # e.g. astype_nansafe can fail on object-dtype of strings D:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py in astype_nansafe(arr, dtype, copy, skipna) 872 # work around NumPy brokenness, #1987 873 if np.issubdtype(dtype.type, np.integer): --&gt; 874 return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape) 875 876 # if we have a datetime/timedelta array of objects pandas\\_libs\\lib.pyx in pandas._libs.lib.astype_intsafe() ValueError: invalid literal for int() with base 10: '-' !! 이번에는 \"-\"가 멀썽이네요… 3-5. str.replace()를 활용하여 “-” 제거하기 1df[\"분양가격\"] = df[\"분양가격\"].str.replace(\"-\", \"\") 1df.loc[df[\"분양가격\"] == \"\", \"분양가격\"] = 0 1df[\"분양가격\"].astype(int) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-211-5870dcdf031c&gt; in &lt;module&gt; ----&gt; 1 df[\"분양가격\"].astype(int) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py in astype(self, dtype, copy, errors) 5696 else: 5697 # else, only a single dtype is given -&gt; 5698 new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors) 5699 return self._constructor(new_data).__finalize__(self) 5700 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in astype(self, dtype, copy, errors) 580 581 def astype(self, dtype, copy: bool = False, errors: str = \"raise\"): --&gt; 582 return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors) 583 584 def convert(self, **kwargs): D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in apply(self, f, filter, **kwargs) 440 applied = b.apply(f, **kwargs) 441 else: --&gt; 442 applied = getattr(b, f)(**kwargs) 443 result_blocks = _extend_blocks(applied, result_blocks) 444 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py in astype(self, dtype, copy, errors) 623 vals1d = values.ravel() 624 try: --&gt; 625 values = astype_nansafe(vals1d, dtype, copy=True) 626 except (ValueError, TypeError): 627 # e.g. astype_nansafe can fail on object-dtype of strings D:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py in astype_nansafe(arr, dtype, copy, skipna) 872 # work around NumPy brokenness, #1987 873 if np.issubdtype(dtype.type, np.integer): --&gt; 874 return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape) 875 876 # if we have a datetime/timedelta array of objects pandas\\_libs\\lib.pyx in pandas._libs.lib.astype_intsafe() ValueError: cannot convert float NaN to integer 1df[\"분양가격\"] = df[\"분양가격\"].fillna(0) 1df[\"분양가격\"] = df[\"분양가격\"].astype(int) 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 4505 entries, 0 to 4504 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 지역명 4505 non-null object 1 규모구분 4505 non-null object 2 연도 4505 non-null int64 3 월 4505 non-null int64 4 분양가격 4505 non-null int32 dtypes: int32(1), int64(2), object(2) memory usage: 158.5+ KB 이제 드디어 “분양가격” column의 Type을 int로 성공적으로 바꿨습니다!!! 3-6. 규모구분 column에 불필요한 “전용면적” 제거하기 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 0 서울 전체 2015 10 5841 1 서울 전용면적 60㎡이하 2015 10 5652 2 서울 전용면적 60㎡초과 85㎡이하 2015 10 5882 3 서울 전용면적 85㎡초과 102㎡이하 2015 10 5721 4 서울 전용면적 102㎡초과 2015 10 5879 1df[\"규모구분\"] = df[\"규모구분\"].str.replace(\"전용면적\", \"\") 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 0 서울 전체 2015 10 5841 1 서울 60㎡이하 2015 10 5652 2 서울 60㎡초과 85㎡이하 2015 10 5882 3 서울 85㎡초과 102㎡이하 2015 10 5721 4 서울 102㎡초과 2015 10 5879 4. 전처리 내용 복습하기 방급 진행 했던 전처리 과정을 복습해봅시다! 1df2 = pd.read_csv(\"seoul_house_price.csv\") (1) 콤마가 있는 경우 df_name [ “col_name” ] .str.replace (’,’, ‘’) 1df2.iloc[2125] 지역명 서울 규모구분 전체 연도 2017 월 11 분양가격(㎡) 6,657 Name: 2125, dtype: object 1df2 = df2.rename(columns = {\"분양가격(㎡)\" : \"분양가격\"}) 1df2[\"분양가격\"] = df2[\"분양가격\"].str.replace(\",\", \"\") 1df2.iloc[2125] 지역명 서울 규모구분 전체 연도 2017 월 11 분양가격 6657 Name: 2125, dtype: object (2) - 가 있는 경우 df_name [ “col_name” ] **.str.replace(’-’, ‘’) 1df2.loc[df2[\"분양가격\"] == \"-\"] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 3683 광주 전용면적 85㎡초과 102㎡이하 2019 5 - 3686 대전 전용면적 60㎡이하 2019 5 - 3688 대전 전용면적 85㎡초과 102㎡이하 2019 5 - 3690 울산 전체 2019 5 - 3691 울산 전용면적 60㎡이하 2019 5 - 3692 울산 전용면적 60㎡초과 85㎡이하 2019 5 - 3693 울산 전용면적 85㎡초과 102㎡이하 2019 5 - 3694 울산 전용면적 102㎡초과 2019 5 - 3696 세종 전용면적 60㎡이하 2019 5 - 1df2[\"분양가격\"] = df2[\"분양가격\"].str.replace(\"-\", \"\") 1df2.loc[df2[\"분양가격\"] == \"-\"] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 (3) 공백이 2개 들어간 경우 df_name [ “col_name” ] **.str.strip(\" \") 1df2.loc[df2[\"분양가격\"] == \" \"] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 28 광주 전용면적 85㎡초과 102㎡이하 2015 10 29 광주 전용면적 102㎡초과 2015 10 34 대전 전용면적 102㎡초과 2015 10 81 제주 전용면적 60㎡이하 2015 10 113 광주 전용면적 85㎡초과 102㎡이하 2015 11 114 광주 전용면적 102㎡초과 2015 11 119 대전 전용면적 102㎡초과 2015 11 166 제주 전용면적 60㎡이하 2015 11 198 광주 전용면적 85㎡초과 102㎡이하 2015 12 199 광주 전용면적 102㎡초과 2015 12 204 대전 전용면적 102㎡초과 2015 12 251 제주 전용면적 60㎡이하 2015 12 283 광주 전용면적 85㎡초과 102㎡이하 2016 1 284 광주 전용면적 102㎡초과 2016 1 289 대전 전용면적 102㎡초과 2016 1 336 제주 전용면적 60㎡이하 2016 1 1df2[\"분양가격\"] = df2[\"분양가격\"].str.strip(\" \") 1df2.loc[df2[\"분양가격\"] == \" \"] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 (4) 빈 칸을 0으로 채우기 df_name.loc [ df_name [ “col_name” ] == “” , “col_name”] = 0 1df2.loc[df2[\"분양가격\"] == \"\"] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 28 광주 전용면적 85㎡초과 102㎡이하 2015 10 29 광주 전용면적 102㎡초과 2015 10 34 대전 전용면적 102㎡초과 2015 10 81 제주 전용면적 60㎡이하 2015 10 113 광주 전용면적 85㎡초과 102㎡이하 2015 11 114 광주 전용면적 102㎡초과 2015 11 119 대전 전용면적 102㎡초과 2015 11 166 제주 전용면적 60㎡이하 2015 11 198 광주 전용면적 85㎡초과 102㎡이하 2015 12 199 광주 전용면적 102㎡초과 2015 12 204 대전 전용면적 102㎡초과 2015 12 251 제주 전용면적 60㎡이하 2015 12 283 광주 전용면적 85㎡초과 102㎡이하 2016 1 284 광주 전용면적 102㎡초과 2016 1 289 대전 전용면적 102㎡초과 2016 1 336 제주 전용면적 60㎡이하 2016 1 3683 광주 전용면적 85㎡초과 102㎡이하 2019 5 3686 대전 전용면적 60㎡이하 2019 5 3688 대전 전용면적 85㎡초과 102㎡이하 2019 5 3690 울산 전체 2019 5 3691 울산 전용면적 60㎡이하 2019 5 3692 울산 전용면적 60㎡초과 85㎡이하 2019 5 3693 울산 전용면적 85㎡초과 102㎡이하 2019 5 3694 울산 전용면적 102㎡초과 2019 5 3696 세종 전용면적 60㎡이하 2019 5 1df2.loc[df2[\"분양가격\"] == \"\", \"분양가격\"] = 0 1df2.loc[df2[\"분양가격\"] == \"\"] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 (5) NaN 값을 0으로 바꾸기 df_name.loc [ df_name [ “col_name” ] .isna() ] df_name [ “col_name” ].fillna(0) 1df2.loc[df2[\"분양가격\"].isna()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 368 광주 전용면적 85㎡초과 102㎡이하 2016 2 NaN 369 광주 전용면적 102㎡초과 2016 2 NaN 374 대전 전용면적 102㎡초과 2016 2 NaN 388 강원 전용면적 85㎡초과 102㎡이하 2016 2 NaN 421 제주 전용면적 60㎡이하 2016 2 NaN ... ... ... ... ... ... 4461 세종 전용면적 60㎡이하 2020 2 NaN 4488 전남 전용면적 85㎡초과 102㎡이하 2020 2 NaN 4493 경북 전용면적 85㎡초과 102㎡이하 2020 2 NaN 4499 경남 전용면적 102㎡초과 2020 2 NaN 4503 제주 전용면적 85㎡초과 102㎡이하 2020 2 NaN 295 rows × 5 columns 1df2[\"분양가격\"] = df2[\"분양가격\"].fillna(0) 1df2.loc[df2[\"분양가격\"].isna()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 (6) column type 바꾸기 df_name [ “col_name” ] .astype(…) 1df2[\"분양가격\"].astype(int) 0 5841 1 5652 2 5882 3 5721 4 5879 ... 4500 3955 4501 4039 4502 3962 4503 0 4504 3601 Name: 분양가격, Length: 4505, dtype: int32 5. 지역별 분양가격을 확인해보기 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 0 서울 전체 2015 10 5841 1 서울 60㎡이하 2015 10 5652 2 서울 60㎡초과 85㎡이하 2015 10 5882 3 서울 85㎡초과 102㎡이하 2015 10 5721 4 서울 102㎡초과 2015 10 5879 ... ... ... ... ... ... 4500 제주 전체 2020 2 3955 4501 제주 60㎡이하 2020 2 4039 4502 제주 60㎡초과 85㎡이하 2020 2 3962 4503 제주 85㎡초과 102㎡이하 2020 2 0 4504 제주 102㎡초과 2020 2 3601 4505 rows × 5 columns 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 4505 entries, 0 to 4504 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 지역명 4505 non-null object 1 규모구분 4505 non-null object 2 연도 4505 non-null int64 3 월 4505 non-null int64 4 분양가격 4505 non-null int32 dtypes: int32(1), int64(2), object(2) memory usage: 158.5+ KB 5-1. 지역별 평균 분양가격 확인해보기 1df.groupby(\"지역명\")[\"분양가격\"].mean() 지역명 강원 2339.807547 경기 4072.667925 경남 2761.275472 경북 2432.128302 광주 2450.728302 대구 3538.920755 대전 2479.135849 부산 3679.920755 서울 7225.762264 세종 2815.098113 울산 1826.101887 인천 3578.433962 전남 2270.177358 전북 2322.060377 제주 2979.407547 충남 2388.324528 충북 2316.871698 Name: 분양가격, dtype: float64 5-2. 분양가격이 100보다 작은 행을 제거해보기 특정 조건에 만족하는 행을 제거하고자 할 때는 index를 list로 가져온다 idx = df.loc [ 조건식 ] .index drop을 활용하여 행을 제거한다 df_name = df_name .drop (idx, axis = 0) 1idx = df.loc[df[\"분양가격\"] &lt; 100].index 1idx Int64Index([ 28, 29, 34, 81, 113, 114, 119, 166, 198, 199, ... 4418, 4448, 4453, 4458, 4459, 4461, 4488, 4493, 4499, 4503], dtype='int64', length=320) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 0 서울 전체 2015 10 5841 1 서울 60㎡이하 2015 10 5652 2 서울 60㎡초과 85㎡이하 2015 10 5882 3 서울 85㎡초과 102㎡이하 2015 10 5721 4 서울 102㎡초과 2015 10 5879 ... ... ... ... ... ... 4500 제주 전체 2020 2 3955 4501 제주 60㎡이하 2020 2 4039 4502 제주 60㎡초과 85㎡이하 2020 2 3962 4503 제주 85㎡초과 102㎡이하 2020 2 0 4504 제주 102㎡초과 2020 2 3601 4505 rows × 5 columns 1df = df.drop(idx, axis = 0) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 0 서울 전체 2015 10 5841 1 서울 60㎡이하 2015 10 5652 2 서울 60㎡초과 85㎡이하 2015 10 5882 3 서울 85㎡초과 102㎡이하 2015 10 5721 4 서울 102㎡초과 2015 10 5879 ... ... ... ... ... ... 4498 경남 85㎡초과 102㎡이하 2020 2 3247 4500 제주 전체 2020 2 3955 4501 제주 60㎡이하 2020 2 4039 4502 제주 60㎡초과 85㎡이하 2020 2 3962 4504 제주 102㎡초과 2020 2 3601 4185 rows × 5 columns 다시 한 번 지역명으로 group을 묶어 분양가격을 확인해보자! 1df.groupby(\"지역명\")[\"분양가격\"].mean() 지역명 강원 2412.642023 경기 4072.667925 경남 2814.376923 경북 2547.486166 광주 3049.028169 대구 3663.335938 대전 3128.433333 부산 3679.920755 서울 7225.762264 세종 2984.004000 울산 3043.503145 인천 3633.275862 전남 2304.969349 전북 2348.648855 제주 3432.795652 충남 2501.604743 충북 2316.871698 Name: 분양가격, dtype: float64 5-3. 지역별 “분양가격” 데이터의 갯수를 확인해보기 1df.groupby(\"지역명\")[\"분양가격\"].count() 지역명 강원 257 경기 265 경남 260 경북 253 광주 213 대구 256 대전 210 부산 265 서울 265 세종 250 울산 159 인천 261 전남 261 전북 262 제주 230 충남 253 충북 265 Name: 분양가격, dtype: int64 5-4. 지역별 제일 비싼 분양가를 확인해보기 1df.groupby(\"지역명\")[\"분양가격\"].max() 지역명 강원 3906 경기 5670 경남 4303 경북 3457 광주 4881 대구 5158 대전 4877 부산 4623 서울 13835 세종 3931 울산 3594 인천 5188 전남 3053 전북 3052 제주 5462 충남 3201 충북 2855 Name: 분양가격, dtype: int32 6. 연도별 평균 분양가격을 확인해보기 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 0 서울 전체 2015 10 5841 1 서울 60㎡이하 2015 10 5652 2 서울 60㎡초과 85㎡이하 2015 10 5882 3 서울 85㎡초과 102㎡이하 2015 10 5721 4 서울 102㎡초과 2015 10 5879 1df.groupby(\"연도\")[\"분양가격\"].mean() 연도 2015 2788.707819 2016 2934.250000 2017 3143.311795 2018 3326.951034 2019 3693.422149 2020 3853.960526 Name: 분양가격, dtype: float64 7. 피벗테이블 활용하기 행 인덱스: 연도 열 인덱스: 규모구분 값: 분양가 (평균) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 지역명 규모구분 연도 월 분양가격 0 서울 전체 2015 10 5841 1 서울 60㎡이하 2015 10 5652 2 서울 60㎡초과 85㎡이하 2015 10 5882 3 서울 85㎡초과 102㎡이하 2015 10 5721 4 서울 102㎡초과 2015 10 5879 1pd.pivot_table(df, index = \"연도\", columns = \"규모구분\", values = \"분양가격\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 규모구분 102㎡초과 60㎡이하 60㎡초과 85㎡이하 85㎡초과 102㎡이하 전체 연도 2015 2980.977778 2712.583333 2694.490196 2884.395833 2694.862745 2016 3148.099476 2848.144279 2816.965686 3067.380435 2816.073529 2017 3427.649746 3112.538071 2981.950980 3204.075145 3008.279412 2018 3468.355932 3286.184783 3227.458128 3467.184211 3235.098522 2019 4039.854839 3486.910112 3538.545918 3933.538462 3515.974490 2020 4187.566667 3615.968750 3594.852941 4532.090909 3603.911765 8. 연도별, 규모별 가격을 알아보기 1df.groupby([\"연도\", \"규모구분\"])[\"분양가격\"].mean() 연도 규모구분 2015 102㎡초과 2980.977778 60㎡이하 2712.583333 60㎡초과 85㎡이하 2694.490196 85㎡초과 102㎡이하 2884.395833 전체 2694.862745 2016 102㎡초과 3148.099476 60㎡이하 2848.144279 60㎡초과 85㎡이하 2816.965686 85㎡초과 102㎡이하 3067.380435 전체 2816.073529 2017 102㎡초과 3427.649746 60㎡이하 3112.538071 60㎡초과 85㎡이하 2981.950980 85㎡초과 102㎡이하 3204.075145 전체 3008.279412 2018 102㎡초과 3468.355932 60㎡이하 3286.184783 60㎡초과 85㎡이하 3227.458128 85㎡초과 102㎡이하 3467.184211 전체 3235.098522 2019 102㎡초과 4039.854839 60㎡이하 3486.910112 60㎡초과 85㎡이하 3538.545918 85㎡초과 102㎡이하 3933.538462 전체 3515.974490 2020 102㎡초과 4187.566667 60㎡이하 3615.968750 60㎡초과 85㎡이하 3594.852941 85㎡초과 102㎡이하 4532.090909 전체 3603.911765 Name: 분양가격, dtype: float64 예쁘게 출력이 안되어서 보기가 힘들때는 pd.DataFrame()으로 한 번 더 감싸주면 됩니다. 1pd.DataFrame(df.groupby([\"연도\", \"규모구분\"])[\"분양가격\"].mean()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 분양가격 연도 규모구분 2015 102㎡초과 2980.977778 60㎡이하 2712.583333 60㎡초과 85㎡이하 2694.490196 85㎡초과 102㎡이하 2884.395833 전체 2694.862745 2016 102㎡초과 3148.099476 60㎡이하 2848.144279 60㎡초과 85㎡이하 2816.965686 85㎡초과 102㎡이하 3067.380435 전체 2816.073529 2017 102㎡초과 3427.649746 60㎡이하 3112.538071 60㎡초과 85㎡이하 2981.950980 85㎡초과 102㎡이하 3204.075145 전체 3008.279412 2018 102㎡초과 3468.355932 60㎡이하 3286.184783 60㎡초과 85㎡이하 3227.458128 85㎡초과 102㎡이하 3467.184211 전체 3235.098522 2019 102㎡초과 4039.854839 60㎡이하 3486.910112 60㎡초과 85㎡이하 3538.545918 85㎡초과 102㎡이하 3933.538462 전체 3515.974490 2020 102㎡초과 4187.566667 60㎡이하 3615.968750 60㎡초과 85㎡이하 3594.852941 85㎡초과 102㎡이하 4532.090909 전체 3603.911765 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【EXERCISE】","slug":"【EXERCISE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/"},{"name":"Python","slug":"【EXERCISE】/Python","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"전처리","slug":"전처리","permalink":"https://hyemin-kim.github.io/tags/%EC%A0%84%EC%B2%98%EB%A6%AC/"}]},{"title":"Python >> Pandas 전처리 - (7) 기타","slug":"S-Python-Pandas-Pre7","date":"2020-06-20T13:28:42.000Z","updated":"2020-11-06T05:19:28.852Z","comments":true,"path":"2020/06/20/S-Python-Pandas-Pre7/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/20/S-Python-Pandas-Pre7/","excerpt":"","text":"기타 1. 데이터 타입별 column 선택 (select_dtypes) 문자열이 있는 column만 선택 / 배제 2. One-hot-encoding (원핫인코딩) 1import pandas as pd 1df = pd.read_csv(\"korean-idol.csv\") 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1. 데이터 타입별 column 선택 (select_dtypes) 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 15 entries, 0 to 14 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 이름 15 non-null object 1 그룹 14 non-null object 2 소속사 15 non-null object 3 성별 15 non-null object 4 생년월일 15 non-null object 5 키 13 non-null float64 6 혈액형 15 non-null object 7 브랜드평판지수 15 non-null int64 dtypes: float64(1), int64(1), object(6) memory usage: 1.1+ KB 문자열이 있는 column만 선택 / 배제 df_name .select_dtypes (include = ‘object’) df_name .select_dtypes (exclude = ‘object’) (1) 문자열 column만 선택 1df.select_dtypes(include = 'object') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 혈액형 0 지민 방탄소년단 빅히트 남자 1995-10-13 A 1 지드래곤 빅뱅 YG 남자 1988-08-18 A 2 강다니엘 NaN 커넥트 남자 1996-12-10 A 3 뷔 방탄소년단 빅히트 남자 1995-12-30 AB 4 화사 마마무 RBW 여자 1995-07-23 A 5 정국 방탄소년단 빅히트 남자 1997-09-01 A 6 민현 뉴이스트 플레디스 남자 1995-08-09 O 7 소연 아이들 큐브 여자 1998-08-26 B 8 진 방탄소년단 빅히트 남자 1992-12-04 O 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 A 10 태연 소녀시대 SM 여자 1989-03-09 A 11 차은우 아스트로 판타지오 남자 1997-03-30 B 12 백호 뉴이스트 플레디스 남자 1995-07-21 AB 13 JR 뉴이스트 플레디스 남자 1995-06-08 O 14 슈가 방탄소년단 빅히트 남자 1993-03-09 O (2) 문자열 column 배제 (문자열이 아닌 column만 선택) 1df.select_dtypes(exclude = 'object') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 키 브랜드평판지수 0 173.6 10523260 1 177.0 9916947 2 180.0 8273745 3 178.0 8073501 4 162.1 7650928 5 178.0 5208335 6 182.3 4989792 7 NaN 4668615 8 179.2 4570308 9 167.1 4036489 10 NaN 3918661 11 183.0 3506027 12 175.0 3301654 13 176.0 3274137 14 174.0 2925442 문자열이 포함된 DataFrame의 연산으로 발생되는 Error문제는 이 방법을 이용하여 해결할 수 있다 1df + 10 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py in na_arithmetic_op(left, right, op, str_rep) 148 try: --&gt; 149 result = expressions.evaluate(op, str_rep, left, right) 150 except TypeError: D:\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py in evaluate(op, op_str, a, b, use_numexpr) 207 if use_numexpr: --&gt; 208 return _evaluate(op, op_str, a, b) 209 return _evaluate_standard(op, op_str, a, b) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py in _evaluate_numexpr(op, op_str, a, b) 120 if result is None: --&gt; 121 result = _evaluate_standard(op, op_str, a, b) 122 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py in _evaluate_standard(op, op_str, a, b) 69 with np.errstate(all=\"ignore\"): ---&gt; 70 return op(a, b) 71 TypeError: can only concatenate str (not \"int\") to str ​ 1df.select_dtypes(exclude = 'object') + 10 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 키 브랜드평판지수 0 183.6 10523270 1 187.0 9916957 2 190.0 8273755 3 188.0 8073511 4 172.1 7650938 5 188.0 5208345 6 192.3 4989802 7 NaN 4668625 8 189.2 4570318 9 177.1 4036499 10 NaN 3918671 11 193.0 3506037 12 185.0 3301664 13 186.0 3274147 14 184.0 2925452 (3) “문자열 column” / “비문자열 column” 의 column명을 추출 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 15 entries, 0 to 14 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 이름 15 non-null object 1 그룹 14 non-null object 2 소속사 15 non-null object 3 성별 15 non-null object 4 생년월일 15 non-null object 5 키 13 non-null float64 6 혈액형 15 non-null object 7 브랜드평판지수 15 non-null int64 dtypes: float64(1), int64(1), object(6) memory usage: 1.1+ KB 12obj_cols = df.select_dtypes(include = 'object').columnsobj_cols Index(['이름', '그룹', '소속사', '성별', '생년월일', '혈액형'], dtype='object') 12num_cols = df.select_dtypes(exclude = 'object').columnsnum_cols Index(['키', '브랜드평판지수'], dtype='object') 1df[num_cols] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 키 브랜드평판지수 0 173.6 10523260 1 177.0 9916947 2 180.0 8273745 3 178.0 8073501 4 162.1 7650928 5 178.0 5208335 6 182.3 4989792 7 NaN 4668615 8 179.2 4570308 9 167.1 4036489 10 NaN 3918661 11 183.0 3506027 12 175.0 3301654 13 176.0 3274137 14 174.0 2925442 2. One-hot-encoding (원핫인코딩) One-hot-encoding: Categorical data를 dummy data로 변환시키는 방법 Dummy data로 변환 시 한개의 요소는 True (1) 로, 나머지 요소는 Flase (0) 로 변환시킨다 pd.get_dummies (df_name [ ‘col_name’ ], prefix = “…”) prefix: dummy data 로 분리된 새 column들의 column name에 접두사 붙이기 1df['혈액형'] 0 A 1 A 2 A 3 AB 4 A 5 A 6 O 7 B 8 O 9 A 10 A 11 B 12 AB 13 O 14 O Name: 혈액형, dtype: object 1pd.get_dummies(df['혈액형']) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A AB B O 0 1 0 0 0 1 1 0 0 0 2 1 0 0 0 3 0 1 0 0 4 1 0 0 0 5 1 0 0 0 6 0 0 0 1 7 0 0 1 0 8 0 0 0 1 9 1 0 0 0 10 1 0 0 0 11 0 0 1 0 12 0 1 0 0 13 0 0 0 1 14 0 0 0 1 1pd.get_dummies(df['혈액형'], prefix = '혈액형') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 혈액형_A 혈액형_AB 혈액형_B 혈액형_O 0 1 0 0 0 1 1 0 0 0 2 1 0 0 0 3 0 1 0 0 4 1 0 0 0 5 1 0 0 0 6 0 0 0 1 7 0 0 1 0 8 0 0 0 1 9 1 0 0 0 10 1 0 0 0 11 0 0 1 0 12 0 1 0 0 13 0 0 0 1 14 0 0 0 1 categorical data의 각 카테고리가 숫자형식으로 표현됐을 때 one-hot-encoding이 더 중요해지는 이유: categorical data의 각 카테고리를 상징하는 숫자들은 그저 분류의 의미를 가질 뿐, 숫자의 크기 자체는 아무 의미도 없고, 숫자들의 연산도 역시 무의미하다. 하지만 이를 one-hot-encoding 작업 없이 머신러닝 알고리즘에 바로 넣으면 컴퓨터가 이 숫자들을 대소비교가 가능하고 연산이 가능하는 \"숫자\"로 인식하게 되므로 카테고리 간에 잘못된 관계를 맺을 수 있음. 따라서 이런 경우에는 one-hot-encoding 작업이 꼭 필요하다 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 123456blood_map = { 'A': 0, 'B': 1, 'AB': 2, 'O': 3,} 1df[\"혈액형_code\"] = df[\"혈액형\"].map(blood_map) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 혈액형_code 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 0 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 0 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 0 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 2 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 0 1df[\"혈액형_code\"].value_counts() 0 7 3 4 2 2 1 2 Name: 혈액형_code, dtype: int64 1df[\"혈액형_code\"] 0 0 1 0 2 0 3 2 4 0 5 0 6 3 7 1 8 3 9 0 10 0 11 1 12 2 13 3 14 3 Name: 혈액형_code, dtype: int64 1pd.get_dummies(df[ \"혈액형_code\" ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 1 0 0 0 1 1 0 0 0 2 1 0 0 0 3 0 0 1 0 4 1 0 0 0 5 1 0 0 0 6 0 0 0 1 7 0 1 0 0 8 0 0 0 1 9 1 0 0 0 10 1 0 0 0 11 0 1 0 0 12 0 0 1 0 13 0 0 0 1 14 0 0 0 1 1pd.get_dummies(df[\"혈액형_code\"], prefix = \"혈액형\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 혈액형_0 혈액형_1 혈액형_2 혈액형_3 0 1 0 0 0 1 1 0 0 0 2 1 0 0 0 3 0 0 1 0 4 1 0 0 0 5 1 0 0 0 6 0 0 0 1 7 0 1 0 0 8 0 0 0 1 9 1 0 0 0 10 1 0 0 0 11 0 1 0 0 12 0 0 1 0 13 0 0 0 1 14 0 0 0 1 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"},{"name":"Python - 전처리","slug":"【STUDY-Python】/Python-전처리","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%A0%84%EC%B2%98%EB%A6%AC/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"전처리","slug":"전처리","permalink":"https://hyemin-kim.github.io/tags/%EC%A0%84%EC%B2%98%EB%A6%AC/"}]},{"title":"Python >> Pandas 전처리 - (6) 데이터프레임의 산술연산","slug":"S-Python-Pandas-Pre6","date":"2020-06-20T13:28:21.000Z","updated":"2020-11-06T05:19:23.379Z","comments":true,"path":"2020/06/20/S-Python-Pandas-Pre6/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/20/S-Python-Pandas-Pre6/","excerpt":"","text":"데이터프레임의 산술연산 1. Column 과 Column 간 연산 (+, -, *, /, %) 2. Column 과 숫자 간 연산 (+, -, *, /, %) 3. 복합 연산 4. mean(), sum() 을 axis 기준으로 연산 5. NaN 값이 존재할 경우 연산 6. DataFrame 과 DataFrame 간 연산 6-1. 문자열이 포함된 Series / DataFrame의 연산은 불가하다 6-2. 두 DataFrame의 column 이름은 같으나 column 순서만 바뀌어 있는 경우 6-3. 행의 갯수가 다른 경우 1import pandas as pd 1import numpy as np 예제 DataFrame 생성 1df = pd.DataFrame({\"통계\": [60, 70, 80, 85, 75], \"미술\": [50, 55, 80, 100, 95], \"체육\": [70, 65, 50, 95, 100] }) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 통계 미술 체육 0 60 50 70 1 70 55 65 2 80 80 50 3 85 100 95 4 75 95 100 1. Column 과 Column 간 연산 (+, -, *, /, %) 1type(df[\"통계\"]) pandas.core.series.Series 즉 Series 과 Series 간의 연산 1df[\"통계\"] + df[\"미술\"] + df[\"체육\"] 0 180 1 190 2 210 3 280 4 270 dtype: int64 1df[\"통계\"] - df[\"미술\"] 0 10 1 15 2 0 3 -15 4 -20 dtype: int64 1df[\"통계\"] * df[\"미술\"] 0 3000 1 3850 2 6400 3 8500 4 7125 dtype: int64 1df[\"통계\"] / df[\"미술\"] 0 1.200000 1 1.272727 2 1.000000 3 0.850000 4 0.789474 dtype: float64 1df[\"통계\"] % df[\"미술\"] 0 10 1 15 2 0 3 85 4 75 dtype: int64 2. Column 과 숫자 간 연산 (+, -, *, /, %) 1df[\"통계\"] 0 60 1 70 2 80 3 85 4 75 Name: 통계, dtype: int64 1df[\"통계\"] + 10 0 70 1 80 2 90 3 95 4 85 Name: 통계, dtype: int64 1df[\"통계\"] - 10 0 50 1 60 2 70 3 75 4 65 Name: 통계, dtype: int64 1df[\"통계\"] * 10 0 600 1 700 2 800 3 850 4 750 Name: 통계, dtype: int64 1df[\"통계\"] / 10 0 6.0 1 7.0 2 8.0 3 8.5 4 7.5 Name: 통계, dtype: float64 1df[\"통계\"] % 10 0 0 1 0 2 0 3 5 4 5 Name: 통계, dtype: int64 3. 복합 연산 1df = pd.DataFrame({\"통계\": [60, 70, 80, 85, 75], \"미술\": [50, 55, 80, 100, 95], \"체육\": [70, 65, 50, 95, 100] }) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 통계 미술 체육 0 60 50 70 1 70 55 65 2 80 80 50 3 85 100 95 4 75 95 100 1df[\"통계미술+10\"] = df[\"통계\"] + df[\"미술\"] + 10 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 통계 미술 체육 통계미술+10 0 60 50 70 120 1 70 55 65 135 2 80 80 50 170 3 85 100 95 195 4 75 95 100 180 1df[\"통계\"] + df[\"미술\"] - df[\"체육\"] 0 40 1 60 2 110 3 90 4 70 dtype: int64 4. mean(), sum() 을 axis 기준으로 연산 1df = pd.DataFrame({\"통계\": [60, 70, 80, 85, 75], \"미술\": [50, 55, 80, 100, 95], \"체육\": [70, 65, 50, 95, 100] }) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 통계 미술 체육 0 60 50 70 1 70 55 65 2 80 80 50 3 85 100 95 4 75 95 100 (1) 각 column의 모든 row 값의 합 구하기 1df.sum(axis = 0) 통계 370 미술 380 체육 380 dtype: int64 (2) 각 column의 모든 row 값의 평균 구하기 1df.mean(axis = 0) 통계 74.0 미술 76.0 체육 76.0 dtype: float64 (3) 각 row의 모든 column 값의 합 구하기 1df.sum(axis = 1) 0 180 1 190 2 210 3 280 4 270 dtype: int64 (4) 각 row의 모든 column 값의 평균 구하기 1df.mean(axis = 1) 0 60.000000 1 63.333333 2 70.000000 3 93.333333 4 90.000000 dtype: float64 5. NaN 값이 존재할 경우 연산 NaN 값이 포함된 모든 연산의 결과가 다 NaN 값이다 1df = pd.DataFrame({\"통계\": [60, np.nan, 80, 85, 75], \"미술\": [50, 55, np.nan, 100, 95], \"체육\": [70, 65, 50, 95, np.nan] }) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 통계 미술 체육 0 60.0 50.0 70.0 1 NaN 55.0 65.0 2 80.0 NaN 50.0 3 85.0 100.0 95.0 4 75.0 95.0 NaN 1df[\"통계\"] / 2 0 30.0 1 NaN 2 40.0 3 42.5 4 37.5 Name: 통계, dtype: float64 11000 / df[\"통계\"] 0 16.666667 1 NaN 2 12.500000 3 11.764706 4 13.333333 Name: 통계, dtype: float64 1df[\"통계\"] / np.nan 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN Name: 통계, dtype: float64 1np.nan / df[\"통계\"] 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN Name: 통계, dtype: float64 6. DataFrame 과 DataFrame 간 연산 6-1. 문자열이 포함된 Series / DataFrame의 연산은 불가하다 1df1 = pd.DataFrame({'통계': [60, 70, 80, 85, 75], '미술': [50, 55, 80, 100, 95], '체육': [70, 65, 50, 95, 100] }) 1df2 = pd.DataFrame({'통계': ['good', 'bad', 'ok' , 'good', 'ok'], '미술': [50, 60 , 80, 100, 95], '체육': [70, 65, 50, 70 , 100] }) 1df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 통계 미술 체육 0 60 50 70 1 70 55 65 2 80 80 50 3 85 100 95 4 75 95 100 1df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 통계 미술 체육 0 good 50 70 1 bad 60 65 2 ok 80 50 3 good 100 70 4 ok 95 100 1df1 + df2 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py in na_arithmetic_op(left, right, op, str_rep) 148 try: --&gt; 149 result = expressions.evaluate(op, str_rep, left, right) 150 except TypeError: D:\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py in evaluate(op, op_str, a, b, use_numexpr) 207 if use_numexpr: --&gt; 208 return _evaluate(op, op_str, a, b) 209 return _evaluate_standard(op, op_str, a, b) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py in _evaluate_numexpr(op, op_str, a, b) 120 if result is None: --&gt; 121 result = _evaluate_standard(op, op_str, a, b) 122 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py in _evaluate_standard(op, op_str, a, b) 69 with np.errstate(all=\"ignore\"): ---&gt; 70 return op(a, b) 71 TypeError: unsupported operand type(s) for +: 'int' and 'str' 1df2 + 10 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py in na_arithmetic_op(left, right, op, str_rep) 148 try: --&gt; 149 result = expressions.evaluate(op, str_rep, left, right) 150 except TypeError: D:\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py in evaluate(op, op_str, a, b, use_numexpr) 207 if use_numexpr: --&gt; 208 return _evaluate(op, op_str, a, b) 209 return _evaluate_standard(op, op_str, a, b) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py in _evaluate_numexpr(op, op_str, a, b) 120 if result is None: --&gt; 121 result = _evaluate_standard(op, op_str, a, b) 122 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py in _evaluate_standard(op, op_str, a, b) 69 with np.errstate(all=\"ignore\"): ---&gt; 70 return op(a, b) 71 TypeError: can only concatenate str (not \"int\") to str 6-2. 두 DataFrame의 column 이름은 같으나 column 순서만 바뀌어 있는 경우 연산시 자동으로 column 이름 기준으로 연산 된다 12df1 = pd.DataFrame({'미술': [10, 20, 30, 40, 50], '통계':[60, 70, 80, 90, 100] })df2 = pd.DataFrame({'통계': [10, 20, 30, 40, 50], '미술': [60, 70, 80, 90, 100] }) 1df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 미술 통계 0 10 60 1 20 70 2 30 80 3 40 90 4 50 100 1df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 통계 미술 0 10 60 1 20 70 2 30 80 3 40 90 4 50 100 1df1 + df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 미술 통계 0 70 70 1 90 90 2 110 110 3 130 130 4 150 150 6-3. 행의 갯수가 다른 경우 행 index 기준으로 연산하되, 하나의 DataFrame에만 존재하는 행은 연산결과가 NaN으로 나옴 12df1 = pd.DataFrame({'미술': [10, 20, 30, 40, 50, 60], '통계':[60, 70, 80, 90, 100, 110] })df2 = pd.DataFrame({'통계': [10, 20, 30, 40, 50], '미술': [60, 70, 80, 90, 100] }) 1df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 미술 통계 0 10 60 1 20 70 2 30 80 3 40 90 4 50 100 5 60 110 1df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 통계 미술 0 10 60 1 20 70 2 30 80 3 40 90 4 50 100 1df1 * df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 미술 통계 0 600.0 600.0 1 1400.0 1400.0 2 2400.0 2400.0 3 3600.0 3600.0 4 5000.0 5000.0 5 NaN NaN document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"},{"name":"Python - 전처리","slug":"【STUDY-Python】/Python-전처리","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%A0%84%EC%B2%98%EB%A6%AC/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"전처리","slug":"전처리","permalink":"https://hyemin-kim.github.io/tags/%EC%A0%84%EC%B2%98%EB%A6%AC/"}]},{"title":"Python >> Pandas 전처리 - (5) column 값을 변환시키는 방법","slug":"S-Python-Pandas-Pre5","date":"2020-06-19T12:11:52.000Z","updated":"2020-11-06T05:19:17.612Z","comments":true,"path":"2020/06/19/S-Python-Pandas-Pre5/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/19/S-Python-Pandas-Pre5/","excerpt":"","text":"DataFrame의 column 값을 변환시키는 방법 1. apply + 일반 함수 1-1. (목표) ‘성별’ column의 “남자” / \"여자\"를 1 / 2로 바꾼다 1-2. (목표) cm당 브랜드 평판지수를 구한다 (브랜드평판지수 / 키) 2. apply + lamda 함수 3. map + map 함수 1import pandas as pd 1df = pd.read_csv('korean-idol.csv') 1. apply + 일반 함수 apply는 Series나 DataFrame에 좀 더 구체적인 로직을 적용하고 싶은 경우 활용한다 apply를 적용하기 위해서는 함수가 먼저 정의되어야한다 apply는 정의한 로직 함수를 인자로 넘겨준다 Series에 적용할 경우: df_name [ “col_name” ] .apply( func ) DataFrame에 적용할 경우: df_name .apply( func, axis = 1) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1-1. (목표) ‘성별’ column의 “남자” / \"여자\"를 1 / 2로 바꾼다 변환 규칙: 남자: 1 여자: 2 기타: -1 (1) 로직 함수 정의 [주의] 반드시 return 값이 존재하여야한다 12345def male_or_female(x): if x == \"남자\": return 1 elif x == \"여자\": return 2 (2) apply로 DataFrame에 적용 1df[\"성별_NEW\"] = df[\"성별\"].apply(male_or_female) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 성별_NEW 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 1 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 1 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 1 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 2 1-2. (목표) cm당 브랜드 평판지수를 구한다 (브랜드평판지수 / 키) 변환 규칙: 키: 178 브랜드평판지수: 99000 값: 99000 / 178 (1) 로직 함수 정의 123def cm_to_brand(df): value = df[\"브랜드평판지수\"] / df[\"키\"] return value (2) apply로 DataFrame에 적용 1df.apply(cm_to_brand, axis = 1) 0 60617.857143 1 56027.949153 2 45965.250000 3 45356.747191 4 47198.815546 5 29260.308989 6 27371.321997 7 NaN 8 25503.950893 9 24156.128067 10 NaN 11 19158.617486 12 18866.594286 13 18603.051136 14 16812.885057 dtype: float64 2. apply + lamda 함수 df_name [ “col_name” ] .apply (lambda_func) lambda는 1줄로 작성하는 간단 함수식이다 return을 별도로 멱기하지 않는다 (1) male_or_female 함수 1male_or_female = lambda x: 1 if x == \"남자\" else 0 1df[\"성별\"].apply(male_or_female) 0 1 1 1 2 1 3 1 4 0 5 1 6 1 7 0 8 1 9 1 10 0 11 1 12 1 13 1 14 1 Name: 성별, dtype: int64 (2) 실제로는 간단한 계산식을 적용하려는 경우에 많이 사용한다 1df[\"키/2\"] = df[\"키\"].apply(lambda x: x / 2) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 성별_NEW 키/2 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 86.80 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 1 88.50 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 1 90.00 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 1 89.00 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 2 81.05 apply에 함수식을 만들어서 적용해주는 것과 동일하기 때문에, 복잠한 조건식은 &lt;함수&gt;로, 간단한 계산식은 &lt; lambda &gt; 로 적용하면 된다 3. map + map 함수 df_name [ “col_name” ] .map ( map_func ) Step 1: dictionary 형식으로 map 함수를 정의하기 Step 2: DataFrame / Series에 map 함수를 적용 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 성별_NEW 키/2 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 86.80 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 1 88.50 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 1 90.00 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 1 89.00 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 2 81.05 1234my_map = { \"남자\": \"male\", \"여자\": \"female\"} 1df[\"성별\"].map(my_map) 0 male 1 male 2 male 3 male 4 female 5 male 6 male 7 female 8 male 9 male 10 female 11 male 12 male 13 male 14 male Name: 성별, dtype: object document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"},{"name":"Python - 전처리","slug":"【STUDY-Python】/Python-전처리","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%A0%84%EC%B2%98%EB%A6%AC/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"전처리","slug":"전처리","permalink":"https://hyemin-kim.github.io/tags/%EC%A0%84%EC%B2%98%EB%A6%AC/"}]},{"title":"Python >> Pandas 전처리 - (4) Series의 Type 변환하기","slug":"S-Python-Pandas-Pre4","date":"2020-06-19T06:53:13.000Z","updated":"2020-11-06T05:19:12.309Z","comments":true,"path":"2020/06/19/S-Python-Pandas-Pre4/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/19/S-Python-Pandas-Pre4/","excerpt":"","text":"Series의 Type 변환하기 1. Series의 Type 1-1. Type 확인하기 1-2. Type 변환하기 1-3. 날짜 (datatime) 타입 변환하기 1import pandas as pd 1df = pd.read_csv('korean-idol.csv') 1. Series의 Type 1-1. Type 확인하기 df_name.info() 명령어를 사용하여 Dataframe의 Series Type을 확인할 수 있다 df_name [ “col_name” ] .dtypes 명령어를 사용하여 특정 Series의 Type을 확인할 수 있다 Series Type object: 일반 문자영 타입 float: 실수 int: 정수 category: 카테고리 datatime: 시간 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 15 entries, 0 to 14 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 이름 15 non-null object 1 그룹 14 non-null object 2 소속사 15 non-null object 3 성별 15 non-null object 4 생년월일 15 non-null object 5 키 13 non-null float64 6 혈액형 15 non-null object 7 브랜드평판지수 15 non-null int64 dtypes: float64(1), int64(1), object(6) memory usage: 1.1+ KB 1df[\"이름\"].dtypes dtype('O') 1-2. Type 변환하기 df_name [ “col_name” ] .astype(…) e.g. “키” column을 float에서 int로 변환해보기 1df[\"키\"].astype(int) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-12-c145a39acdb2&gt; in &lt;module&gt; ----&gt; 1 df[\"키\"].astype(int) D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py in astype(self, dtype, copy, errors) 5696 else: 5697 # else, only a single dtype is given -&gt; 5698 new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors) 5699 return self._constructor(new_data).__finalize__(self) 5700 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in astype(self, dtype, copy, errors) 580 581 def astype(self, dtype, copy: bool = False, errors: str = \"raise\"): --&gt; 582 return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors) 583 584 def convert(self, **kwargs): D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py in apply(self, f, filter, **kwargs) 440 applied = b.apply(f, **kwargs) 441 else: --&gt; 442 applied = getattr(b, f)(**kwargs) 443 result_blocks = _extend_blocks(applied, result_blocks) 444 D:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py in astype(self, dtype, copy, errors) 623 vals1d = values.ravel() 624 try: --&gt; 625 values = astype_nansafe(vals1d, dtype, copy=True) 626 except (ValueError, TypeError): 627 # e.g. astype_nansafe can fail on object-dtype of strings D:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py in astype_nansafe(arr, dtype, copy, skipna) 866 867 if not np.isfinite(arr).all(): --&gt; 868 raise ValueError(\"Cannot convert non-finite values (NA or inf) to integer\") 869 870 elif is_object_dtype(arr): ValueError: Cannot convert non-finite values (NA or inf) to integer “키” column에 NaN값이 존재하기 때문에 Error 발생! column에 NaN 값이 있는 경우: 면저 NaN 값을 다른 값으로 대체한 후 Type을 변환할 수 있다 1df[\"키\"] = df[\"키\"].fillna(-1) 1df[\"키\"] 0 173.6 1 177.0 2 180.0 3 178.0 4 162.1 5 178.0 6 182.3 7 -1.0 8 179.2 9 167.1 10 -1.0 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 1df[\"키\"].astype(int) 0 173 1 177 2 180 3 178 4 162 5 178 6 182 7 -1 8 179 9 167 10 -1 11 183 12 175 13 176 14 174 Name: 키, dtype: int32 1-3. 날짜 (datatime) 타입 변환하기 (1) datetime 타입으로 변환하기 pd.to_datetime ( df_name [ “col_nema”] ) 1df[\"생년월일\"] 0 1995-10-13 1 1988-08-18 2 1996-12-10 3 1995-12-30 4 1995-07-23 5 1997-09-01 6 1995-08-09 7 1998-08-26 8 1992-12-04 9 1994-03-22 10 1989-03-09 11 1997-03-30 12 1995-07-21 13 1995-06-08 14 1993-03-09 Name: 생년월일, dtype: object 1pd.to_datetime(df[\"생년월일\"]) 0 1995-10-13 1 1988-08-18 2 1996-12-10 3 1995-12-30 4 1995-07-23 5 1997-09-01 6 1995-08-09 7 1998-08-26 8 1992-12-04 9 1994-03-22 10 1989-03-09 11 1997-03-30 12 1995-07-21 13 1995-06-08 14 1993-03-09 Name: 생년월일, dtype: datetime64[ns] 변환된 것을 원래 column에 다시 대입을 해줘야 정상적으로 변환된 값이 들어간다 12df[\"생년월일\"] = pd.to_datetime(df[\"생년월일\"])df[\"생년월일\"] 0 1995-10-13 1 1988-08-18 2 1996-12-10 3 1995-12-30 4 1995-07-23 5 1997-09-01 6 1995-08-09 7 1998-08-26 8 1992-12-04 9 1994-03-22 10 1989-03-09 11 1997-03-30 12 1995-07-21 13 1995-06-08 14 1993-03-09 Name: 생년월일, dtype: datetime64[ns] (2) datatime 타입을 활용하기 df_name [ “datetime_col” ] .dt 을 활용하여 매우 손쉽게 년, 월, 일, 요일 등등 날짜 정보를 세부적으로 추출해낼 수 있다 년: df_name [ “datetime_col” ] .dt.year 월: df_name [ “datetime_col” ] .dt.month 일: df_name [ “datetime_col” ] .dt.day 요일: df_name [ “datetime_col” ] .dt.dayofweek 주: df_name [ “datetime_col” ] .dt.weekofyear 1df[\"생년월일\"] 0 1995-10-13 1 1988-08-18 2 1996-12-10 3 1995-12-30 4 1995-07-23 5 1997-09-01 6 1995-08-09 7 1998-08-26 8 1992-12-04 9 1994-03-22 10 1989-03-09 11 1997-03-30 12 1995-07-21 13 1995-06-08 14 1993-03-09 Name: 생년월일, dtype: datetime64[ns] 년 추출: 1df[\"생년월일\"].dt.year 0 1995 1 1988 2 1996 3 1995 4 1995 5 1997 6 1995 7 1998 8 1992 9 1994 10 1989 11 1997 12 1995 13 1995 14 1993 Name: 생년월일, dtype: int64 월 추출: 1df[\"생년월일\"].dt.month 0 10 1 8 2 12 3 12 4 7 5 9 6 8 7 8 8 12 9 3 10 3 11 3 12 7 13 6 14 3 Name: 생년월일, dtype: int64 일 추출: 1df[\"생년월일\"].dt.day 0 13 1 18 2 10 3 30 4 23 5 1 6 9 7 26 8 4 9 22 10 9 11 30 12 21 13 8 14 9 Name: 생년월일, dtype: int64 요일 추출: 월 [0], 화 [1], 수 [2], 목 [3], 금 [4], 토 [5], 일 [6] 1df[\"생년월일\"].dt.dayofweek 0 4 1 3 2 1 3 5 4 6 5 0 6 2 7 2 8 4 9 1 10 3 11 6 12 4 13 3 14 1 Name: 생년월일, dtype: int64 주 추출: 1df[\"생년월일\"].dt.weekofyear 0 41 1 33 2 50 3 52 4 29 5 36 6 32 7 35 8 49 9 12 10 10 11 13 12 29 13 23 14 10 Name: 생년월일, dtype: int64 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"},{"name":"Python - 전처리","slug":"【STUDY-Python】/Python-전처리","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%A0%84%EC%B2%98%EB%A6%AC/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"전처리","slug":"전처리","permalink":"https://hyemin-kim.github.io/tags/%EC%A0%84%EC%B2%98%EB%A6%AC/"}]},{"title":"Python >> Pandas 전처리 - (3) DataFrame의 합침 및 병합","slug":"S-Python-Pandas-Pre3","date":"2020-06-19T06:52:54.000Z","updated":"2020-11-06T05:19:07.255Z","comments":true,"path":"2020/06/19/S-Python-Pandas-Pre3/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/19/S-Python-Pandas-Pre3/","excerpt":"","text":"DataFrame의 합침 및 병합 1. DataFrame 합치기 (concat) 1-1. Row 기준 합치기 (밑으로 합침) 1-2. column 기준으로 합치기 (옆으로 합침) 2. DataFrame 병합하기 (merge) 2-0. 예제 데이터 만들기 2-1. left, right 방식 2-2. inner, outer 방식 2-3. column명은 다르지만, 동일한 성질의 데이터 인 경우? 1import pandas as pd 1df = pd.read_csv('korean-idol.csv') 1df2 = pd.read_csv('korean-idol-2.csv') 1. DataFrame 합치기 (concat) 1-1. Row 기준 합치기 (밑으로 합침) df_concat = pd.concat ( [ df_name1 , df_name2 ], sort = False) df_concat .reset_index (drop = True) 합칠 데이터프리임을 list로 묶어준다. sort=False 옵션을 주어 column의 순서가 유지되도록 한다 합친 dataframe을 새 변수에 대입한 뒤 reset_index 옵션으로 index를 초기화한다 (아님 각각 원래의 index을 가지고 있음) reseet_index에서 drop=True 옵션을 사용해 원래의 행 index가 새로 index column으로 생성되지 않도록 한다 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df_copy = df.copy() (1) sort 옵션 sort = False: column 순서 유지; sort = True: column을 이름순으로 재정열 1pd.concat([df, df_copy], sort = False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1pd.concat([df, df_copy], sort = True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 그룹 브랜드평판지수 생년월일 성별 소속사 이름 키 혈액형 0 방탄소년단 10523260 1995-10-13 남자 빅히트 지민 173.6 A 1 빅뱅 9916947 1988-08-18 남자 YG 지드래곤 177.0 A 2 NaN 8273745 1996-12-10 남자 커넥트 강다니엘 180.0 A 3 방탄소년단 8073501 1995-12-30 남자 빅히트 뷔 178.0 AB 4 마마무 7650928 1995-07-23 여자 RBW 화사 162.1 A 5 방탄소년단 5208335 1997-09-01 남자 빅히트 정국 178.0 A 6 뉴이스트 4989792 1995-08-09 남자 플레디스 민현 182.3 O 7 아이들 4668615 1998-08-26 여자 큐브 소연 NaN B 8 방탄소년단 4570308 1992-12-04 남자 빅히트 진 179.2 O 9 핫샷 4036489 1994-03-22 남자 스타크루이엔티 하성운 167.1 A 10 소녀시대 3918661 1989-03-09 여자 SM 태연 NaN A 11 아스트로 3506027 1997-03-30 남자 판타지오 차은우 183.0 B 12 뉴이스트 3301654 1995-07-21 남자 플레디스 백호 175.0 AB 13 뉴이스트 3274137 1995-06-08 남자 플레디스 JR 176.0 O 14 방탄소년단 2925442 1993-03-09 남자 빅히트 슈가 174.0 O 0 방탄소년단 10523260 1995-10-13 남자 빅히트 지민 173.6 A 1 빅뱅 9916947 1988-08-18 남자 YG 지드래곤 177.0 A 2 NaN 8273745 1996-12-10 남자 커넥트 강다니엘 180.0 A 3 방탄소년단 8073501 1995-12-30 남자 빅히트 뷔 178.0 AB 4 마마무 7650928 1995-07-23 여자 RBW 화사 162.1 A 5 방탄소년단 5208335 1997-09-01 남자 빅히트 정국 178.0 A 6 뉴이스트 4989792 1995-08-09 남자 플레디스 민현 182.3 O 7 아이들 4668615 1998-08-26 여자 큐브 소연 NaN B 8 방탄소년단 4570308 1992-12-04 남자 빅히트 진 179.2 O 9 핫샷 4036489 1994-03-22 남자 스타크루이엔티 하성운 167.1 A 10 소녀시대 3918661 1989-03-09 여자 SM 태연 NaN A 11 아스트로 3506027 1997-03-30 남자 판타지오 차은우 183.0 B 12 뉴이스트 3301654 1995-07-21 남자 플레디스 백호 175.0 AB 13 뉴이스트 3274137 1995-06-08 남자 플레디스 JR 176.0 O 14 방탄소년단 2925442 1993-03-09 남자 빅히트 슈가 174.0 O (2) reset_index 옵션 reset_index(): index가 초기화됨, 원래의 index가 새로 index column으로 저장됨 reset_index(drop = True): index가 초기화됨, 원래의 index가 새로 index column으로 생성되지 않음 1df_concat = pd.concat([df, df_copy], sort = False) 1df_concat.reset_index() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } index 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 15 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 16 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 17 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 18 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 19 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 20 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 21 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 22 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 23 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 24 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 25 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 26 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 27 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 28 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 29 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1df_concat.reset_index(drop = True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 15 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 16 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 17 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 18 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 19 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 20 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 21 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 22 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 23 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 24 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 25 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 26 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 27 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 28 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 29 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1-2. column 기준으로 합치기 (옆으로 합침) column 기준으로 합치고자 할 때는 axis = 1 옵션을 준다: pd.concat ( [df_name1, df_name2], axis = 1) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df2 = pd.read_csv('korean-idol-2.csv') 1df2.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 연봉 가족수 0 지민 3000 3 1 지드래곤 3500 3 2 강다니엘 3200 4 3 뷔 3050 4 4 화사 4300 3 1pd.concat([df, df2], axis = 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 이름 연봉 가족수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 지민 3000 3 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 지드래곤 3500 3 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 강다니엘 3200 4 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 뷔 3050 4 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 화사 4300 3 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 정국 2900 5 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 민현 3400 6 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 소연 4500 5 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 진 4200 4 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 하성운 4300 4 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 태연 3700 3 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 차은우 3850 5 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 백호 3900 4 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 JR 4100 3 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 슈가 4150 3 행의 갯수가 맞지 않을 시 두 DataFrame이 행 index기준으로 합치게 됨 행 갯수가 적은 DataFrame의 빈칸에는 NaN로 채워지게 됨 12df3 = df2.drop([3,5])df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 연봉 가족수 0 지민 3000 3 1 지드래곤 3500 3 2 강다니엘 3200 4 4 화사 4300 3 6 민현 3400 6 7 소연 4500 5 8 진 4200 4 9 하성운 4300 4 10 태연 3700 3 11 차은우 3850 5 12 백호 3900 4 13 JR 4100 3 14 슈가 4150 3 1pd.concat([df, df3], axis = 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 이름 연봉 가족수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 지민 3000.0 3.0 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 지드래곤 3500.0 3.0 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 강다니엘 3200.0 4.0 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 NaN NaN NaN 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 화사 4300.0 3.0 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 NaN NaN NaN 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 민현 3400.0 6.0 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 소연 4500.0 5.0 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 진 4200.0 4.0 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 하성운 4300.0 4.0 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 태연 3700.0 3.0 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 차은우 3850.0 5.0 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 백호 3900.0 4.0 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 JR 4100.0 3.0 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 슈가 4150.0 3.0 12df4 = df2.drop([13, 14])pd.concat([df,df4], axis = 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 이름 연봉 가족수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 지민 3000.0 3.0 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 지드래곤 3500.0 3.0 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 강다니엘 3200.0 4.0 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 뷔 3050.0 4.0 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 화사 4300.0 3.0 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 정국 2900.0 5.0 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 민현 3400.0 6.0 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 소연 4500.0 5.0 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 진 4200.0 4.0 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 하성운 4300.0 4.0 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 태연 3700.0 3.0 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 차은우 3850.0 5.0 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 백호 3900.0 4.0 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 NaN NaN NaN 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 NaN NaN NaN 2. DataFrame 병합하기 (merge) concat과 merge의 차이: concat: row 나 column 기준으로 단순하게 이어 붙히기 merge: 특정 고유한 키(unique id) 값을 기준으로 병합하기 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df2.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 연봉 가족수 0 지민 3000 3 1 지드래곤 3500 3 2 강다니엘 3200 4 3 뷔 3050 4 4 화사 4300 3 df와 df2는 \"이름\"이라는 column이 겹친다 따라서, 우리는 \"이름\"을 기준으로 두 DataFrame을 병합할 수 있다 pd.merge (left_df, right_df, on = “기준 column”, how = “…” ) left_df와 right_df 에는 병합할 두 DataFrame을 대입한다 on 에는 병합의 기준이 되는 column을 넣어 준다 how 에는 ‘left’, ‘right’, ‘inner’, 'outer’라는 4가지의 병합 방식중 한가지를 택한다 2-0. 예제 데이터 만들기 1df_right = df2.drop([1,3,5,7]) 1df_right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 연봉 가족수 0 지민 3000 3 2 강다니엘 3200 4 4 화사 4300 3 6 민현 3400 6 8 진 4200 4 9 하성운 4300 4 10 태연 3700 3 11 차은우 3850 5 12 백호 3900 4 13 JR 4100 3 14 슈가 4150 3 12df_right = df_right.reset_index(drop = True)df_right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 연봉 가족수 0 지민 3000 3 1 강다니엘 3200 4 2 화사 4300 3 3 민현 3400 6 4 진 4200 4 5 하성운 4300 4 6 태연 3700 3 7 차은우 3850 5 8 백호 3900 4 9 JR 4100 3 10 슈가 4150 3 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 concat로 합치는 경우: 데이터가 행 index기준으로 합치게 되기 때문에 이름이 다른 시람의 데이터가 합치게 된다 1pd.concat([df, df_right], axis = 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 이름 연봉 가족수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 지민 3000.0 3.0 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 강다니엘 3200.0 4.0 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 화사 4300.0 3.0 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 민현 3400.0 6.0 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 진 4200.0 4.0 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 하성운 4300.0 4.0 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 태연 3700.0 3.0 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 차은우 3850.0 5.0 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 백호 3900.0 4.0 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 JR 4100.0 3.0 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 슈가 4150.0 3.0 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 NaN NaN NaN 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 NaN NaN NaN 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 NaN NaN NaN 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 NaN NaN NaN 따리서, 우리는 merge를 사용하여 두 DataFrame를 “이름” 기준으로 병합한다 2-1. left, right 방식 \"left\"옵션을 부여할 때: left DataFrame에 키 값이 존재하면 해당 데이터를 유지하고, 병합한 right DataFrame의 값은 NaN이 대입 됨 반대로, \"right\"옵션을 부여할 때 right DataFrame을 기준으로 병합하게 됨 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1df_right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 연봉 가족수 0 지민 3000 3 1 강다니엘 3200 4 2 화사 4300 3 3 민현 3400 6 4 진 4200 4 5 하성운 4300 4 6 태연 3700 3 7 차은우 3850 5 8 백호 3900 4 9 JR 4100 3 10 슈가 4150 3 1pd.merge(df, df_right, on = \"이름\", how = \"left\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 연봉 가족수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 3000.0 3.0 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 NaN NaN 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3200.0 4.0 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 NaN NaN 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 4300.0 3.0 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 NaN NaN 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 3400.0 6.0 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 NaN NaN 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 4200.0 4.0 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 4300.0 4.0 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 3700.0 3.0 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 3850.0 5.0 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 3900.0 4.0 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 4100.0 3.0 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 4150.0 3.0 1pd.merge(df, df_right, on = \"이름\", how = \"right\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 연봉 가족수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 3000 3 1 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3200 4 2 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 4300 3 3 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 3400 6 4 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 4200 4 5 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 4300 4 6 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 3700 3 7 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 3850 5 8 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 3900 4 9 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 4100 3 10 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 4150 3 현재, left DataFrame이 더 많은 데이터를 보유하고 있으니, right를 기준으로 병합하면 DataFrame 사이즈가 줄어드게 된다 2-2. inner, outer 방식 inner 방식은 두 DataFrame에 모두 키 값이 존재하는 경우만 병합한다 (교집합과 비슷) outer 방식은 하나의 DataFrame에만 키 값이 존재하더라도 모두 병합한다 (합집합과 비슷) outer 방식에서는 없는 값은 NaN으로 대입된다 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1df_right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 연봉 가족수 0 지민 3000 3 1 강다니엘 3200 4 2 화사 4300 3 3 민현 3400 6 4 진 4200 4 5 하성운 4300 4 6 태연 3700 3 7 차은우 3850 5 8 백호 3900 4 9 JR 4100 3 10 슈가 4150 3 1pd.merge(df, df_right, on = \"이름\", how = 'inner') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 연봉 가족수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 3000 3 1 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3200 4 2 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 4300 3 3 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 3400 6 4 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 4200 4 5 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 4300 4 6 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 3700 3 7 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 3850 5 8 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 3900 4 9 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 4100 3 10 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 4150 3 1pd.merge(df, df_right, on = \"이름\", how = 'outer') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 연봉 가족수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 3000.0 3.0 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 NaN NaN 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3200.0 4.0 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 NaN NaN 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 4300.0 3.0 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 NaN NaN 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 3400.0 6.0 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 NaN NaN 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 4200.0 4.0 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 4300.0 4.0 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 3700.0 3.0 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 3850.0 5.0 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 3900.0 4.0 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 4100.0 3.0 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 4150.0 3.0 2-3. column명은 다르지만, 동일한 성질의 데이터 인 경우? pd.merge ( left_df, right_df, left_on = “left_col”, right_on = “right_col”, how = “…” ) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df_right.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 연봉 가족수 0 지민 3000 3 1 강다니엘 3200 4 2 화사 4300 3 3 민현 3400 6 4 진 4200 4 1df_right.columns = [\"성함\", \"연봉\", \"기족수\"] 1df_right.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 성함 연봉 기족수 0 지민 3000 3 1 강다니엘 3200 4 2 화사 4300 3 3 민현 3400 6 4 진 4200 4 df의 \"이름\"과 df_right의 \"성함\"은 column name이 다르지만, 동일한 성질의 데이터다. 이럴 때는 left_on, right_on 옵션을 사용해 기준 column을 지정한다 1pd.merge(df, df_right, left_on = \"이름\", right_on = \"성함\", how = \"outer\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 성함 연봉 기족수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 지민 3000.0 3.0 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 NaN NaN NaN 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 강다니엘 3200.0 4.0 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 NaN NaN NaN 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 화사 4300.0 3.0 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 NaN NaN NaN 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 민현 3400.0 6.0 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 NaN NaN NaN 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 진 4200.0 4.0 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 하성운 4300.0 4.0 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 태연 3700.0 3.0 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 차은우 3850.0 5.0 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 백호 3900.0 4.0 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 JR 4100.0 3.0 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 슈가 4150.0 3.0 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"},{"name":"Python - 전처리","slug":"【STUDY-Python】/Python-전처리","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%A0%84%EC%B2%98%EB%A6%AC/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"전처리","slug":"전처리","permalink":"https://hyemin-kim.github.io/tags/%EC%A0%84%EC%B2%98%EB%A6%AC/"}]},{"title":"Python >> Pandas 전처리 - (2) 결측값 및 중복값 처리","slug":"S-Python-Pandas-Pre2","date":"2020-06-17T15:07:04.000Z","updated":"2020-11-06T05:19:01.490Z","comments":true,"path":"2020/06/18/S-Python-Pandas-Pre2/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/18/S-Python-Pandas-Pre2/","excerpt":"","text":"결측값 및 중복값 처리 1. 결측값을 제거하기 – dropna() 2. 결측값을 채워주기 – fillna 2-1. NA값을 특정 숫자로 채우기 2-2. NA값을 통계값으로 채우기 3. 중복된 값을 제거하기 – drop_duplicates 3-1. column의 중복값 제거 3-2. 행 전체 제거 1import pandas as pd 1df = pd.read_csv('korean-idol.csv') 1. 결측값을 제거하기 – dropna() 결측값이 있는 행을 제거: (1) df_name .dropna() (2) df_name .dropna(axis=0) 결측값이 있는 열을 제거: df_name .dropna(axis=1) NA가 하나라도 있는 경우 제거: df_name .dropna(axis=0, how = ‘any’) 모두가 NA인 경우 제거: df_name .dropna(axis=0, how = ‘all’) 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 15 entries, 0 to 14 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 이름 15 non-null object 1 그룹 14 non-null object 2 소속사 15 non-null object 3 성별 15 non-null object 4 생년월일 15 non-null object 5 키 13 non-null float64 6 혈액형 15 non-null object 7 브랜드평판지수 15 non-null int64 dtypes: float64(1), int64(1), object(6) memory usage: 1.1+ KB 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 (1) 결측값이 있는 행 제거 1df.dropna() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1df.dropna(axis = 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 (2) 결측 값이 있는 열 제거 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 15 entries, 0 to 14 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 이름 15 non-null object 1 그룹 14 non-null object 2 소속사 15 non-null object 3 성별 15 non-null object 4 생년월일 15 non-null object 5 키 13 non-null float64 6 혈액형 15 non-null object 7 브랜드평판지수 15 non-null int64 dtypes: float64(1), int64(1), object(6) memory usage: 1.1+ KB 1df.dropna(axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 소속사 성별 생년월일 혈액형 브랜드평판지수 0 지민 빅히트 남자 1995-10-13 A 10523260 1 지드래곤 YG 남자 1988-08-18 A 9916947 2 강다니엘 커넥트 남자 1996-12-10 A 8273745 3 뷔 빅히트 남자 1995-12-30 AB 8073501 4 화사 RBW 여자 1995-07-23 A 7650928 5 정국 빅히트 남자 1997-09-01 A 5208335 6 민현 플레디스 남자 1995-08-09 O 4989792 7 소연 큐브 여자 1998-08-26 B 4668615 8 진 빅히트 남자 1992-12-04 O 4570308 9 하성운 스타크루이엔티 남자 1994-03-22 A 4036489 10 태연 SM 여자 1989-03-09 A 3918661 11 차은우 판타지오 남자 1997-03-30 B 3506027 12 백호 플레디스 남자 1995-07-21 AB 3301654 13 JR 플레디스 남자 1995-06-08 O 3274137 14 슈가 빅히트 남자 1993-03-09 O 2925442 (3) NA가 하나라도 있는 경우 행 제거 1df.dropna(axis=0, how = 'any') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 (4) 모두가 NA인 경우 행 제거 1import numpy as np 1df.iloc[10] = np.nan 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260.0 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947.0 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745.0 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501.0 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928.0 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335.0 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792.0 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615.0 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308.0 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489.0 10 NaN NaN NaN NaN NaN NaN NaN NaN 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027.0 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654.0 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137.0 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442.0 1df.dropna(axis=0, how = 'all') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260.0 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947.0 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745.0 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501.0 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928.0 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335.0 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792.0 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615.0 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308.0 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489.0 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027.0 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654.0 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137.0 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442.0 2. 결측값을 채워주기 – fillna df_name [ 'na_col_name ’ ] .fillna(fill_value) 결측값을 채운 데이터프레임을 유지시키려면: (1) inplace = True 옵션을 추가함 (2) 원 dataframe에 다시 대입함 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 15 entries, 0 to 14 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 이름 15 non-null object 1 그룹 14 non-null object 2 소속사 15 non-null object 3 성별 15 non-null object 4 생년월일 15 non-null object 5 키 13 non-null float64 6 혈액형 15 non-null object 7 브랜드평판지수 15 non-null int64 dtypes: float64(1), int64(1), object(6) memory usage: 1.1+ KB \"키\"에 2개의 데이터가 누락, \"그룹\"에 1개의 데이터가 누락된 것을 확인할 수 있다 2-1. NA값을 특정 숫자로 채우기 df_name[ 'na_col_name ’ ] .fillna (new_value, inplace = True) df_name[ 'na_col_name ’ ] = df_name[ 'na_col_name ’ ] .fillna (new_value) e.g. 누락된 ‘키’ 값을 '-1’로 채워줌 1df['키'].fillna(-1) 0 173.6 1 177.0 2 180.0 3 178.0 4 162.1 5 178.0 6 182.3 7 -1.0 8 179.2 9 167.1 10 -1.0 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 이때는 원 데이터가 변화되지 않음. 1df['키'] 0 173.6 1 177.0 2 180.0 3 178.0 4 162.1 5 178.0 6 182.3 7 NaN 8 179.2 9 167.1 10 NaN 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 수정된 데이터를 유지시키려면: &lt;방법1&gt; 1df2 = df.copy() 1df2['키'].fillna(-1, inplace = True) 1df2['키'] 0 173.6 1 177.0 2 180.0 3 178.0 4 162.1 5 178.0 6 182.3 7 -1.0 8 179.2 9 167.1 10 -1.0 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 &lt;방법2&gt; 1df2 = df.copy() 1df2['키'] = df2['키'].fillna(-1) 1df2['키'] 0 173.6 1 177.0 2 180.0 3 178.0 4 162.1 5 178.0 6 182.3 7 -1.0 8 179.2 9 167.1 10 -1.0 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 2-2. NA값을 통계값으로 채우기 df_name[ 'na_col_name ’ ] .fillna (df_name[ 'na_col_name ’ ] .mean(), inplace = True) df_name[ 'na_col_name ’ ] = df_name[ 'na_col_name ’ ] .fillna (df_name[ 'na_col_name ’ ] .mean()) 1df2 = df.copy() 1df2['키'] 0 173.6 1 177.0 2 180.0 3 178.0 4 162.1 5 178.0 6 182.3 7 NaN 8 179.2 9 167.1 10 NaN 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 (1) 평균으로 대체 1df2['키'].mean() 175.79230769230767 1df2['키'].fillna(df2['키'].mean(), inplace = True) 1df2['키'] = df2['키'].fillna(df2['키'].mean()) 1df2['키'] 0 173.600000 1 177.000000 2 180.000000 3 178.000000 4 162.100000 5 178.000000 6 182.300000 7 175.792308 8 179.200000 9 167.100000 10 175.792308 11 183.000000 12 175.000000 13 176.000000 14 174.000000 Name: 키, dtype: float64 (2) 중위값으로 대체 1df2 = df.copy() 1df2['키'].median() 177.0 1df2['키'].fillna(df2['키'].median(), inplace = True) 1df2['키'] = df2['키'].fillna(df2['키'].median()) 1df2['키'] 0 173.6 1 177.0 2 180.0 3 178.0 4 162.1 5 178.0 6 182.3 7 177.0 8 179.2 9 167.1 10 177.0 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 3. 중복된 값을 제거하기 – drop_duplicates 1df = pd.read_csv('korean-idol.csv') 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 3-1. column의 중복값 제거 df_name [“col_name”] .drop_duplicates( keep = … ) 여러 개 중복값 (NaN 포함) 중에서 기본적으로 첫번째 것만 유지시키고 나머지는 다 제거한다 하지만 keep 옵션으로 유지하고 싶은 데이터를 선택할 수 있다. [keep: ‘first’ / ‘last’] 이때는 해당 위치의 값만 삭제되고 행 자체는 유지된다 (1) 중복값 중의 첫번째를 유지시킴 (default) 1df['키'] 0 173.6 1 177.0 2 180.0 3 NaN 4 162.1 5 178.0 6 182.3 7 NaN 8 179.2 9 167.1 10 NaN 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 1df['키'].drop_duplicates() # remove 2nd \"178.0\" &amp; 2nd \"NaN\" 0 173.6 1 177.0 2 180.0 3 NaN 4 162.1 5 178.0 6 182.3 8 179.2 9 167.1 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 1df['키'].drop_duplicates(keep='first') 0 173.6 1 177.0 2 180.0 3 NaN 4 162.1 5 178.0 6 182.3 8 179.2 9 167.1 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 (2) 중복값 중의 마지막을 유지시킴 1df['키'] 0 173.6 1 177.0 2 180.0 3 178.0 4 162.1 5 178.0 6 182.3 7 NaN 8 179.2 9 167.1 10 NaN 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 1df['키'].drop_duplicates(keep='last') 0 173.6 1 177.0 2 180.0 4 162.1 5 178.0 6 182.3 8 179.2 9 167.1 10 NaN 11 183.0 12 175.0 13 176.0 14 174.0 Name: 키, dtype: float64 이때는 해당위치의 값만 제거되고 행 자체는 유지됨 1df['키'] = df['키'].drop_duplicates(keep='last') 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 NaN AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 3-2. 행 전체 제거 df_name .drop_duplicates(“col_name”, keep = …) 지정한 column에서 중복값이 포함되어 있으면 중복값을 포함한 행을 전체 제거 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 NaN AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1df['그룹'] 0 방탄소년단 1 빅뱅 2 NaN 3 방탄소년단 4 마마무 5 방탄소년단 6 뉴이스트 7 아이들 8 방탄소년단 9 핫샷 10 소녀시대 11 아스트로 12 뉴이스트 13 뉴이스트 14 방탄소년단 Name: 그룹, dtype: object 1df.drop_duplicates('그룹') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 1df.drop_duplicates('그룹', keep = 'last') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"},{"name":"Python - 전처리","slug":"【STUDY-Python】/Python-전처리","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%A0%84%EC%B2%98%EB%A6%AC/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"전처리","slug":"전처리","permalink":"https://hyemin-kim.github.io/tags/%EC%A0%84%EC%B2%98%EB%A6%AC/"}]},{"title":"Python >> Pandas 전처리 - (1) row & column 의 추가 및 제거","slug":"S-Python-Pandas-Pre1","date":"2020-06-17T15:02:25.000Z","updated":"2020-11-06T05:18:56.150Z","comments":true,"path":"2020/06/18/S-Python-Pandas-Pre1/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/18/S-Python-Pandas-Pre1/","excerpt":"","text":"row &amp; column 의 추가 및 제거 1. row의 추가 2. column의 추가 3. row의 제거 4. column의 제거 1import pandas as pd 1df = pd.read_csv('korean-idol.csv') 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1. row의 추가 df_name .append ( {…} , ignore_index = True ) dictionary 형태의 데이터를 만들어 준다음 append() 함수를 사용하여 데이터를 추가할 수 있다. ignore_index=True옵션을 반드시 같이 추가해야한다 1df = df.append({'이름': '홍길동', '그룹': 'a그룹', '소속사':'A사', '성별': '남자', '생년월일': '1990-01-01', '키': 185.0, '혈액형': 'B', '브랜드평판지수': 12345678}, ignore_index=True) 1df.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 15 홍길동 a그룹 A사 남자 1990-01-01 185.0 B 12345678 2. column의 추가 새로운 column을 만들고 값을 대입해주면, column이 쉽게 추가될 수 있다 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df['국적'] = '대한민국' 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 국적 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 대한민국 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 대한민국 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 대한민국 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 대한민국 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 대한민국 새로운 column의 값을 다르게 부여하고 싶다면 loc 함수를 활용하면 된다 1df.loc[ df['이름'] == '지드래곤', '국적'] = 'korea' 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 국적 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 대한민국 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 korea 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 대한민국 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 대한민국 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 대한민국 3. row의 제거 하나의 행: df_name .drop (index_num, axis = 0) 복수의 행: df_name .drop ( [ index_num1, index_num2 ], axis = 0) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1df.drop(3, axis = 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1df.drop([3, 5], axis = 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 4. column의 제거 하나의 열: df_name .drop ( ‘col_name’, axis = 1) 복수의 열: df_name .drop ( [ ‘col_name1’, ‘col_name2’ ], axis = 1) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1df.drop(\"그룹\", axis = 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 큐브 여자 1998-08-26 NaN B 4668615 8 진 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 SM 여자 1989-03-09 NaN A 3918661 11 차은우 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 빅히트 남자 1993-03-09 174.0 O 2925442 1df.drop([\"그룹\", \"소속사\"], axis = 1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 남자 1995-10-13 173.6 A 10523260 1 지드래곤 남자 1988-08-18 177.0 A 9916947 2 강다니엘 남자 1996-12-10 180.0 A 8273745 3 뷔 남자 1995-12-30 178.0 AB 8073501 4 화사 여자 1995-07-23 162.1 A 7650928 5 정국 남자 1997-09-01 178.0 A 5208335 6 민현 남자 1995-08-09 182.3 O 4989792 7 소연 여자 1998-08-26 NaN B 4668615 8 진 남자 1992-12-04 179.2 O 4570308 9 하성운 남자 1994-03-22 167.1 A 4036489 10 태연 여자 1989-03-09 NaN A 3918661 11 차은우 남자 1997-03-30 183.0 B 3506027 12 백호 남자 1995-07-21 175.0 AB 3301654 13 JR 남자 1995-06-08 176.0 O 3274137 14 슈가 남자 1993-03-09 174.0 O 2925442 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"},{"name":"Python - 전처리","slug":"【STUDY-Python】/Python-전처리","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%A0%84%EC%B2%98%EB%A6%AC/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"전처리","slug":"전처리","permalink":"https://hyemin-kim.github.io/tags/%EC%A0%84%EC%B2%98%EB%A6%AC/"}]},{"title":"Python >> Pandas 데이터 파악 - (7) 기타","slug":"S-Python-Pandas7","date":"2020-06-17T06:12:40.000Z","updated":"2020-11-06T05:20:05.964Z","comments":true,"path":"2020/06/17/S-Python-Pandas7/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/17/S-Python-Pandas7/","excerpt":"","text":"기타 1. 피벗테이블 2. GroupBy (그룹으로 묶어 보기) 3. Multi-Index (복합 인덱스) 3-1. Multi-Index 적용 3-2. Multi-Index 데이터 프레임을 피벗테이블로 변환 3-3. 인덱스 초기화 (reset_index) 1import pandas as pd 1df = pd.read_csv('korean-idol.csv') 1. 피벗테이블 데이터 열 중에서 두 개의 열을 각각 행 인덱스, 열 인덱스로 사용하여 데이터를 조회하여 펼쳐놓은 건을 의미함 왼쪽에 나타나는 인덱스를 행 인덱스, 상단에 나타나는 인덱스를 열 인덱스라고 부른다 pd.pivot_table(df_name, index = “col_name_분류기준1”, columns = “col_name_분류기준2”, values = “col_name_조회대상”, aggfunc = …) index는 행 인덱스 columns는 열 인덱스 values는 조회하고 싶은 값 aggfunc는 value를 산출하는 연산법 (1) e.g.: aggfunc = np.sum / np.mean (2) 설정하지 않은 경우 기본적으로 평균값을 구한다 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1pd.pivot_table(df, index = \"소속사\", columns = \"혈액형\", values = \"키\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 혈액형 A AB B O 소속사 RBW 162.1 NaN NaN NaN YG 177.0 NaN NaN NaN 빅히트 175.8 178.0 NaN 176.60 스타크루이엔티 167.1 NaN NaN NaN 커넥트 180.0 NaN NaN NaN 판타지오 NaN NaN 183.0 NaN 플레디스 NaN 175.0 NaN 179.15 1import numpy as np 1pd.pivot_table(df, index = \"그룹\", columns = \"혈액형\", values = \"브랜드평판지수\", aggfunc = np.sum) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 혈액형 A AB B O 그룹 뉴이스트 NaN 3301654.0 NaN 8263929.0 마마무 7650928.0 NaN NaN NaN 방탄소년단 15731595.0 8073501.0 NaN 7495750.0 빅뱅 9916947.0 NaN NaN NaN 소녀시대 3918661.0 NaN NaN NaN 아스트로 NaN NaN 3506027.0 NaN 아이들 NaN NaN 4668615.0 NaN 핫샷 4036489.0 NaN NaN NaN 2. GroupBy (그룹으로 묶어 보기) groupby는 데이터를 그룹으로 묶어 분석할 때 활용한다 소속사별 키의 평균, 성별 키의 평균 등 특정, 그룹별 통계 및 데이터의 성질을 확인하고자 할 때 활용한다 groupby와 함께 count() - 갯수 sum() - 합계 mean() - 평균 var() - 분산 std() -표준편차 min() / max() - 최소값, 최대값 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df.groupby(\"소속사\") &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000024E760EC288&gt; 1df.groupby('소속사').count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 성별 생년월일 키 혈액형 브랜드평판지수 소속사 RBW 1 1 1 1 1 1 1 SM 1 1 1 1 0 1 1 YG 1 1 1 1 1 1 1 빅히트 5 5 5 5 5 5 5 스타크루이엔티 1 1 1 1 1 1 1 커넥트 1 0 1 1 1 1 1 큐브 1 1 1 1 0 1 1 판타지오 1 1 1 1 1 1 1 플레디스 3 3 3 3 3 3 3 산술 통계는 자동으로 산술통계가 가능한 열만 출력됨. 1df.groupby('그룹').mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 키 브랜드평판지수 그룹 뉴이스트 177.766667 3.855194e+06 마마무 162.100000 7.650928e+06 방탄소년단 176.560000 6.260169e+06 빅뱅 177.000000 9.916947e+06 소녀시대 NaN 3.918661e+06 아스트로 183.000000 3.506027e+06 아이들 NaN 4.668615e+06 핫샷 167.100000 4.036489e+06 1df.groupby('성별').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 키 브랜드평판지수 성별 남자 2123.2 68599637 여자 162.1 16238204 특정 열만 출력하고 싶다면? 1df.groupby('혈액형')['키'].mean() 혈액형 A 172.966667 AB 176.500000 B 183.000000 O 177.875000 Name: 키, dtype: float64 3. Multi-Index (복합 인덱스) 3-1. Multi-Index 적용 행 인덱스를 복합적으로 구성하고 싶은 경우는 인덱스를 리스트로 만들어 준다 df_name .groupby([‘col_name_1’,‘col_name_2’]) .mean() 데이터를 먼저 col_1기준으로 분류한 다음, col_2기준으로 한번 더 분류한다. 2번 분류 후의 데이터에 대해 산술통계값을 구한다 1df.groupby(['혈액형', '성별']).mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 키 브랜드평판지수 혈액형 성별 A 남자 175.140 7591755.20 여자 162.100 5784794.50 AB 남자 176.500 5687577.50 B 남자 183.000 3506027.00 여자 NaN 4668615.00 O 남자 177.875 3939919.75 3-2. Multi-Index 데이터 프레임을 피벗테이블로 변환 Multi-Index로 된 데이터프레임을 피벗테이블 형태로 다시 변환해줄 수 있다 df_name .unstack( ‘col_열’ ) col_열: groupby에서 선택한 두 column중 pivot table의 열인덱스로 지정해주고 싶은 column명을 입력 1df2 = df.groupby(['혈액형', '성별']).mean() 1df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 키 브랜드평판지수 혈액형 성별 A 남자 175.140 7591755.20 여자 162.100 5784794.50 AB 남자 176.500 5687577.50 B 남자 183.000 3506027.00 여자 NaN 4668615.00 O 남자 177.875 3939919.75 1df2.unstack('혈액형') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } 키 브랜드평판지수 혈액형 A AB B O A AB B O 성별 남자 175.14 176.5 183.0 177.875 7591755.2 5687577.5 3506027.0 3939919.75 여자 162.10 NaN NaN NaN 5784794.5 NaN 4668615.0 NaN 1df2.unstack('성별') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } 키 브랜드평판지수 성별 남자 여자 남자 여자 혈액형 A 175.140 162.1 7591755.20 5784794.5 AB 176.500 NaN 5687577.50 NaN B 183.000 NaN 3506027.00 4668615.0 O 177.875 NaN 3939919.75 NaN 3-3. 인덱스 초기화 (reset_index) reset_index() 는 Multi-Index로 구성된 데이터 프레임의 인덱스를 초기화해 준다 그 의미는 Multi-Index로 구성된 데이터 프레임 중의 index들을 dataframe의 column으로 변환시키는 것 df_name = df_name .reset_index() 1df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 키 브랜드평판지수 혈액형 성별 A 남자 175.140 7591755.20 여자 162.100 5784794.50 AB 남자 176.500 5687577.50 B 남자 183.000 3506027.00 여자 NaN 4668615.00 O 남자 177.875 3939919.75 1df2 = df2.reset_index() 1df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 혈액형 성별 키 브랜드평판지수 0 A 남자 175.140 7591755.20 1 A 여자 162.100 5784794.50 2 AB 남자 176.500 5687577.50 3 B 남자 183.000 3506027.00 4 B 여자 NaN 4668615.00 5 O 남자 177.875 3939919.75 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"데이터파악","slug":"데이터파악","permalink":"https://hyemin-kim.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%8C%8C%EC%95%85/"}]},{"title":"Python >> Pandas 데이터 파악 - (6) 결측값 확인 및 추출","slug":"S-Python-Pandas6","date":"2020-06-11T16:21:05.000Z","updated":"2020-11-06T05:20:02.104Z","comments":true,"path":"2020/06/12/S-Python-Pandas6/","link":"","permalink":"https://hyemin-kim.github.io/2020/06/12/S-Python-Pandas6/","excerpt":"","text":"결측값 확인 및 추출 1. 결측값에 대하여 2. column별 (비)결측값 개수 확인 – info() 3. (비)결측값 위치 확인 3-1. 전체 Data 3-2. 특정 column 4. (비)결측값 추출 4-1. 해당 column만 추출 4-2. 전체 column 추출 4-3. 지정한 column 추출 1import pandas as pd 1df = pd.read_csv('korean-idol.csv') 1. 결측값에 대하여 Null 값은 비어있는 값, 고급 언어로 결측값이다 pandas 에서는 NaN =&gt; Not a Number 로 표기 된다 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 2. column별 (비)결측값 개수 확인 – info() info() 로 각 column별의 결측값(NaN) 개수를 쉽게 확인할 수 있다. 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 15 entries, 0 to 14 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 이름 15 non-null object 1 그룹 14 non-null object 2 소속사 15 non-null object 3 성별 15 non-null object 4 생년월일 15 non-null object 5 키 13 non-null float64 6 혈액형 15 non-null object 7 브랜드평판지수 15 non-null int64 dtypes: float64(1), int64(1), object(6) memory usage: 1.1+ KB 3. (비)결측값 위치 확인 .isna() .isnull() .notna() .notnull() 3-1. 전체 Data df_name .명령어 (1) 결측값 = True 1df.isna() 1df.isnull() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 False False False False False False False False 1 False False False False False False False False 2 False True False False False False False False 3 False False False False False False False False 4 False False False False False False False False 5 False False False False False False False False 6 False False False False False False False False 7 False False False False False True False False 8 False False False False False False False False 9 False False False False False False False False 10 False False False False False True False False 11 False False False False False False False False 12 False False False False False False False False 13 False False False False False False False False 14 False False False False False False False False (2) 비결측값 = True 1df.notna() 1df.notnull() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 True True True True True True True True 1 True True True True True True True True 2 True False True True True True True True 3 True True True True True True True True 4 True True True True True True True True 5 True True True True True True True True 6 True True True True True True True True 7 True True True True True False True True 8 True True True True True True True True 9 True True True True True True True True 10 True True True True True False True True 11 True True True True True True True True 12 True True True True True True True True 13 True True True True True True True True 14 True True True True True True True True 3-2. 특정 column df_name [ ‘col_name’ ] .명령어 (1) 결측값 = True 1df['그룹'].isna() 1df['그룹'].isnull() 0 False 1 False 2 True 3 False 4 False 5 False 6 False 7 False 8 False 9 False 10 False 11 False 12 False 13 False 14 False Name: 그룹, dtype: bool (2) 비결측값 = True 1df['그룹'].notna() 1df['그룹'].notnull() 0 True 1 True 2 False 3 True 4 True 5 True 6 True 7 True 8 True 9 True 10 True 11 True 12 True 13 True 14 True Name: 그룹, dtype: bool 4. (비)결측값 추출 4-1. 해당 column만 추출 결측값: df_name [ ‘col_name’] [ df_name [ ‘col_name’ ] .isna() / isnull() ] 비결측값: df_name [ ‘col_name’ ] [df_name [ ‘col_name’ ] .notna() / notnull()] 1df['그룹'][df['그룹'].isna()] 2 NaN Name: 그룹, dtype: object 1df['그룹'][df['그룹'].notnull()] 0 방탄소년단 1 빅뱅 3 방탄소년단 4 마마무 5 방탄소년단 6 뉴이스트 7 아이들 8 방탄소년단 9 핫샷 10 소녀시대 11 아스트로 12 뉴이스트 13 뉴이스트 14 방탄소년단 Name: 그룹, dtype: object 4-2. 전체 column 추출 결측값: df_name .loc [df_name [ ‘col_name’ ] .isna() / isnull() ] 비결측값: df_name .loc [df_name ['col_name] .notna() / notnull() ] 1df.loc[df['그룹'].isna()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 1df.loc[df['그룹'].notnull()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 4-3. 지정한 column 추출 결측값: df_name .loc [df_name [ ‘na_col_name’ ] .isna() / isnull() , [‘col_name1’, ‘col_name2’, …]] 비결측값: df_name .loc [df_name ['na_col_name] .notna() / notnull() , [‘col_name1’, ‘col_name2’, …]] 1df.loc[df['그룹'].isna(), ['이름', '소속사']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 소속사 2 강다니엘 커넥트 1df.loc[df['그룹'].notnull(), ['이름', '소속사']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 소속사 0 지민 빅히트 1 지드래곤 YG 3 뷔 빅히트 4 화사 RBW 5 정국 빅히트 6 민현 플레디스 7 소연 큐브 8 진 빅히트 9 하성운 스타크루이엔티 10 태연 SM 11 차은우 판타지오 12 백호 플레디스 13 JR 플레디스 14 슈가 빅히트 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"데이터파악","slug":"데이터파악","permalink":"https://hyemin-kim.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%8C%8C%EC%95%85/"}]},{"title":"Python >> Pandas 데이터 파악 - (5) 범위선택","slug":"S-Python-Pandas5","date":"2020-05-24T12:58:03.000Z","updated":"2020-11-06T05:19:58.633Z","comments":true,"path":"2020/05/24/S-Python-Pandas5/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/24/S-Python-Pandas5/","excerpt":"","text":"범위선택 1. 단일 column을 선택하는 방법 2. index &amp; column 범위 선택 (range selection) 2-1. 단순 index에 대한 범위 선택 2-2. index &amp; column 범위선택 – loc 2-3. index &amp; column 범위선택 – iloc (position으로 색인) 3. index &amp; column 조건범위선택 – Boolean Indexing 3-1. 조건에 만족한 row들의 모든 column을 추출 3-2. 조건에 만족한 row들의 특정 column들을 추출 4. index &amp; column 조건범위선택 – inis을 활용란 색인 1import pandas as pd 1df = pd.read_csv('korean-idol.csv') 1. 단일 column을 선택하는 방법 df_name [ 'col_name ’ ] df_name [ \"col_name \" ] df_name .col_name 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df['이름'] 1df[\"이름\"] 1df.이름 0 지민 1 지드래곤 2 강다니엘 3 뷔 4 화사 5 정국 6 민현 7 소연 8 진 9 하성운 10 태연 11 차은우 12 백호 13 JR 14 슈가 Name: 이름, dtype: object 2. index &amp; column 범위 선택 (range selection) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 2-1. 단순 index에 대한 범위 선택 1df[:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 1df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 2-2. index &amp; column 범위선택 – loc df_name .loc [행(index) 범위, (열)column 범위] 행 범위는 “:” “:b” “a:b” 등 형식을 사용 열 범위는 'column name ’ ['column name1 ', 'column name2 '] 'column name1 ’ : 'column name2 ’ 등 형식을 사용 주의: pandas의 loc에서 범위 a : b는 index a &amp; index b 모두 포함 numpy에서는 index a 포함, index b 미포함 1df.loc[:, '이름'] 0 지민 1 지드래곤 2 강다니엘 3 뷔 4 화사 5 정국 6 민현 7 소연 8 진 9 하성운 10 태연 11 차은우 12 백호 13 JR 14 슈가 Name: 이름, dtype: object 1df.loc[:, ['이름', '생년월일']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 생년월일 0 지민 1995-10-13 1 지드래곤 1988-08-18 2 강다니엘 1996-12-10 3 뷔 1995-12-30 4 화사 1995-07-23 5 정국 1997-09-01 6 민현 1995-08-09 7 소연 1998-08-26 8 진 1992-12-04 9 하성운 1994-03-22 10 태연 1989-03-09 11 차은우 1997-03-30 12 백호 1995-07-21 13 JR 1995-06-08 14 슈가 1993-03-09 1df.loc[3:8, ['이름', '생년월일']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 생년월일 3 뷔 1995-12-30 4 화사 1995-07-23 5 정국 1997-09-01 6 민현 1995-08-09 7 소연 1998-08-26 8 진 1992-12-04 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df.loc[2:5, '이름':'생년월일'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 2 강다니엘 NaN 커넥트 남자 1996-12-10 3 뷔 방탄소년단 빅히트 남자 1995-12-30 4 화사 마마무 RBW 여자 1995-07-23 5 정국 방탄소년단 빅히트 남자 1997-09-01 2-3. index &amp; column 범위선택 – iloc (position으로 색인) 행(index) 범위 선택은 loc와 동일 열(column) 범위는 'column 명’대신 column position을 사용 행 범위는 “:” “:b” “a:b” 등 형식을 사용 열 범위는 “c” “[c, d]” “c:d” 등 형식을 사용 주의: pandas의 iloc에서 범위 a : b는 index a 포함, index b 미포함 (numpy와 동일) pandas의 loc에서 범위 a : b는 index a &amp; index b 모두 포함 1df.iloc[:, [0, 2]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 소속사 0 지민 빅히트 1 지드래곤 YG 2 강다니엘 커넥트 3 뷔 빅히트 4 화사 RBW 5 정국 빅히트 6 민현 플레디스 7 소연 큐브 8 진 빅히트 9 하성운 스타크루이엔티 10 태연 SM 11 차은우 판타지오 12 백호 플레디스 13 JR 플레디스 14 슈가 빅히트 1df.iloc[1:5, [0, 2]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 소속사 1 지드래곤 YG 2 강다니엘 커넥트 3 뷔 빅히트 4 화사 RBW 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df.iloc[1:5, 0:4] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 1 지드래곤 빅뱅 YG 남자 2 강다니엘 NaN 커넥트 남자 3 뷔 방탄소년단 빅히트 남자 4 화사 마마무 RBW 여자 3. index &amp; column 조건범위선택 – Boolean Indexing Boolean indexing은 Numpy에서의 Boolean indexing과 같은 원리다 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 3-1. 조건에 만족한 row들의 모든 column을 추출 df [조건 ] 1df['키'] &gt; 180 0 False 1 False 2 False 3 False 4 False 5 False 6 True 7 False 8 False 9 False 10 False 11 True 12 False 13 False 14 False Name: 키, dtype: bool 1df[df['키'] &gt; 180] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 3-2. 조건에 만족한 row들의 특정 column들을 추출 방법 1. df_name [조건 ] [column범위 ] 1df[ df['키'] &gt; 180 ] ['이름'] 6 민현 11 차은우 Name: 이름, dtype: object 1df [ df['키'] &gt; 180 ] [['이름', '키']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 키 6 민현 182.3 11 차은우 183.0 방법 2. loc를 활용: df_name.loc[ 조건 , column범위 ] 【추천】 1df.loc[ df['키'] &gt; 180, '이름' ] 6 민현 11 차은우 Name: 이름, dtype: object 1df.loc[ df['키'] &gt; 180, ['이름', '그룹'] ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 6 민현 뉴이스트 11 차은우 아스트로 1df.loc[ df['키'] &gt; 180, '이름' : '성별'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 6 민현 뉴이스트 플레디스 남자 11 차은우 아스트로 판타지오 남자 4. index &amp; column 조건범위선택 – inis을 활용란 색인 column값이 미리 정의한 list에 속한다는 조건을 걸고자 할 때 사용한다 1my_condition = ['플레디스', 'SM'] 1df['소속사'].isin(my_condition) 0 False 1 False 2 False 3 False 4 False 5 False 6 True 7 False 8 False 9 False 10 True 11 False 12 True 13 True 14 False Name: 소속사, dtype: bool 1df.loc[ df['소속사'].isin(my_condition) ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 1df.loc[ df['소속사'].isin(my_condition) , ['이름', '소속사'] ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 소속사 6 민현 플레디스 10 태연 SM 12 백호 플레디스 13 JR 플레디스 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"데이터파악","slug":"데이터파악","permalink":"https://hyemin-kim.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%8C%8C%EC%95%85/"}]},{"title":"Python >> Pandas 데이터 파악 - (4) 정렬","slug":"S-Python-Pandas4","date":"2020-05-24T08:07:08.000Z","updated":"2020-11-06T05:19:54.859Z","comments":true,"path":"2020/05/24/S-Python-Pandas4/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/24/S-Python-Pandas4/","excerpt":"","text":"정렬 (sort) 1. index 순으로 정렬 2. column의 value순으로 정렬 2-1. 단일 column 기준 2-2. 복수 column 기준 1import pandas as pd 1df = pd.read_csv('korean-idol.csv') 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1. index 순으로 정렬 오름차순 정렬: df_name.sort_index() (default) 내림차순 정렬: df_name.sort_index(ascending = False) 1df.sort_index() # 오름차순 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1df.sort_index(ascending = False) # 내림차순 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 2. column의 value순으로 정렬 오름차순 정렬: df_name.sort_values(by = ‘col_name’) 내림차순 정렬: df_name.sort_values(by = ‘col_name’, ascending = False) 2-1. 단일 column 기준 1df.sort_values(by='키') # 오름차순 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 1df.sort_values(by = '키', ascending = False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 2-2. 복수 column 기준 먼저 column1 기준으로 정렬하고, column1 값이 동일한 row들은 column2기준으로 정렬: df_name .sort_value ( by = [ ‘col_name 1’ , ‘col_name 2’ ] ) 1df.sort_values(by = ['키', '브랜드평판지수']) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 1df.sort_values(by = ['키', '브랜드평판지수'], ascending = False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"데이터파악","slug":"데이터파악","permalink":"https://hyemin-kim.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%8C%8C%EC%95%85/"}]},{"title":"Python >> Pandas 데이터 파악 - (3) 기본정보 & 통계정보 파악","slug":"S-Python-Pandas3","date":"2020-05-24T08:06:08.000Z","updated":"2020-11-06T05:19:51.023Z","comments":true,"path":"2020/05/24/S-Python-Pandas3/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/24/S-Python-Pandas3/","excerpt":"","text":"기본정보 &amp; 통계정보 파악 1. 파일 읽어오기 (csv) 2. 기본 행&amp;열 정보 알아보기 (column, index, info) 2-1. column (열) 이름 출력하기 2-2. column (열) 이름 재정의하기 2-3. index (행) 정보 출력하기 2-4. info (기본적인 column 정보와 데이터 타입) 3. 형태 (shape) 알아보기 4. 상위 5개, 하위 5개의 정보만 보기 5. 통계 정보 알아보기 5-1. 전체 통계 정보 5-2. 최소값(min), 최대값(max), 중앙값(median), 최빈값(mode) 5-3. 합계(sum), 평균(mean), 분산(var), 표준편차(std) 5-4. 갯수를 세는 count 1import pandas as pd 1. 파일 읽어오기 (csv) 1df = pd.read_csv('korean-idol.csv') 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 2. 기본 행&amp;열 정보 알아보기 (column, index, info) 2-1. column (열) 이름 출력하기 df_name .columns 1df.columns Index(['이름', '그룹', '소속사', '성별', '생년월일', '키', '혈액형', '브랜드평판지수'], dtype='object') ​ 2-2. column (열) 이름 재정의하기 (1) 전체 column 이름 df_name .columns = […] 예: “이름” --&gt; “name”: 1new_col = ['name', '그룹', '소속사', '성별', '생년월일', '키', '혈액형', '브랜드평판지수'] 1df.columns = new_col 1df.columns Index(['name', '그룹', '소속사', '성별', '생년월일', '키', '혈액형', '브랜드평판지수'], dtype='object') ​ (2) 개별 column 이름 df_name .rename ( columns = { “old_name” : “new_name” } ) ​ 1df = pd.read_csv('korean-idol.csv') 1df.columns Index(['이름', '그룹', '소속사', '성별', '생년월일', '키', '혈액형', '브랜드평판지수'], dtype='object') ​ 1df = df.rename(columns = {\"이름\" : \"name\"}) 1df.columns Index(['name', '그룹', '소속사', '성별', '생년월일', '키', '혈액형', '브랜드평판지수'], dtype='object') ​ 2-3. index (행) 정보 출력하기 df_name .index 1df.index RangeIndex(start=0, stop=15, step=1) 2-4. info (기본적인 column 정보와 데이터 타입) df_name .info() Tip: info메소드는 주로 빠진 값 (null 값)과 데이터 타입을 볼 때 활용함 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 15 entries, 0 to 14 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 name 15 non-null object 1 그룹 14 non-null object 2 소속사 15 non-null object 3 성별 15 non-null object 4 생년월일 15 non-null object 5 키 13 non-null float64 6 혈액형 15 non-null object 7 브랜드평판지수 15 non-null int64 dtypes: float64(1), int64(1), object(6) memory usage: 1.1+ KB “object” type은 주로 문자형 데이터를 가리킴. 3. 형태 (shape) 알아보기 shape는 tuple형태로 반환되며, 첫번째는 row, 두번째는 column의 숫자를 의미함. 1df.shape (15, 8) 4. 상위 5개, 하위 5개의 정보만 보기 상위 5개 row: df_name .head() 하위 5개 row: df_name .tail() 상위 n개 row: df_name .head(n) 하위 n개 row: df_name .tail(n) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 1df.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 5. 통계 정보 알아보기 통계값은 산술 연산이 가능한 숫자형 (float / int) 인 column을 다룬다 5-1. 전체 통계 정보 df_name .describe() 산술 연산이 가능한 column만 출력됨 1df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 키 브랜드평판지수 count 13.000000 1.500000e+01 mean 175.792308 5.655856e+06 std 5.820576 2.539068e+06 min 162.100000 2.925442e+06 25% 174.000000 3.712344e+06 50% 177.000000 4.668615e+06 75% 179.200000 7.862214e+06 max 183.000000 1.052326e+07 5-2. 최소값(min), 최대값(max), 중앙값(median), 최빈값(mode) 최소값: df_name [ ‘col_name’ ] .min() 최대값: df_name [ ‘col_name’ ] .max() 중앙값: df_name [ ‘col_name’ ] .median() 최빈값: df_name [ ‘col_name’ ] .mode() 1df['키'].min() 162.1 1df['키'].max() 183.0 1df['키'].median() 177.0 1df['키'].mode() 0 178.0 dtype: float64 5-3. 합계(sum), 평균(mean), 분산(var), 표준편차(std) 합계(sum): df_name [ ‘col_name’ ] .sum() 평균(mean): df_name [ ‘col_name’ ] .mean() 분산(variance): df_name [ ‘col_name’ ] .var() 표준편차(standard deviation): df_name [ ‘col_name’ ] .std() 1df['키'].sum() 2285.3 1df['키'].mean() 175.7923076923077 1df['키'].var() 33.879102564102595 1df['키'].std() 5.820575793175672 5-4. 갯수를 세는 count df_name [ ‘col_name’ ] .count 1df['키'].count() 13 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"데이터파악","slug":"데이터파악","permalink":"https://hyemin-kim.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%8C%8C%EC%95%85/"}]},{"title":"Python >> Pandas 데이터 파악 - (2) 파일 불러오기 및 복사","slug":"S-Python-Pandas2","date":"2020-05-24T06:04:59.000Z","updated":"2020-11-06T05:19:47.245Z","comments":true,"path":"2020/05/24/S-Python-Pandas2/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/24/S-Python-Pandas2/","excerpt":"","text":"파일 불러오기 및 복사 1. csv파일 읽어오기 – \"pd.read_csv\" 1-1. Jupyter Notebook 기반 1-2. Colab 기반 2. Excle파일 읽어오기 – \"pd.read_excel\" 2-1. Jupyter Notebook 기반 2-2. Colab 기반 3. 복사 (copy) 1. csv파일 읽어오기 – \"pd.read_csv\" 1-1. Jupyter Notebook 기반 1import pandas as pd 1pd.read_csv('korean-idol.csv') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 1-2. Colab 기반 방법 1. 로컬에서 파일 읽어오기 123456from google.colab import filesmyfile = files.upload()import iopd.read_csv(io.BytesIO(myfile['korean-idol.csv'])) 방법 2: 구글 드라이브에 있는 샘플 파일 읽어오기 123456789from google.colab import drivedrive.mount('/content/drive')# 나타나는 link에 따라 google drive 로그인하여 link복사, # 'Enter your authorization code:'에서 복사된 link를 입력filename = 'colab 왼쪽 목록에서 파일 경로를 복사하여 붙혀놓기'pd.read_csv(filename) 2. Excle파일 읽어오기 – \"pd.read_excel\" 2-1. Jupyter Notebook 기반 1pd.read_excel('korean-idol.xlsx') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 5 정국 방탄소년단 빅히트 남자 1997-09-01 178.0 A 5208335 6 민현 뉴이스트 플레디스 남자 1995-08-09 182.3 O 4989792 7 소연 아이들 큐브 여자 1998-08-26 NaN B 4668615 8 진 방탄소년단 빅히트 남자 1992-12-04 179.2 O 4570308 9 하성운 핫샷 스타크루이엔티 남자 1994-03-22 167.1 A 4036489 10 태연 소녀시대 SM 여자 1989-03-09 NaN A 3918661 11 차은우 아스트로 판타지오 남자 1997-03-30 183.0 B 3506027 12 백호 뉴이스트 플레디스 남자 1995-07-21 175.0 AB 3301654 13 JR 뉴이스트 플레디스 남자 1995-06-08 176.0 O 3274137 14 슈가 방탄소년단 빅히트 남자 1993-03-09 174.0 O 2925442 2-2. Colab 기반 구글 드라이브에 있는 샘플 파일 읽어오기 123456from google.colab import drivedrive.mount('/content/drive')filename = '파일 경로 붙혀놓기'pd.read_excel(filename) 3. 복사 (copy) dataframe을 복사할 때 \"df_name.copy()\"를 사용한다 \"=\"를 사용하여 원본데이터를 \"복사\"하면 복사된 데이터를 수정할 때 원본 데이터도 같이 변화한다 1df = pd.read_csv('korean-idol.csv') 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df_new = df 1df_new.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df_new['이름'] = 0 1df_new.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 0 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 0 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 0 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 0 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 0 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 0 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 0 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 0 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 0 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 0 마마무 RBW 여자 1995-07-23 162.1 A 7650928 이렇게 되는 이유는 두 dataframe이 같은 메모리 주소를 참조하기 때문이다. 1hex(id(df_new)) '0x25109f6e6c8' 1hex(id(df)) '0x25109f6e6c8' 원본 데이터를 유지 시키고, 새로운 변수에 복사할 때 copy() 를 사용한다 1df = pd.read_csv('korean-idol.csv') 1df_copy = df.copy() 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df_copy.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 복사된 dataframe이 원본 데이터와 같은 메모리 주소를 참조한다. 1hex(id(df)) '0x25109fefa48' 1hex(id(df_copy)) '0x25109ff4408' copy본을 수정할 때 원본 데이터가 유지된다 1df_copy['이름'] = 0 1df_copy.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 0 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 0 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 0 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 0 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 0 마마무 RBW 여자 1995-07-23 162.1 A 7650928 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 그룹 소속사 성별 생년월일 키 혈액형 브랜드평판지수 0 지민 방탄소년단 빅히트 남자 1995-10-13 173.6 A 10523260 1 지드래곤 빅뱅 YG 남자 1988-08-18 177.0 A 9916947 2 강다니엘 NaN 커넥트 남자 1996-12-10 180.0 A 8273745 3 뷔 방탄소년단 빅히트 남자 1995-12-30 178.0 AB 8073501 4 화사 마마무 RBW 여자 1995-07-23 162.1 A 7650928 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"데이터파악","slug":"데이터파악","permalink":"https://hyemin-kim.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%8C%8C%EC%95%85/"}]},{"title":"Python >> Pandas 데이터 파악 - (1) Series와 DataFrame","slug":"S-Python-Pandas1","date":"2020-05-22T11:37:46.000Z","updated":"2020-11-06T05:19:43.274Z","comments":true,"path":"2020/05/22/S-Python-Pandas1/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/22/S-Python-Pandas1/","excerpt":"","text":"Series &amp; DataFrame 1. pandas 패키지 로드 2. pandas의 Series 와 DataFrame 2-1. Series 2-2. DataFrame 방법 1. list로 만들기 방법 2. dict로 만들기 2-3. index를 특정column으로 지정하기 2-4. column = Series 1. pandas 패키지 로드 1import pandas 별칭은 주로 pd로 사용한다 1import pandas as pd 1pd &lt;module 'pandas' from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\pandas\\\\__init__.py'&gt; 2. pandas의 Series 와 DataFrame 1차원, 1개의 column은 Series라고 한다 2-1. Series Series 생성: pd.Series(“list”) pd.Series(“list_name”) (1) pd.Series(“list”) 1pd.Series([1, 2, 3, 4]) 0 1 1 2 2 3 3 4 dtype: int64 (2) pd.Series(“list_name”) 1a = [1, 2, 3, 4] 1pd.Series(a) 0 1 1 2 2 3 3 4 dtype: int64 1mylist = [1, 2, 3, 4] 1pd.Series(mylist) 0 1 1 2 2 3 3 4 dtype: int64 2-2. DataFrame 방법 1. list로 만들기 123company1 = [['삼성', 2000, '스마트폰'], ['현대', 1000, '자동차'], ['네이버', 500, '포털']] 1pd.DataFrame(company1) .dataframe tbody tr th:only-of-type { vertical-align: middle } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 삼성 2000 스마트폰 1 현대 1000 자동차 2 네이버 500 포털 &lt;활용을 하기 위해 DataFrame을 변수에 지정하기&gt; 1df1 = pd.DataFrame(company1) 1df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 삼성 2000 스마트폰 1 현대 1000 자동차 2 네이버 500 포털 &lt;제목컬럼 만들기&gt; – “dfname.column = [ ]” 1df1.columns = ['기업명', '매출액', '업종'] 1df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 기업명 매출액 업종 0 삼성 2000 스마트폰 1 현대 1000 자동차 2 네이버 500 포털 주의: column명의 개수는 반드시 DataFrame의 column수와 동일해야 함 방법 2. dict로 만들기 1234company2 = {'기업명': ['삼성', '현대', '네이버'], '매출액': [2000, 1000, 500], '업종': ['스므트폰', '자동차', '포털'] } 1df2 = pd.DataFrame(company2) 1df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 기업명 매출액 업종 0 삼성 2000 스므트폰 1 현대 1000 자동차 2 네이버 500 포털 2-3. index를 특정column으로 지정하기 “dfname.index = [ ]” 명령을 사용한다 1df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 기업명 매출액 업종 0 삼성 2000 스마트폰 1 현대 1000 자동차 2 네이버 500 포털 1df1.index = df1['기업명'] 1df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 기업명 매출액 업종 기업명 삼성 삼성 2000 스마트폰 현대 현대 1000 자동차 네이버 네이버 500 포털 2-4. column = Series 1df1['매출액'] 기업명 삼성 2000 현대 1000 네이버 500 Name: 매출액, dtype: int64 1type(df1['매출액']) pandas.core.series.Series document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"데이터파악","slug":"데이터파악","permalink":"https://hyemin-kim.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%8C%8C%EC%95%85/"}]},{"title":"Python >> Numpy - (4) 행렬. Broadcasting","slug":"S-Python-Numpy4","date":"2020-05-20T07:55:34.000Z","updated":"2020-11-06T05:18:49.013Z","comments":true,"path":"2020/05/20/S-Python-Numpy4/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/20/S-Python-Numpy4/","excerpt":"행렬 (덧셈, 뺄셈, 곱셈). Broadcasting.","text":"행렬 (덧셈, 뺄셈, 곱셈). Broadcasting. 목록 1. 행렬 - 덧셈 1-1. 덧셈 1-2. Sum – Matrix안의 계산 2. 행렬 - 뺄셈 3. 행렬 - 곱셈 3-1. 일반 곱셈 3-2. dot product / 내적곱 4. Broadcasting 4-1. 숫자의 연산 4-2. array (배열)의 broadcasting 1import numpy as np 1. 행렬 - 덧셈 행렬의 shape이 같아야 덧셈 가능 1-1. 덧셈 12a = np.array([[1, 2, 3], [2, 3, 4]]) 12b = np.array([[3, 4, 5], [1, 2, 3]]) 1a + b array([[4, 6, 8], [3, 5, 7]]) 12a = np.array([[1, 2, 3], [2, 3, 4]]) 123b = np.array([[1, 2], [3, 4], [5, 6]]) 1a + b # shape이 다르면 error발생 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-7-37f7d36ad418&gt; in &lt;module&gt; ----&gt; 1 a + b # shape이 다르면 error발생 ValueError: operands could not be broadcast together with shapes (2,3) (3,2) 1-2. Sum – Matrix안의 계산 명령어: np.sum(‘array_name’, axis = ‘0/1/…’) 주의: 계산할 때 axis의 방향대로 Sum을 구한다. 예를 들면, 2darray에서, axis = 0 이면: 수직방향으로 Sum을 구한다 axis = 1 이면: 수평방향으로 Sum을 구한다 12a = np.array([[1, 2, 3], [2, 3, 4]]) 1np.sum(a, axis = 0) array([3, 5, 7]) 1np.sum(a, axis = 1) array([6, 9]) 2. 행렬 - 뺄셈 12a = np.array([[1, 2, 3], [2, 3, 4]]) 12b = np.array([[3, 4, 5], [1, 2, 3]]) 1a - b array([[-2, -2, -2], [ 1, 1, 1]]) 12a = np.array([[1, 2, 3], [2, 3, 4]]) 123b = np.array([[1, 2], [3, 4], [5, 6]]) 1a - b # shape이 다르면 error발생 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-18-e62ba154daaa&gt; in &lt;module&gt; ----&gt; 1 a - b # shape이 다르면 error발생 ValueError: operands could not be broadcast together with shapes (2,3) (3,2) 3. 행렬 - 곱셈 3-1. 일반 곱셈 일반곱셈은 덧셈과 뺏셈이랑 동일하게 같은 위치에 있는 애들끼리 곱한다. [shape이 완전 같아야 함] 12a = np.array([[1, 2, 3], [2, 3, 4]]) 12b = np.array([[3, 4, 5], [1, 2, 3]]) 1a * b array([[ 3, 8, 15], [ 2, 6, 12]]) 3-2. dot product / 내적곱 [맞닿는 shape이 같아야 함] 12a = np.array([[1, 2, 3], [2, 3, 4]]) 123b = np.array([[1, 2], [3, 4], [5, 6]]) 1a.shape, b.shape ((2, 3), (3, 2)) 방법 1: np.dot(a, b) 1np.dot(a, b) array([[22, 28], [31, 40]]) 방법2: a.dot(b) 1a.dot(b) array([[22, 28], [31, 40]]) 4. Broadcasting 4-1. 숫자의 연산 array a 의 모든 원소에 3을 더하고 싶다면: 단순히 행렬 덧셈을 사용할 때: 12a = np.array([[1, 2, 3], [2, 3, 4]]) 12b = np.array([[3, 3, 3], [3, 3, 3]]) 1a + b array([[4, 5, 6], [5, 6, 7]]) Broadcasting 사용할 때: 12a = np.array([[1, 2, 3], [2, 3, 4]]) 1a + 3 array([[4, 5, 6], [5, 6, 7]]) 1a - 3 array([[-2, -1, 0], [-1, 0, 1]]) 1a * 3 array([[ 3, 6, 9], [ 6, 9, 12]]) 1a / 3 array([[0.33333333, 0.66666667, 1. ], [0.66666667, 1. , 1.33333333]]) 4-2. array (배열)의 broadcasting original array의 shape이 유지됨. 12a = np.array([[1, 2, 3], [2, 3, 4]]) 12b = np.array([[1], [2]]) 1a.shape, b.shape ((2, 3), (2, 1)) 1a * b array([[1, 2, 3], [4, 6, 8]]) 12a = np.array([[1, 2, 3], [2, 3, 4]]) 1b = np.array([1, 2, 3]) 1a * b array([[ 1, 4, 9], [ 2, 6, 12]]) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 1. Numpy","slug":"【STUDY-Python】/Python-1-Numpy","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-1-Numpy/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"https://hyemin-kim.github.io/tags/Numpy/"}]},{"title":"Python >> Numpy - (3) 수열. 정렬","slug":"S-Python-Numpy3","date":"2020-05-19T17:10:54.000Z","updated":"2020-11-06T05:18:44.857Z","comments":true,"path":"2020/05/20/S-Python-Numpy3/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/20/S-Python-Numpy3/","excerpt":"arange. range. 정렬(sort &amp; argsort)","text":"arange. range. 정렬(sort &amp; argsort) 목록 1. arange란? 1-1. 순서대로 리스트에 값을 생성하려면? 1-2. arange를 사용해서 쉽게 생성하기 1-3. keyword인자를 사용해보기 1-4. 홀수의 값만 생성 2. range (Numpy와는 상관없는 Python문법) 3. 정렬 3-1. 1차원 정렬 3-2. N차원 정렬 3-3. index를 반환하는 argsort 1import numpy as np 1. arange란? arange와 range를 같이 보고 이해하면 됨 [실제 상황 예시] 우리는 순차적인 값을 생성할 때가 많다. 예를 들면: 회원에 대한 가입번호 부여 100개 한정 판매 상품에 대한 고유 번호 부여 이 밖에도 데이터 관리를 위한 인덱스를 차례대로 부여하는 것은 매우 흔한 일이다. 1-1. 순서대로 리스트에 값을 생성하려면? 1~10까지 값을 생성하려면? 1arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 1arr [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 1-2. arange를 사용해서 쉽게 생성하기 np.arange(a, b): a 부터 b-1 까지 생성한다 (a포함, b미포함) 1arr = np.arange(1, 11) 1arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 1-3. keyword인자를 사용해보기 np.arange(start = a, stop = b) 1arr = np.arange(start=1, stop=11) 1arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 1arr = np.arange(stop=11, start=1) # start &amp; stop 지정했기 때문에 순서 바꿔도 됨 1arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 1arr = np.arange(11,1) # start &amp; stop 지정 안하면 순서 바꿨을 때 오류 남 1arr array([], dtype=int32) 1-4. 홀수의 값만 생성 1~10 사이의 값중 홀수만 생성 step 키워드 활용 np.arange(start, stop, step) 1arr = np.arange(1, 11, 2) 1arr array([1, 3, 5, 7, 9]) 1arr = np.arange(start=1, stop=11, step=2) 1arr array([1, 3, 5, 7, 9]) 2. range (Numpy와는 상관없는 Python문법) range는 말 그대로 범위를 지정해 주는 것이다 보통 for-in 의 반복문에서 많이 사용된다 arange와는 다르게 array형태로 저장되어있지 않고 그냥 가볍게 바로바로 쓴다 arange 구문 활용시 1arr = np.arange(1, 11) 1arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 12for i in arr: print(i) 1 2 3 4 5 6 7 8 9 10 range 구문 활용시 12for i in range(1, 11): print(i) 1 2 3 4 5 6 7 8 9 10 12for i in range(1, 11, 2): print(i) 1 3 5 7 9 3. 정렬 3-1. 1차원 정렬 1차원 정렬은 매우 간단함 오름차순으로 정렬: np.sort(arr) 내림차순으로 정렬: np.sort(arr)[::-1] 1arr = np.array([1, 10, 5, 8, 2, 4, 3, 6, 8, 7, 9]) 1arr array([ 1, 10, 5, 8, 2, 4, 3, 6, 8, 7, 9]) 1np.sort(arr) array([ 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10]) 1np.sort(arr)[::-1] array([10, 9, 8, 8, 7, 6, 5, 4, 3, 2, 1]) 하지만, 그냥 이상태에서는 정렬된 이 값들이 유지가 안됨 값을 sort 된 상태로 유지시키려면: 변수로 다시 지정해주기 np.sort(arr) 대신 arr.sort() 쓴다 [arr자체에 sort명령을 씌워줌] 1arr array([ 1, 10, 5, 8, 2, 4, 3, 6, 8, 7, 9]) 1np.sort(arr) array([ 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10]) 1arr # np.sort 만 실행했을 때 유지가 안됨 array([ 1, 10, 5, 8, 2, 4, 3, 6, 8, 7, 9]) 1arr2 = np.sort(arr) # 방법1: arr2로 지정하기 1arr2 array([ 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10]) 1arr.sort() # 방법2: arr.sort 사용하기 1arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10]) 3-2. N차원 정렬 N차원 정렬에서는 axis 중요함. (즉, 정렬 기준이 되는 축) 123arr2d = np.array([[5, 6, 7, 8], [4, 3, 2, 1], [10, 9, 12, 11]]) 1arr2d.shape (3, 4) 열 정렬 (왼쪽에서 오른쪽으로 정렬) – axis 1을 기준으로 삼 1arr2d # 정렬 전 array([[ 5, 6, 7, 8], [ 4, 3, 2, 1], [10, 9, 12, 11]]) 1np.sort(arr2d, axis = 1) # 정렬 후 array([[ 5, 6, 7, 8], [ 1, 2, 3, 4], [ 9, 10, 11, 12]]) 행 정렬 (위에서 아래로 정렬) – axis 0을 기준으로 삼 1arr2d # 정렬 전 array([[ 5, 6, 7, 8], [ 4, 3, 2, 1], [10, 9, 12, 11]]) 1np.sort(arr2d, axis = 0) # 정렬 후 array([[ 4, 3, 2, 1], [ 5, 6, 7, 8], [10, 9, 12, 11]]) 3-3. index를 반환하는 argsort 정렬한 결과에는 값을 반환하는 것이 아닌 index를 반환한다 열 정렬 (왼쪽에서 오른쪽으로 정렬) 1arr2d # 정렬 전 array([[ 5, 6, 7, 8], [ 4, 3, 2, 1], [10, 9, 12, 11]]) 1np.sort(arr2d, axis = 1) # sort 정렬 후 array([[ 5, 6, 7, 8], [ 1, 2, 3, 4], [ 9, 10, 11, 12]]) 1np.argsort(arr2d, axis = 1) # argsort 정렬 후 array([[0, 1, 2, 3], [3, 2, 1, 0], [1, 0, 3, 2]], dtype=int64) 행 정렬 (위에서 아래로 정렬) 1arr2d # 정렬 전 array([[ 5, 6, 7, 8], [ 4, 3, 2, 1], [10, 9, 12, 11]]) 1np.sort(arr2d, axis = 0) # sort 정렬 후 array([[ 4, 3, 2, 1], [ 5, 6, 7, 8], [10, 9, 12, 11]]) 1np.argsort(arr2d, axis = 0) # argsort 정렬 후 array([[1, 1, 1, 1], [0, 0, 0, 0], [2, 2, 2, 2]], dtype=int64) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 1. Numpy","slug":"【STUDY-Python】/Python-1-Numpy","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-1-Numpy/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"https://hyemin-kim.github.io/tags/Numpy/"}]},{"title":"Python >> Numpy - (2) Slicing. 인덱싱","slug":"S-Python-Numpy2","date":"2020-05-19T12:55:06.000Z","updated":"2020-11-06T05:18:39.967Z","comments":true,"path":"2020/05/19/S-Python-Numpy2/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/19/S-Python-Numpy2/","excerpt":"슬라이싱 (Slicing). Fancy 인덱싱. Boolean 인덱싱.","text":"슬라이싱 (Slicing). Fancy 인덱싱. Boolean 인덱싱. 목록 1. 슬라이싱 (Slicing) 1-1. index 지정하여 색인 1차원 array 2차원 array 1-2. 범위 색인 1차원 array 2차원 array 2. Fancy 인덱싱 2-1. 1차원 array 2-2. 2차원 array 3. Boolean 인덱싱 3-1. True와 False값으로 색인하기 3-2. 조건필터 1. 슬라이싱 (Slicing) 1import numpy as np 베열의 부분 선택 (과일을 슬라이스해서 부분만 먹듯…) 1arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 1arr.shape (10,) 1-1. index 지정하여 색인 1차원 array 1arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 1arr[0] # index: 앞에서 부터 0, 1, 2, ... 0 1arr[5] 5 1arr[10] # index가 넘으면 error남 --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-7-ff656e92d79c&gt; in &lt;module&gt; ----&gt; 1 arr[10] IndexError: index 10 is out of bounds for axis 0 with size 10 1arr[-1] # 뒤에서 부터 1번째. index: 뒤에서 부터 -1, -2, -3,... 9 1arr[-10] 0 1arr[-11] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-10-91f133f07612&gt; in &lt;module&gt; ----&gt; 1 arr[-11] IndexError: index -11 is out of bounds for axis 0 with size 10 2차원 array 123arr2d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]) 1arr2d.shape (3, 4) arr2d[행, 열] 1arr2d[0, 2] 3 1arr2d[2, 1] 10 1-2. 범위 색인 1차원 array arr[a, b] – arr의 “index a” 부터 \"index b-1\"까지 (a 포함, b 미포함) index: 1 이상 1arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 1arr[1:] # index 1 포함 array([1, 2, 3, 4, 5, 6, 7, 8, 9]) index: 5 미만 1arr[:5] # index 5 미포함 array([0, 1, 2, 3, 4]) index: 1이상 5미만 1arr[1:5] # index 1 포함 &amp; index 5 미포함 array([1, 2, 3, 4]) index: -1까지 1arr[:-1] # index -1 (index 9) 미포함 array([0, 1, 2, 3, 4, 5, 6, 7, 8]) 2차원 array 123arr2d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]) row(행)을 모두 가져오려는 경우 1arr2d[0,:] # 0번 행의 모든 열 가져오기 array([1, 2, 3, 4]) colomn(열)을 모두 가져오려는 경우 1arr2d[:,2] array([ 3, 7, 11]) 부분적으로 가져오려는 경우 1arr2d[:2, :] # 0,1번 행의 모든 열 가져오기 array([[1, 2, 3, 4], [5, 6, 7, 8]]) 1arr2d[:2, 2:] # 0,1번 행의 2,3번 열 가져오기 array([[3, 4], [7, 8]]) 2. Fancy 인덱싱 fancy인덱싱은 범위가 아닌 특정 index의 집합의 값을 선택하여 추출하고 싶을 때 활용한다 1arr = np.array([10, 23, 2, 7, 90, 65, 32, 66, 70]) 2-1. 1차원 array 방법 1: 추출하고 싶은 index의 집합을 **[꺾쇠 괄호로]**묶어서 추출 1arr[[1, 3, 5]] array([23, 7, 65]) 방법 2: 추출하고 싶은 index의 집합을 변수에 지정한 후 추출 1idx = [1, 3, 5] 1arr[index] array([23, 7, 65]) 2-2. 2차원 array 123arr2d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]) 1arr2d[[0,1], :] array([[1, 2, 3, 4], [5, 6, 7, 8]]) 1arr2d[:, [1,3]] array([[ 2, 4], [ 6, 8], [10, 12]]) 3. Boolean 인덱싱 조건 필터링을 통하여 Boolean값을 이용한 색인 1arr = np.array([1, 2, 3, 4, 5, 6, 7]) 123arr2d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]) 3-1. True와 False값으로 색인하기 boolean index의 수가 꼭 array의 index와 같아야 됨! 1myTrueFalse = [True, False, True] 1arr[myTrueFalse] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-43-9c52b39d81ae&gt; in &lt;module&gt; ----&gt; 1 arr[myTrueFalse] IndexError: boolean index did not match indexed array along dimension 0; dimension is 7 but corresponding boolean dimension is 3 1myTrueFalse = [True, False, True, False, True, False, True] 1arr[myTrueFalse] array([1, 3, 5, 7]) 3-2. 조건필터 조건 연산자를 활용하여 필터를 생성할 수 있다 1arr2d array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) 1arr2d &gt; 2 # \"2보다 크다\"라는 조건의 만족여부에 따라 Boolean index 생성 array([[False, False, True, True], [ True, True, True, True], [ True, True, True, True]]) 위 Boolean index를 다시 array에 적용하여 해당 부분을 추출: arr2d[조건필터] 1arr2d[arr2d &gt; 2] # 1차원 array로 반환 array([ 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]) 1arr2d[arr2d &lt; 5] array([1, 2, 3, 4]) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 1. Numpy","slug":"【STUDY-Python】/Python-1-Numpy","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-1-Numpy/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"https://hyemin-kim.github.io/tags/Numpy/"}]},{"title":"Python >> Numpy - (1) Numpy. array","slug":"S-Python-Numpy1","date":"2020-05-18T15:07:32.000Z","updated":"2020-11-06T05:18:34.788Z","comments":true,"path":"2020/05/19/S-Python-Numpy1/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/19/S-Python-Numpy1/","excerpt":"Numpy개요. Numpy import하기. nd array 생성. array에서의 데이터 타입","text":"Numpy개요. Numpy import하기. nd array 생성. array에서의 데이터 타입 목록 1. Numpy 개요 1-1. Numpy이란? 1-2. 별칭 - np 1-3. array (배열) 1-4. shape(차원) &amp; axis(축) 2. Numpy import하기 2-1. 별칭 (alias) 지정하기 (항상 해주세요!) 3. ndarray 생성하기 – \"np.array([…])\" 3-1. list로 부터 생성하기 – “np.array(list_name)” 3-2. shape확인하기 – “array_name .shape” 4. array에서의 data type 4-1. list에서의 data type 4-2. array에서의 data type case 1. int와 float타입이 혼재된 경우 case 2. int와 float 타입이 혼재되었으나, dtype을 지정한 경우 case 3. int / float 와 str 타입이 혼재된 경우 case 4. int와 str 타입이 혼재되어 있고 dtype이 int로 지정한 경우 1. Numpy 개요 1-1. Numpy이란? Numpy: 수학, 과학 계산용 패키지 ​ 1-2. 별칭 - np 1import numpy as np 1-3. array (배열) 배열: 여러 값들의 그룹 &lt; 1차원 배열 &gt; numpy.array([1, 2, 3, 4]) &lt; 2차원 배열 &gt; numpy.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) &lt; n차원 배열 &gt; (nd array: n dimention array) 1-4. shape(차원) &amp; axis(축) shape은 차원의 수 를 확인 (3, ) =&gt; 3 X 1의 배열 (4,3) =&gt; 4 X 3의 배열 (2,5,3) =&gt; 2 X 5 X 3의 배열 axis는 기준이 되는 축 axis는 앞에서 부터 0, 1, 2… nd array의 축: axis 0, axis 1, axis 2, … axis n 2. Numpy import하기 1import numpy 1numpy &lt;module 'numpy' from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py'&gt; 2-1. 별칭 (alias) 지정하기 (항상 해주세요!) 1import numpy as np 1np &lt;module 'numpy' from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py'&gt; 3. ndarray 생성하기 – \"np.array([…])\" 1arr = np.array([1,2,3,4], dtype=int) 1arr # 주의: list와 다름 array([1, 2, 3, 4]) 1[1, 2, 3, 4] # list [1, 2, 3, 4] 1type(arr) numpy.ndarray 3-1. list로 부터 생성하기 – “np.array(list_name)” 1mylist1 = [1, 2, 3, 4] 12mylist2 = [[1, 2, 3, 4], [5, 6, 7, 8]] 1arr1 = np.array(mylist1) 1arr1 array([1, 2, 3, 4]) 1arr2 = np.array(mylist2) 1arr2 array([[1, 2, 3, 4], [5, 6, 7, 8]]) 3-2. shape확인하기 – “array_name .shape” 1arr1.shape (4,) 1arr2.shape (2, 4) 4. array에서의 data type array에서는 list와 다르게 1개의 단일 데이터 타입 만 허용 된다 4-1. list에서의 data type 1mylist = [1, 3.14, '사과', '1234'] 1mylist [1, 3.14, '사과', '1234'] 1mylist[0] 1 1mylist[2] '사과' 4-2. array에서의 data type case 1. int와 float타입이 혼재된 경우 int와 float타입이 혼재된 경우 int(정수)가 float(실수)로 바꿔진다 1arr = np.array([1, 2, 3, 3.14]) 1arr # 정수가 실수로 바꿔진다 array([1. , 2. , 3. , 3.14]) ​ case 2. int와 float 타입이 혼재되었으나, dtype을 지정한 경우 int와 float 타입이 혼재되었으나, dtype가 int로 지정된 경우, float의 앞에 정수 부분만 보류된다 1arr = np.array([1, 2, 3, 3.14], dtype = int) 1arr array([1, 2, 3, 3]) case 3. int / float 와 str 타입이 혼재된 경우 int / float 와 float타입이 혼재된 경우 int(정수)가 str(문자열)로 바꿔진다 1arr = np.array([1, 3.14, '사과', '1234']) 1arr array(['1', '3.14', '사과', '1234'], dtype='&lt;U32') 1arr[0] + arr[1] #str로 되어버려서 숫자의 사칙 연산이 안됨 '13.14' case 4. int와 str 타입이 혼재되어 있고 dtype이 int로 지정한 경우 (1) 문자내용인 str이 존재한 경우 error 발생 1arr = np.array([1, 3.14, '사과', '1234', '5.8'], dtype = int) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-50-88e75a912236&gt; in &lt;module&gt; ----&gt; 1 arr = np.array([1, 3.14, '사과', '1234', '5.8'], dtype = int) ValueError: invalid literal for int() with base 10: '사과' (2) 실수(float)내용인 str이 존재한 경우도 error발생 1arr = np.array([1, 3.14, '1234', '5.8'], dtype = int) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-52-98017763e514&gt; in &lt;module&gt; ----&gt; 1 arr = np.array([1, 3.14, '1234', '5.8'], dtype = int) ValueError: invalid literal for int() with base 10: '5.8' (3) 정수(int)내용인 str만 존재한 경우 해당 str이 자동으로 int로 바꿔짐 1arr = np.array([1, 3.14, '1234'], dtype = int) 1arr array([ 1, 3, 1234]) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 1. Numpy","slug":"【STUDY-Python】/Python-1-Numpy","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-1-Numpy/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"https://hyemin-kim.github.io/tags/Numpy/"}]},{"title":"Python 기초문법 - (6) Package","slug":"S-Python-base6","date":"2020-05-16T04:52:05.000Z","updated":"2020-11-06T05:18:08.300Z","comments":true,"path":"2020/05/16/S-Python-base6/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/16/S-Python-base6/","excerpt":"패키지(Package) 와 import","text":"패키지(Package) 와 import 목록 1. 패키지와 모듈 그리고 함수의 관계도 2. 모듈 import 하기 3. 패키지 에서 import하기 4. 별칭 (alias) 지어주기 5. 앞으로 자주 사용할 패키지, 모듈 미리보기 패키지(Package) 와 import 1. 패키지와 모듈 그리고 함수의 관계도 함수들이 뭉쳐진 하나의 .py파일 안에 이루어진 것을 모듈이라고 한다 여러 개의 모듈을 그룹화 하면 패키지가 된다 패키지는 종종 라이브러비라고도 불린다 123from IPython.display import Image# 출척: pythonstudy.xyzImage('http://pythonstudy.xyz/images/basics/python-package.png') 2. 모듈 import 하기 import 하는 방법 .py (파이썬 파일 확장자)로 된 파일을 우리는 모듈 이라고 한다, import 구문을 통해 해당 파일을 불러올 수 있다 1import pandas 위의 코드는 pandas라는 모듈을 우리가 불러오겠다라는 의미이다 3. 패키지 에서 import하기 패키지 안에서 하나의 모듈을 불러온다 1from pandas import DataFrame # pandas라는 패키지 안에서 DataFrame이라는 모듈을 불러온다 1DataFrame() # 모듈 DataFrame사용 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 통째로 패키지나 모듈을 불러온다 1import pandas 1pandas.DataFrame() # DataFrame이라는 모듈을 사용하기 위해서는 .을 찍고 이어서 쓰면 됨 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 4. 별칭 (alias) 지어주기 pandas라는 패키지 이름이 너무 길기 때문에 우리는 약어로 줄여쓸 수 있다. 보통 pd를 보편적으로 많이 사용한다. 줄여서 별명을 지어줄 때는 as를 붙혀준다 1import pandas as pd 1pd.DataFrame() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 5. 앞으로 자주 사용할 패키지, 모듈 미리보기 1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns numpy: 과학계산을 위한 패키지 pandas: 데이터 분석을 할 때 가장 많이 쓰이는 패키지 matplotlib: 시각확를 위한 패키지 seaborn: 시각화를 위한 패키지 (matplotlib을 더 쉽게 사용할 수 있도록 도와주는 패키지) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 0. Base","slug":"【STUDY-Python】/Python-0-Base","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-0-Base/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Python_Base","slug":"Python-Base","permalink":"https://hyemin-kim.github.io/tags/Python-Base/"}]},{"title":"Python 기초문법 - (5) List Comprehension. 문자열","slug":"S-Python-base5","date":"2020-05-13T16:37:58.000Z","updated":"2020-11-06T05:18:03.972Z","comments":true,"path":"2020/05/14/S-Python-base5/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/14/S-Python-base5/","excerpt":"List Comprehension (List에 조건필터를 적용). 문자열을 가지고 노는 방법.","text":"List Comprehension (List에 조건필터를 적용). 문자열을 가지고 노는 방법. 목록 1. List Comprehension (파이썬 고유의 아름다운 문법) 1-1. list comprehension 조건필터 1-2. [STEP 1] list를 만들어야 하니 일단 꺾쇠[ ]를 씌운다 1-3. [STEP 2] 조건 필터를 걸어 준다 1-4. [응용 STEP] 변수 값을 가공할 수도 있다 2. 문자열(string)을 가지고 놀기 2-1. 문자의 길이 2-2. 문장 쪼개기 – “.split” 2-3. 대문자 / 소문자로 만들기 – “.upper” / “.lower” 2.4. ~로 시작하는, ~로 끝나는 – “.startswith” , “.endswith” 2-5. 바꾸기 – “.replace(‘바꿀 대상, 바꿔야할 값’)” 2-6. 불필요한 공백 제거하기 – “.strip” 1. List Comprehension (파이썬 고유의 아름다운 문법) for ~ in 구조를 기본적으로 가지고 있다 List Comprehension 이니까 당연히 List를 사용한다 [실제 사례 연구] mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 이라는 list를 만들어 주고 우리는 이 중 짝수만 출력하고 싶으면 아래와 같이 쓸 수 있다: 1mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 123for i in mylist: if i % 2 == 0: print(i) 2 4 6 8 10 그럼 mylist에서 짝수만 뽑아서 list로 만들어 주고 싶다면: 12345678mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]even = []for i in mylist: if i % 2 == 0: even.append(i)print(even) [2, 4, 6, 8, 10] 이렇게 for in 문으로 해줄 수 있다. 하지만, 우리는 list comprehension을 통해 더욱 쉽게 해결 할 수 있다!! 1-1. list comprehension 조건필터 1mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 아래 문법이 바로 list comprehension 이다. 한 줄로 해결해 버리는 것이 매력임! 1even = [i for i in mylist if i % 2 == 0] 1even [2, 4, 6, 8, 10] 1-2. [STEP 1] list를 만들어야 하니 일단 꺾쇠[ ]를 씌운다 꺾쇠 안에 반복문이 들어간다 반복문을 돌면서 return 된 i값을 list에 넣는 원리이기 때문에 for구분 앞에 i를 써준다 1even = [i for i in mylist] 1even [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 1-3. [STEP 2] 조건 필터를 걸어 준다 [i for i in mylist (이곳에 조건문)] 1[i for i in mylist if i % 2 == 0] [2, 4, 6, 8, 10] 이것을 변수에 다시 할당해주면 끝! 1even = [i for i in mylist if i % 2 == 0] 1even [2, 4, 6, 8, 10] 1-4. [응용 STEP] 변수 값을 가공할 수도 있다 예를 들어: mylist의 모든 값에 +2를 하고 다시 even이라는 list에 저장하고 싶다면 1mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 1even = [i+2 for i in mylist] 1even [3, 4, 5, 6, 7, 8, 9, 10, 11, 12] 2. 문자열(string)을 가지고 놀기 2-1. 문자의 길이 1a = 'banana' 1len(a) 6 1a = 'banana pen' 1len(a) # 공백도 count된다 10 1b = '한글' 1len(b) 2 1b = '한글 바나나' 1len(b) 6 2-2. 문장 쪼개기 – “.split” split은 문장을 특정 규칙에 의해 쪼개 주는 기능을 한다 명령어: 변수명.split(‘쪼개는 기준’) 쪼개는 기준이 설정되어 있지 않으면 그냥 '빈칸’으로 인식된다 1a = 'This is a pen' 1a.split(' ') ['This', 'is', 'a', 'pen'] 1a.split() ['This', 'is', 'a', 'pen'] 1b = 'This-is-a-pen' 1b.split('-') ['This', 'is', 'a', 'pen'] return된 값을 list형식으로 저장한다 1aa = a.split(' ') 1aa ['This', 'is', 'a', 'pen'] 1aa[0] 'This' 1aa[2] 'a' 1aa[0] + aa[2] 'Thisa' 1c = '한글은 어떻게 될까요?' 1c.split() ['한글은', '어떻게', '될까요?'] 2-3. 대문자 / 소문자로 만들기 – “.upper” / “.lower” 1a = 'My name is hyemin' 1a.upper() 'MY NAME IS HYEMIN' 1a.lower() 'my name is hyemin' 1b = '한글엔 대소문자가 없어요ㅠ' 1b.upper() '한글엔 대소문자가 없어요ㅠ' 1b.lower() '한글엔 대소문자가 없어요ㅠ' 2.4. ~로 시작하는, ~로 끝나는 – “.startswith” , “.endswith” 123a = '01-sample.png'b = '02-sample.jpg'c = '03-sample.pdf' 1a.startswith('01') True 1a.endswith('.jpg') False 1b.endswith('.jpg') True 조건(혹은 형식)에 맞는 파일을 추출하고 싶을 때: 1mylist = [a, b] 123for file in mylist: if file.endswith('jpg'): print(file) 02-sample.jpg 2-5. 바꾸기 – “.replace(‘바꿀 대상, 바꿔야할 값’)” [예] file형식을 바꾸고 싶다면: 1a = '01-sample.png' 1a.replace('.png', '.jpg') '01-sample.jpg' 이 때 a의 값이 변하지 않아. 다시 할당 해야 함 1a '01-sample.png' 1a_new = a.replace('.png', '.jpg') # 새로 지정 1a_new '01-sample.jpg' 1a = a.replace('.png', '.jpg') # 덮어쒸우기 1a '01-sample.jpg' 2-6. 불필요한 공백 제거하기 – “.strip” [예] 12a = ' 01-sample.png'b = '01-sample.png' 1a == b False strip은 양 끝 불필요한 공백을 제거해 줌. 1a.strip() '01-sample.png' 1a.strip() == b True document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 0. Base","slug":"【STUDY-Python】/Python-0-Base","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-0-Base/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Python_Base","slug":"Python-Base","permalink":"https://hyemin-kim.github.io/tags/Python-Base/"}]},{"title":"Python 기초문법 - (4) 비교/논리 연산자. 조건문. 반복문","slug":"S-Python-base4","date":"2020-05-13T08:25:46.000Z","updated":"2020-11-06T05:17:59.565Z","comments":true,"path":"2020/05/13/S-Python-base4/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/13/S-Python-base4/","excerpt":"비교연산자. 조건문. 논리연산자. 반복문","text":"비교연산자. 조건문. 논리연산자. 반복문 목록 1. 비교연산자 1-1. 대소비교 &gt;, &gt;=, &lt;, &lt;= 1-2. 같다 == 1-3. 같지 않다 != 2. 조건문 2-1. 개념 2-2. if 2-3. else 2-4. elif 2-5. 1이나 0은 참이나 거짓을 표현하기도 한다 3. 논리 연산자 (and, or) 3-1. and 3-2. or 4. 반복문 4-1. 반복문이란? 4-2. for 와 in을 활용하자! 4-3. 반복문에서 짝수만 출력하려면? (continue구문) 4-4. 조건을 충족시 순환에서 빠져나와보자! (break구문) 1. 비교연산자 비교 연산자는 주로 대소비교를 할 때 사용한다. 1-1. 대소비교 &gt;, &gt;=, &lt;, &lt;= 11 &gt; 2 False 110 &gt;= 10 True 19 &lt; 10 True 18 &lt;= 7 False 1-2. 같다 == 주의: = 는 대입연산자. == 는 비교연산자 중의 “같다” 숫자형 &amp; 문자형 모두 비교 가능 12 = 2 File \"&lt;ipython-input-6-a8e553549e25&gt;\", line 1 2 = 2 ^ SyntaxError: can't assign to literal 12 == 2 True 12 == 3 False 1\"나\" == \"나\" True 1-3. 같지 않다 != 숫자형 &amp; 문자형 모두 비교 가능 12 != 2 False 12 != 3 True 1\"나\" != \"너\" True 2. 조건문 2-1. 개념 주어진 조건이 참인 경우 그 다음 내가 규칙(로직)을 실행하는 개념이다 2-2. if if는 어떤 조건이 성립한다면 ~이라는 의미 if구문 끝에는 반드시 콜론( : )이 있어야 함 12if 5 &gt; 3: print('참') 참 if구문 뒤에 indent가 있는 명령어는 if조건이 성립하면 실행 indent가 없으면 if의 성립여부와 무관하여 무조건 실행 12345if 5 &gt; 3: print('참') print('참') print('끝') 참 참 끝 12345if 5 &lt; 3: print('참') print('참') print('끝') # 앞에 indent가 없으면 if의 성립여부와 무관하여 실행 끝 2-3. else else는 if 조견 후에 따라오면, if가 아닌 경우에 실행 됨 1234if 5 &lt; 3: print(\"성립한다\")else: print(\"성립하지 않은다\") 성립하지 않은다 else는 꼭 if랑 같이 써야함. 단독으로 실행할 수 없음 12else: print(\"성립하지 않은다\") File \"&lt;ipython-input-22-6c0f4debaa4b&gt;\", line 1 else: ^ SyntaxError: invalid syntax 2-4. elif elif구문은 3가지 이상 문기(조건)의 동작을 수행할 때 사용 123456if 3 &gt; 5: print('if 구문')elif 3 &lt; 4: print('elif 구문')else: print('이것도 저것도 아니다') elif 구문 그럼, elif구문이 참인 여러 구문을 나열 했을 때는 어떻게 될까? 12345678910if 3 &gt; 5: print('if 구문')elif 3 &lt; 4: print('elif 1 구문')elif 3 &lt; 5: print('elif 2 구문')elif 3 &lt; 6: print('elif 3 구문')else: print('이것도 저것도 아니다') elif 1 구문 elif구문이 참인 여러 구문을 나열 했을 때는 첫번째 참인 elif구문만 실행됨 2-5. 1이나 0은 참이나 거짓을 표현하기도 한다 1234if 1: print('참')else: print('거짓') 참 1234if 0: print('참')else: print('거짓') 거짓 3. 논리 연산자 (and, or) and나 or조건은 두 가지 이상 조건을 다룰 때 활용한다 3-1. and and 조건은 모두 만족할 때 참으로 인식한다 1True and True and True True 1True and False and True False 1234if (0 &lt; 1) and (0 &lt; 2): print('모두 참')else: print('거짓') 모두 참 1234if (0 &lt; 1) and (0 &gt; 2): print('모두 참')else: print('거짓') 거짓 3-2. or or조건은 조건 중 하나라도 만족할 때 참으로 인식한다 1True or False or False True 1False or False or False False 1234if (0 &lt; 1) or (0 &gt; 2): print('하나라도 참')else: print('모두 거짓') 하나라도 참 1234if (0 &gt; 1) or (0 &gt; 2): print('하나라도 참')else: print('모두 거짓') 모두 거짓 4. 반복문 4-1. 반복문이란? 일을 반복 처리 해준다는 것 대상은 반드시 list, dict, set등 집합이어야 한다 [예] 반복문 쓰지 않을 때: 1mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] mylist에 들어 닜는 모든 값들을 출력하려고 한다면? 123456print(mylist[0])print(mylist[1])print(mylist[2])print('...')print(mylist[8])print(mylist[9]) 1 2 3 ... 9 10 반복문은 노가다를 획기적으로 줄여주는 방법이다! 4-2. for 와 in을 활용하자! [기본 문법] for 지정한 변수명 in [꺼내올 집합]: 명령어 1mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 12for i in mylist: print(i) 1 2 3 4 5 6 7 8 9 10 4-3. 반복문에서 짝수만 출력하려면? (continue구문) 1mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 방법1: 123for i in mylist: if i % 2 == 0: print(i) 2 4 6 8 10 방법2: continue구문을 사용하면 조건이 충족할 때 아래 명령어를 SKIP하고 다시 다음 순환으로 넘어간다 1234for i in mylist: if i % 2 == 1: continue print(i) 2 4 6 8 10 4-4. 조건을 충족시 순환에서 빠져나와보자! (break구문) 1mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] i가 6 이상이면 STOP 1234for i in mylist: if i &gt;= 6: # i &gt; 6 이면 6까지 출력한다 break print(i) 1 2 3 4 5 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 0. Base","slug":"【STUDY-Python】/Python-0-Base","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-0-Base/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Python_Base","slug":"Python-Base","permalink":"https://hyemin-kim.github.io/tags/Python-Base/"}]},{"title":"Python 기초문법 - (3) 함수","slug":"S-Python-base3","date":"2020-05-13T07:16:31.000Z","updated":"2020-11-06T05:17:54.749Z","comments":true,"path":"2020/05/13/S-Python-base3/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/13/S-Python-base3/","excerpt":"함수의 기초","text":"함수의 기초 목록 1. 함수란 무엇일까? 2. 함수 정의: def (define) 3. 함수는 값을 return할 수 있고, 안해도 됨 4. parameter가 여러 개 있으면, 함수에 넘겨 줄 때 순서가 중요 함수 1. 함수란 무엇일까? 반복적으로 사용되는 부문을 묶어서, 재사용 가능하도록 만들어 주는 것 함수에는 **들어가는 놈 (input)**이 있고, **나오는 놈 (output 혹은 return)**이 있다. 전해진 로직(규칙)에 따라, input -&gt; output으로 효율적으로 바꿔주는 역할을 한다 [예시] 함수 없이 계산할 때 123a = 1b = 2c = 3 1(a + b) * c 9 123a = 2b = 2c = 3 1(a + b) * c 12 함수로 변경 후 12def func(a, b, c): return (a + b) * c 1func(1, 2, 3) 9 1func(2, 2, 3) 12 2. 함수 정의: def (define) 사용법: def 함수이름 (parameter1, parameter2, parameter3…): parameter는 함수로 부터 넘겨 받은 변수 또는 값이다 끝에 콜론 ( : ) 빼먹지 않음에 주의 해야함! 12def myfunc(var1): print(var1) # 실행 명령 1myfunc(\"안녕하세요\") 안녕하세요 3. 함수는 값을 return할 수 있고, 안해도 됨 리턴이 없는 경우 12def my_func(a, b): print(a, b) 1my_func(1,10) 1 10 리턴이 있는 경우 123def my_func(a, b): s = a + b return s 1my_func(2, 3) 5 리턴이 있는 경우는 변수에 값을 다시 할당 할 수 있음 1result = my_func(2,3) 1print(result) 5 1print(result + 10) 15 4. parameter가 여러 개 있으면, 함수에 넘겨 줄 때 순서가 중요 12def my_func(a, b, c): return (a + b) * c 123a = 10b = 20c = 3 1(a + b) * c 90 1my_func(a, b, c) 90 1my_func(c, b, a) # (c + b) * a = (3 + 20) * 10 230 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 0. Base","slug":"【STUDY-Python】/Python-0-Base","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-0-Base/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Python_Base","slug":"Python-Base","permalink":"https://hyemin-kim.github.io/tags/Python-Base/"}]},{"title":"Python 기초문법 - (2) 집합 형태의 데이터 타입","slug":"S-Python-base2","date":"2020-05-12T17:26:49.000Z","updated":"2020-11-06T05:17:49.545Z","comments":true,"path":"2020/05/13/S-Python-base2/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/13/S-Python-base2/","excerpt":"","text":"집합 형태의 데이터 타입 1. list (순서가 있는 집합) 1-1. [ ] 형테로 표현 1-2. 값 추가 – “.append( )” 1-3. 값 제거 – “.remove” / “.clear” 1-4. 인덱싱(Indexing) -&gt; 색인 1-5. 인덱스로 접근하여 값 바꾸기 1-6. 길이 파악하기 2. tuple (순서가 있는 집합, 읽기 전용) 2-1. ( ) 형태로 표현 2-2. 읽기 전용이라 “값 추가”, “값 제거”, “값 바꾸기” 모두 안됨 2-3. 길이 파악하기 3. set (순서 X, 중복 X) 3-1. set의 할당: set() 3-2. 값 추가 – \".add \" 3-3. 값 제거 – “.remove” / “.clear” 4. dict (사전형 집합, key와 value 쌍) 4-1. { } 형태로 표헌 4-2. 값 추가 (key와 value 모두 지정) 4-3. 값 바꾸기 4-4. 값 제거 – “.pop” / “.clear” 4-5. 길이 파악하기 짐합 형태의 데이터 타입 list (순서 O, 짐합) tuple (순서 X, 읽기 전용 집합) set (순서 X, 중복 X 집합) dict (key, value로 이루어진 사전형 집합) 1. list (순서가 있는 집합) 1-1. [ ] 형테로 표현 1mylist = [] 1mylist [] 1type(mylist) list 12mylist = [1,2,3,4,5]mylist [1, 2, 3, 4, 5] 12mylist2 = [5,4,3,2,1] # 순서가 있다mylist2 [5, 4, 3, 2, 1] 1-2. 값 추가 – “.append( )” 12mylist = []mylist [] 12mylist.append(1)mylist [1] 123mylist.append(2)mylist.append(3)mylist [1, 2, 3] .append 함수 안에 1 argument만 들어갈 수 있다 12mylist.append(4,5)mylist --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-22-6f00703728b8&gt; in &lt;module&gt; ----&gt; 1 mylist.append(4,5) 2 mylist TypeError: append() takes exactly one argument (2 given) 1-3. 값 제거 – “.remove” / “.clear” 부분 제거 – \".remove\" 1mylist [1, 2, 3] 12mylist.remove(1)mylist [2, 3] 전부 제거 – \".clear\" 1mylist.clear() 1mylist [] 같은 값이 여러 개 포함되어 있을 때의 제거 순서 앞에서 부터 순차적으로 제거 됨 12mylist = [1,2,3,1,2,3]mylist [1, 2, 3, 1, 2, 3] 12mylist.remove(1)mylist [2, 3, 1, 2, 3] 12mylist.remove(1)mylist [2, 3, 2, 3] 1-4. 인덱싱(Indexing) -&gt; 색인 인덱스는 0번 부터 시작한다 1mylist = [1,2,3,4] # 인덱스: 0번, 1번, 2번, 3번 1mylist[0] 1 1mylist[3] 4 1mylist[4] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-34-88b11041aa4f&gt; in &lt;module&gt; ----&gt; 1 mylist[4] IndexError: list index out of range 인덱스가 음수일 경우: 뒤에서 부터 n번째 1mylist[-1] 4 1-5. 인덱스로 접근하여 값 바꾸기 1mylist [1, 2, 3, 4] 1mylist[0] 1 1mylist[0] = 100 1mylist [100, 2, 3, 4] 1-6. 길이 파악하기 1mylist [100, 2, 3, 4] 1len(mylist) # length 4 2. tuple (순서가 있는 집합, 읽기 전용) 2-1. ( ) 형태로 표현 1mytuple = (1,2,3,4,5) 2-2. 읽기 전용이라 “값 추가”, “값 제거”, “값 바꾸기” 모두 안됨 1mytuple.append(1) # 읽기 전용이라 값을 추가할 수 없음 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-45-d0f55ea1e3f6&gt; in &lt;module&gt; ----&gt; 1 mytuple.append(1) # 읽기 전용이라 값을 추가할 수 없음 AttributeError: 'tuple' object has no attribute 'append' 1mytuple.remove(1) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-46-05a40423345b&gt; in &lt;module&gt; ----&gt; 1 mytuple.remove(1) AttributeError: 'tuple' object has no attribute 'remove' 1mytuple[0] = 100 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-48-4e527888818c&gt; in &lt;module&gt; ----&gt; 1 mytuple[0] = 100 TypeError: 'tuple' object does not support item assignment 2-3. 길이 파악하기 1mytuple (1, 2, 3, 4, 5) 1len(mytuple) 5 3. set (순서 X, 중복 X) 3-1. set의 할당: set() 12myset = set()myset set() 1type(myset) set 3-2. 값 추가 – \".add \" 1234myset.add(1)myset.add(2)myset.add(3)myset {1, 2, 3} 1234567myset.add(1) myset.add(2)myset.add(3)myset.add(1) # 중복된 값을 한번만 기록myset.add(2)myset.add(3)myset {1, 2, 3} 12myset.add(4)myset {1, 2, 3, 4} 3-3. 값 제거 – “.remove” / “.clear” 부분 제거 – \".remove\" 1myset {1, 2, 3, 4} 1myset.remove(3) 1myset {1, 2, 4} 전부 제거 – \".clear\" 1mylist.clear() 1mylist [] 4. dict (사전형 집합, key와 value 쌍) 4-1. { } 형태로 표헌 1mydict = dict() 1mydict {} 1type(mydict) dict 4-2. 값 추가 (key와 value 모두 지정) mydict [ \" key \" ] = value key는 문자형 (str) / 숫자형 (int &amp; float) 모두 가능 1mydict[\"apple\"] = 123 1mydict {'apple': 123} 1mydict[\"apple\"] 123 1mydict[0] = 2 1mydict {'apple': 123, 0: 2} 1mydict[0] 2 1mydict[3.14] = 1 1mydict {'apple': 123, 0: 2, 3.14: 1} 1mydict[3.14] 1 4-3. 값 바꾸기 새 값을 해당 key에 할당하기 1mydict[\"apple\"] = \"hello\" 1mydict {'apple': 'hello', 0: 2, 3.14: 1} 4-4. 값 제거 – “.pop” / “.clear” 부분 제거 – \".pop\" 1mydict.pop('apple') 'hello' 1mydict {0: 2, 3.14: 1} 1mydict.pop(0) 2 1mydict {3.14: 1} 전부 제거 – \".clear\" 1mydict.clear() 1mydict {} 4-5. 길이 파악하기 123mydict[\"apple\"] = 123mydict[0] = 2mydict[3.14] = 1 1mydict {'apple': 'hello', 0: 2, 3.14: 1} 1len(mydict) 3 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 0. Base","slug":"【STUDY-Python】/Python-0-Base","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-0-Base/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Python_Base","slug":"Python-Base","permalink":"https://hyemin-kim.github.io/tags/Python-Base/"}]},{"title":"Python 기초문법 - (1) 출력. 데이터 타입. 데이터의 응용","slug":"S-Python-base1","date":"2020-05-11T17:18:11.000Z","updated":"2020-11-06T05:17:43.571Z","comments":true,"path":"2020/05/12/S-Python-base1/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/12/S-Python-base1/","excerpt":"출력. 변수. 데이터 타입. 데이터의 응용. 데이터 타입의 변환.","text":"출력. 변수. 데이터 타입. 데이터의 응용. 데이터 타입의 변환. 목록 1. 출력 (print) print( ) 함수 2. 변수와 대입 2-1. 변수의 이름 【가능한 경우】 【불가한 경우】 2-2. 변수의 대입 2-3. 변수의 출력 3. 데이터 타입 3-1. int(정수) 3-2. float(실수) 3-3. str 혹은 object (문자열) 3-4. bool (참/거짓) 3-5. 아무것도 아닌 None타입도 있다 4. 데이터의 응용 4-1. 사칙 연산자 4-2. 문자열의 연결 5. 데이터 타입 변환 5-1. 문자열로 변환: “str( ) 함수” or “따움표” 5-2. 정수로 변환: \" int( ) 함수\" 5-3. 실수로 변환: “float( ) 함수” 1. 출력 (print) print( ) 함수 숫자를 출력할 때 따움표(’ ’ or \" \") 필요없음 문자를 출력할 때 따움표 필요 ’ ’ 와 \" \" 차이없음 ‘’’ ‘’’ 를 사용하면 출력시 “줄 바꿈” 형식이 보류될 수 있음 1print(1) 1 1print(1+2) 3 1print('안녕하세요') 안녕하세요 1print(\"반갑습니다\") 반갑습니다 1234print('''안녕하세요,반갑습니다.''') 안녕하세요, 반갑습니다. 2. 변수와 대입 2-1. 변수의 이름 【가능한 경우】 case 1. 알파벳 1a = 1 1A = 1 case 2. 알파벳 + 숫자 1a1 = 1 case 3. 알파벳 + 언더바(_) 1a_ = 1 case 4. 언더바(_) + 알파벳 1_a = 1 【불가한 경우】 case 1. 언더바(_)를 제외한 특수문자 1* = 1 File \"&lt;ipython-input-23-6d0163a9fd4c&gt;\", line 1 * = 1 ^ SyntaxError: invalid syntax case 2. 알파벳 + 언더바를 제외한 특수문자 1a$ = 1 File \"&lt;ipython-input-25-2501fc576aab&gt;\", line 1 a$ = 1 ^ SyntaxError: invalid syntax case 3. 변수의 이름 사이의 공백 1a b = 1 File \"&lt;ipython-input-26-2bab97d7970c&gt;\", line 1 a b = 1 ^ SyntaxError: invalid syntax 2-2. 변수의 대입 변수 값을 부여할 때 \"=\"를 사용한다 1a = 1 2-3. 변수의 출력 print() 구문 사이에 값을 직접 입력하면, 바로 값이 출력됨. 1print(123) # 숫자는 \"\" 필요없음 123 1print(\"text\") # 문자는 \"\" 필요함 text print()구분 사이에 변수 이름을 입력하면, 변수의 값이 출력됨. 12a = 123print(a) 123 12b = \"text\"print(b) text 3. 데이터 타입 데이터 type: 1. int(정수) 2. float(실수) 3. str(문자열) 4. bool(참/거짓) 3-1. int(정수) 1a = 1 1type(a) int 1print(a) 1 코딩에서 1은 참으로 취급, 0은 거짓으로 취급 다음 코딩으로 진단해보자: 1234if 1: print('1은 참으로 취급')else: print('1은 거짓부렁이') 1은 참으로 취급 1234if 0: print('0은 참으로 취급')else: print('0은 거짓부렁이') 0은 거짓부렁이 1234if 123: print('123은 참으로 취급')else: print('123은 거짓부렁이') 123은 참으로 취급 [0 이외의 정수 다 참으로 취급] 3-2. float(실수) 1a = 3.14 1type(a) float 1print(a) 3.14 3-3. str 혹은 object (문자열) 문자열은 반드시 ’ ’ 혹은 \" \" 로 묶어야 함 1word = '안녕하세요' 1type(word) str 1print(word) 안녕하세요 1word = \"안녕하세요\" 1type(word) str 1print(word) 안녕하세요 ’\" \"’ 를 사용하면 출력시 “줄 바꿈” 형식이 보류될 수 있음 1234print('''안녕하세요,반갑습니다.''') 안녕하세요, 반갑습니다. 3-4. bool (참/거짓) 참: True 거짓: False 1a = True 1a True 1type(a) bool 1b = False 1b False 1type(b) bool 11 == True True 10 == False True 1123 == True False 1 이외의 정수는 조건절에서 참으로 인식되지만, bool과 비교할 때 참이 아니다 3-5. 아무것도 아닌 None타입도 있다 Null값을 넣는다고도 한다. Null: Nullify (무효화하다) – 사전상 의미 Python에서는 None 입니다 1a = None 1print(a) None 1type(a) NoneType 조건문에 None이라면? 1234if None: print(\"None은 참으로 취급\")else: print(\"None은 거짓부렁이\") None은 거짓부렁이 4. 데이터의 응용 4-1. 사칙 연산자 연산자 의미 예 + 더하기 2 + 1 -&gt; 3 - 빼기 1 - 2 -&gt; -1 * 곱하기 1 * 2 -&gt; 2 / 나누기 1 / 2 -&gt; 0.5 // 몫 5 // 2 -&gt; 2 % 나머지 5 % 2 -&gt; 1 ** 멱 2**3 -&gt; 8 4-2. 문자열의 연결 여러 개 문자열을 \"+\"을 통해 연결할 수 있다 12345subject = \"나는 \"object = \"치킨을 \"verb = \"좋아한다\"print(subject + object + verb) 나는 치킨을 좋아한다 하지만 문자열(str)과 숫자(int &amp; float)는 직접 연결할 수 없다 1234567a = \"내가 \"b = \"친구랑 \"c = 12d = \"시에 \"e = \"보기로 했다\"print(a + b + c + d + e) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-82-34cd0f9ce519&gt; in &lt;module&gt; 5 e = \"보기로 했다\" 6 ----&gt; 7 print(a + b + c + d + e) TypeError: can only concatenate str (not \"int\") to str 이 때는 데이터 타입을 변환할 필요가 있다 5. 데이터 타입 변환 5-1. 문자열로 변환: “str( ) 함수” or “따움표” 1type(6) int 1type(str(6)) str 1type('6') str 1type(3.14) float 1type(str(3.14)) str 1type(\"3.14\") str 12345a = \"내가 \"b = \"친구랑 \"c = 12d = \"시에 \"e = \"보기로 했다\" 1print(a + b + str(c) + d + e) 내가 친구랑 12시에 보기로 했다 1print(a + b + '12' + d + e)a 내가 친구랑 12시에 보기로 했다 5-2. 정수로 변환: \" int( ) 함수\" \"str\" --&gt; “int”: str( ) 안 내용이 정수일 때만 가능 1type(int(\"2\")) int 12number1 = \"2\"number2 = \"3\" 1print(int(number1) + int(number2)) 5 1print(int(\"2.6\")) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-103-f4645c45f771&gt; in &lt;module&gt; ----&gt; 1 print(int(\"2.6\")) ValueError: invalid literal for int() with base 10: '2.6' \"float\" --&gt; “int”: 소수점 버림 1type(int(3.6)) int 1print(int(3.6)) 3 5-3. 실수로 변환: “float( ) 함수” \"str\" --&gt; “float”: str( ) 안 내용이 정수일 때만 가능 1type(float(\"3.14\")) float 1print(float(\"3.14\")) 3.14 \"int\" --&gt; “float”: 소수점 하나 추가 1type(float(178)) float 1print(float(178)) 178.0 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - 0. Base","slug":"【STUDY-Python】/Python-0-Base","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-0-Base/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Python - Base","slug":"Python-Base","permalink":"https://hyemin-kim.github.io/tags/Python-Base/"}]},{"title":"利用Git Pages+Hexo搭建博客过程中的参考资料","slug":"Reference","date":"2020-05-07T17:16:53.000Z","updated":"2020-12-21T10:03:37.146Z","comments":true,"path":"2020/05/08/Reference/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/08/Reference/","excerpt":"","text":"博客搭建 bilibili — 超简单易懂的Git入门教程 bilibili — github+hexo搭建个人博客 bilibili — github博客搭建（二）：Markdown语法及hexo主题修改 Git Pages + Jekyll/Hexo搭建自己的博客(最全总结你想知道的都在这里了) 网页配置 &amp; 主题配置 Hexo Usage Documents Hexo Themes default — [Demo] tomotoes — [Demo] / [Documents] Butterfly — [Demo] / [Documents] (应用中) Git Pages + Jekyll/Hexo搭建自己的博客(最全总结你想知道的都在这里了) Hexo主题升级方法（实用！） Markdown渲染 bilibili — github博客搭建（二）：Markdown语法及hexo主题修改 [字体修改方法(17:30)] Markdown在Hexo中的使用实例 [分割线，空行插入方法] Markdown渲染插件 hexo-renderer-markdown-it 插件 快速配置 hexo-renderer-markdown-it [Documents] hexo-renderer-markdown-it-plus （应用中） hexo-renderer-markdown 插入本地图片 markdown插入本地图片小技巧 typora + hexo博客中插入图片（应用中） 其他 记录网站访问量: 不蒜子 hexo博客解决不蒜子统计无法显示问题 设置博客评论： Gitalk申请页面 在个人博客里添加评论系统–Gitalk hexo 使用 gitalk 评论组件的几个注意点 多语言版本: Hexo 巧用 abbrlink 插件实现文章多语言版本 (既然没人帮我，那就)自己弄了个 Hexo 多语言 index 生成插件 更改tag大小写后出现404页面 Hexo 部署到 Github Pages 文件夹大小写问题 更改博客 Front Page 的默认配置 hexo博客Front-matter模板配置 博客主题升级 Hexo主题升级方法（实用！） Git &amp; Github bilibili — 【教程】学会Git玩转Github【全】 bilibili — 超简单易懂的Git入门教程 Git与Github的连接与使用 Git和GitHub使用教程 Jupyter Notebook bilibili — python数据分析神器Jupyter notebook快速入门 bilibili —【冷门教学】记笔记神器-jupyter notebook 第二弹 史上最详细、最完全的jupyter notebook使用教程，Python使用者必备！——ipython系列之三 机器学习新手必看：Jupyter Notebook入门指南 Jupyter notebook简介及嵌入Hexo博客中 用 Hexo 搭建个人博客-02：进阶试验（包括添加Jupyter Notebook支持的方法） 如何在你的Jupyter Notebook中使用R语言？ Markdown &amp; Typora bilibili — 二十分钟精通排版神器Markdown Typora官网 [Documents] Typora中下载并安装主题 bilibili — Typora 编辑器 —— 书写即为美学 bilibili — 【软件教程】如何用Typora记笔记？ | 附带Markdown基础教程 Typora设置（中文字体、颜色、行距、内边距等） Markdown中插入本地图片 markdown插入本地图片小技巧 typora + hexo博客中插入图片 HTML 表格样式 好看的table css样式 CSS 列表样式 HTML基础知识 table中 th, td, tr CSS如何设置html table表格边框样式 CSS如何设置表格中的字体大小 CSS padding 属性 [html/css] margin 속성 자세히 알아보기 漂亮的CSS表格样式 在此感谢所有提供了宝贵学习资料的原po主们~ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【USAGE】","slug":"【USAGE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90USAGE%E3%80%91/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://hyemin-kim.github.io/tags/Hexo/"},{"name":"Typora","slug":"Typora","permalink":"https://hyemin-kim.github.io/tags/Typora/"},{"name":"Markdown","slug":"Markdown","permalink":"https://hyemin-kim.github.io/tags/Markdown/"},{"name":"Git","slug":"Git","permalink":"https://hyemin-kim.github.io/tags/Git/"},{"name":"Github","slug":"Github","permalink":"https://hyemin-kim.github.io/tags/Github/"},{"name":"Jupyter notebook","slug":"Jupyter-notebook","permalink":"https://hyemin-kim.github.io/tags/Jupyter-notebook/"}]},{"title":"在Hexo博文中添加本地图片的方法（基于Typora编辑器）","slug":"Hexo-Insert-local-images","date":"2020-05-06T12:20:48.531Z","updated":"2020-10-28T06:40:17.889Z","comments":true,"path":"2020/05/06/Hexo-Insert-local-images/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/06/Hexo-Insert-local-images/","excerpt":"当我们想在markdown文档中添加网络图片时，可以使用命令!['图片名称'](图片网络地址)进行实现，然而这条命令却不适用于添加本地图片。本文将介绍在使用Typora编辑器编辑Hexo博文时，向markdown文档中添加本地图片的方法。快来看看吧","text":"当我们想在markdown文档中添加网络图片时，可以使用命令!['图片名称'](图片网络地址)进行实现，然而这条命令却不适用于添加本地图片。本文将介绍在使用Typora编辑器编辑Hexo博文时，向markdown文档中添加本地图片的方法。快来看看吧 【编写博客前】— 进行配置 【编写博客时】— 图片导入方法 【编写博客后】— 图片存档结果 【编写博客前】— 进行配置 建立 资源文件夹(Asset Floder)，用来保存添加到博文中的本地图片 在本地Hexo根目录下的source文件夹中创建一个名为 images 的文件夹 在Typora中设置图片的相对路径 打开Typora的文件 &gt; 偏好设置 &gt; 图像，进行如下设置： 此设置会使source/images文件夹下新增一个与所编辑的markdown文档同名的文件夹，文档中所添加的 本地图片 都将存档于此（即拥有了如下路径：'hexo根目录'/source/images/'md文档名'/'图片名称'）)。 撰写markdown文档时配置 图片根目录 ，使其能够同步到hexo博客中去 撰写博文时，先点击Typora菜单栏中的格式 &gt; 图像 &gt; 设置图片根目录 , 将根目录配置为'hexo根目录'/source。然后再撰写博文。【注：每篇需要添加本地图片的博文都要先进行此步骤】 【编写博客时】— 图片导入方法 直接拖拽 将原本存放于其他本地文件夹中的图片直接拖拽到文档中的相应位置中去 此时图片会被自动存档至生成的同名文件夹'hexo根目录'/source/images/'md文档名'中 文档中图片地址的代码会显示成 自动生成的相对路径，即/images/'md文档名'/'图片名称' 利用相对路径调取 当利用 方法1 插入了至少一张图片时（即已生成同名文件夹时），便可以把接下来要插入的图片复制到此同名文件夹中，在文档中利用相对路径 调取图片： 所使用的命令是：![图片显示名称](/images/'md文档名'/'图片名称') 这里的图片显示名称不必与文件夹中保存的图片名称保持一致，'图片名称'中要记得包含图片格式（例如：tupian.jpg 或 picture.png 等） 【注意】当还没有利用 方法1 插入过图片时（即同名文件夹尚未生成时），不可以自己创建同名文件夹保存图片。亲测不好使！！（.md文档中可以显示，但是hexo博文中无法显示） 【编写博客后】— 图片存档结果 在利用上述方法完成了含有本地图片的markdown博文后，我们的资源文件夹'hexo根目录'/source/images/内最终会显示成什么样子呢？ 每一篇配置了图片根目录的博文（即【编写博客前】的第3步），都会在'hexo根目录'/source/images/文件夹中有一个与文档名称同名的文件夹'hexo根目录'/source/images/'md文档名' 该文件夹中会保存博文编写中曾经添加的所有本地图片 所有的含义是：即使编辑过程中某些本地图片在添加后又被删除了，它们也仍然会保留在文件夹中，即该文件夹会备份你在博文中添加的 所有本地图片历史 本地图片的含义是：这里只会保存插入的本地图片，而不会保存插入的网络图片。尽管在【编写博客前】的第2步配置中，我们也同样勾选了对网络位置的图片应用上述规则。（请原谅我并不知道其中的缘由。。） 就此，在Typora编辑器中编写Hexo博文时，向markdown文档中添加本地图片的方法就介绍完毕啦！快去应用到你的博文中去吧~ 本文参考了yinyoupoet的typora + hexo博客中插入图片 更多关于Typora中插入图片的内容可以参考Typora的官方说明 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【USAGE】","slug":"【USAGE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90USAGE%E3%80%91/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://hyemin-kim.github.io/tags/Hexo/"},{"name":"Typora","slug":"Typora","permalink":"https://hyemin-kim.github.io/tags/Typora/"},{"name":"Markdown","slug":"Markdown","permalink":"https://hyemin-kim.github.io/tags/Markdown/"}]},{"title":"Markdown 常用语法（持续更新）","slug":"Markdown-Syntax","date":"2020-05-03T16:40:07.372Z","updated":"2020-11-10T10:28:38.399Z","comments":true,"path":"2020/05/04/Markdown-Syntax/","link":"","permalink":"https://hyemin-kim.github.io/2020/05/04/Markdown-Syntax/","excerpt":"","text":"Markdown 常用语法 标题 一级标题： “#” + 空格 + “一级标题” 二级标题： “##” + 空格 + “二级标题” 三级标题： “###” + 空格 + “三级标题” …… 以此类推 【最多到6级】 换行 “内容” 末尾 + 2个空格 + Enter 斜体 方法一：“内容”前后加1个 * 号（无空格） 方法二：“内容”前后加1个下划线（无空格） *“内容” * ——&gt; “内容” _ “内容” _ ——&gt; 内容 加粗 方法一：“内容”前后加2个 * 号（无空格） 方法二：“内容”前后加2个下划线（无空格） ** “内容” ** ——&gt; \"内容\" __ “内容” __ ——&gt; “内容” 斜体加粗 “内容”前后加 3 个 * 号 （无空格） “内容” 删除线 ”内容”前后加 2 个波浪线（~） ~~ “内容” ~~ ——&gt; “内容” 高亮 “内容”前后加 2 个 = 号 == “内容” == ——&gt; “内容” 字体，颜色，字号 使用 font 标签 1&lt;font face='Microsift Yahei' color='red' size='6'&gt; 字体，颜色和字号 &lt;/font&gt; 字体，颜色和字号 上标 &amp; 下标 上标：“内容”前后加 1 个 ^ 号 下标：“内容”前后加 1 个 ~ 号 我是 ^ 上标 ^ ——&gt; 我是上标 我是 ~ 下标 ~ ——&gt; 我是下标 引用 “内容”前加 &gt; 号 “内容” 引用号可叠用，&gt;号越多，级数越低 例如：可以使用&gt;, &gt;&gt;, &gt;&gt;&gt; 的形式 一级引用 二级引用 三级引用 文字内容对齐设置 1. 使用div标签： 1&lt;div style=\"text-align: right\"&gt;your-text-here&lt;/div&gt; 居左 居中 居右 2. 使用p标签：(在Jupyter Notebook中不适用) 居中：&lt;center&gt; 内容 &lt;/center&gt; 居左/居右：&lt;p align='left'&gt; 内容 &lt;/p&gt; 居左 居中 居右 插入链接 ​ 中括号内输入“显示的文字”，紧接着小括号内输入“网址链接” ​ 【注意：网站地址需要 http 开头，最好直接复制】 点我进入百度 插入图片 ​ 感叹号 + 中括号内输入“显示的文字”，紧接着小括号内输入“图片链接” ​ 【注意：图片链接非网页的网址栏链接，而是右键“复制图片地址”得到的链接 (Chrome)】 调整图片大小： 1&lt;img src=\"链接\" width=\"宽度(数字or百分比)\" height=\"高度\" alt=\"图片名称\" align=center/left/right&gt; 列表 （1） 有序列表 ​ （序号1+点+空格）+内容+回车 ​ （序号2+点+空格）+内容+回车 ​ （序号3+点+空格）+内容+回车 第一行 第二行 第三行 ​ 【注意】：系统会默认调整有序列表的序列数。即，即使你误输入成了1.，2.，4.，系统也会自动更正为 1.，2.，3. 第一点 第二点 第四点 （2）无序列表 ​ 使用“ + ”+空格+内容 ​ ​ 或者“ - ”+空格+内容 ​ ​ 或者“ * ”+空格+内容 ​ 下一级：前面加 tab 第一章 第二章 第三章 第一节 （3）任务列表 ​ 短横线 + 1 个空格 + 中括号（括号中间带 1 个空格） + 1 个空格 + “内容” [x] 学习python [ ] 学习SQL 表格 添加表格 竖线作为列分界线，换行竖线中间输入短横线作为行分界线 表格内容对齐 左对齐 居中 右对齐 :----- :-----: -----: 表格内容换行 插入 &lt;br&gt; 代码 三个 ` 号，再输入所使用的编程语言 1print(\"Python\") # python 1install.packages(\"ggplot2\") # R语言 插入目录 [Only for Typora] 中括号内输入toc In Hexo: @[toc] (在使用hexo-renderer-markdown-it-plus插件时) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[{"name":"【USAGE】","slug":"【USAGE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90USAGE%E3%80%91/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"https://hyemin-kim.github.io/tags/Markdown/"}]}],"categories":[{"name":"【STUDY - SQL】","slug":"【STUDY-SQL】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/"},{"name":"SQL - 9. Table","slug":"【STUDY-SQL】/SQL-9-Table","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-9-Table/"},{"name":"SQL - 8. Manipulation","slug":"【STUDY-SQL】/SQL-8-Manipulation","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-8-Manipulation/"},{"name":"【EXERCISE】","slug":"【EXERCISE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/"},{"name":"SQL","slug":"【EXERCISE】/SQL","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/SQL/"},{"name":"SQL - 7. SubQuery","slug":"【STUDY-SQL】/SQL-7-SubQuery","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-7-SubQuery/"},{"name":"SQL - 6. Aggregate Operations","slug":"【STUDY-SQL】/SQL-6-Aggregate-Operations","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-6-Aggregate-Operations/"},{"name":"SQL - 5. Analytic Function","slug":"【STUDY-SQL】/SQL-5-Analytic-Function","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-5-Analytic-Function/"},{"name":"SQL - 4. Aggregate Function","slug":"【STUDY-SQL】/SQL-4-Aggregate-Function","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-4-Aggregate-Function/"},{"name":"SQL - 3. Join","slug":"【STUDY-SQL】/SQL-3-Join","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-3-Join/"},{"name":"SQL - 2. Data Filtering","slug":"【STUDY-SQL】/SQL-2-Data-Filtering","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-2-Data-Filtering/"},{"name":"SQL - 1. Data Selecting","slug":"【STUDY-SQL】/SQL-1-Data-Selecting","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-SQL%E3%80%91/SQL-1-Data-Selecting/"},{"name":"Python","slug":"【EXERCISE】/Python","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90EXERCISE%E3%80%91/Python/"},{"name":"【STUDY - Python】","slug":"【STUDY-Python】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/"},{"name":"Python - Text Mining","slug":"【STUDY-Python】/Python-Text-Mining","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-Text-Mining/"},{"name":"Python - Machine Learning","slug":"【STUDY-Python】/Python-Machine-Learning","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-Machine-Learning/"},{"name":"Python - 4. Seaborn","slug":"【STUDY-Python】/Python-4-Seaborn","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-4-Seaborn/"},{"name":"Python - 시각화","slug":"【STUDY-Python】/Python-시각화","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%8B%9C%EA%B0%81%ED%99%94/"},{"name":"Python - 3. Matplotlib","slug":"【STUDY-Python】/Python-3-Matplotlib","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-3-Matplotlib/"},{"name":"Python - 2. Pandas","slug":"【STUDY-Python】/Python-2-Pandas","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-2-Pandas/"},{"name":"Python - 전처리","slug":"【STUDY-Python】/Python-전처리","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-%EC%A0%84%EC%B2%98%EB%A6%AC/"},{"name":"Python - 1. Numpy","slug":"【STUDY-Python】/Python-1-Numpy","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-1-Numpy/"},{"name":"Python - 0. Base","slug":"【STUDY-Python】/Python-0-Base","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90STUDY-Python%E3%80%91/Python-0-Base/"},{"name":"【USAGE】","slug":"【USAGE】","permalink":"https://hyemin-kim.github.io/categories/%E3%80%90USAGE%E3%80%91/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://hyemin-kim.github.io/tags/SQL/"},{"name":"Manipulation","slug":"Manipulation","permalink":"https://hyemin-kim.github.io/tags/Manipulation/"},{"name":"SubQuery","slug":"SubQuery","permalink":"https://hyemin-kim.github.io/tags/SubQuery/"},{"name":"Join","slug":"Join","permalink":"https://hyemin-kim.github.io/tags/Join/"},{"name":"Aggregate","slug":"Aggregate","permalink":"https://hyemin-kim.github.io/tags/Aggregate/"},{"name":"Analytic Function","slug":"Analytic-Function","permalink":"https://hyemin-kim.github.io/tags/Analytic-Function/"},{"name":"Selecting","slug":"Selecting","permalink":"https://hyemin-kim.github.io/tags/Selecting/"},{"name":"Filtering","slug":"Filtering","permalink":"https://hyemin-kim.github.io/tags/Filtering/"},{"name":"Python","slug":"Python","permalink":"https://hyemin-kim.github.io/tags/Python/"},{"name":"Text Mining","slug":"Text-Mining","permalink":"https://hyemin-kim.github.io/tags/Text-Mining/"},{"name":"sklearn","slug":"sklearn","permalink":"https://hyemin-kim.github.io/tags/sklearn/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://hyemin-kim.github.io/tags/Machine-Learning/"},{"name":"분류","slug":"분류","permalink":"https://hyemin-kim.github.io/tags/%EB%B6%84%EB%A5%98/"},{"name":"회귀","slug":"회귀","permalink":"https://hyemin-kim.github.io/tags/%ED%9A%8C%EA%B7%80/"},{"name":"비지도 학습","slug":"비지도-학습","permalink":"https://hyemin-kim.github.io/tags/%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5/"},{"name":"앙상블","slug":"앙상블","permalink":"https://hyemin-kim.github.io/tags/%EC%95%99%EC%83%81%EB%B8%94/"},{"name":"전처리","slug":"전처리","permalink":"https://hyemin-kim.github.io/tags/%EC%A0%84%EC%B2%98%EB%A6%AC/"},{"name":"시각화","slug":"시각화","permalink":"https://hyemin-kim.github.io/tags/%EC%8B%9C%EA%B0%81%ED%99%94/"},{"name":"Seaborn","slug":"Seaborn","permalink":"https://hyemin-kim.github.io/tags/Seaborn/"},{"name":"Matplotlib","slug":"Matplotlib","permalink":"https://hyemin-kim.github.io/tags/Matplotlib/"},{"name":"사각화","slug":"사각화","permalink":"https://hyemin-kim.github.io/tags/%EC%82%AC%EA%B0%81%ED%99%94/"},{"name":"Pandas","slug":"Pandas","permalink":"https://hyemin-kim.github.io/tags/Pandas/"},{"name":"데이터파악","slug":"데이터파악","permalink":"https://hyemin-kim.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%8C%8C%EC%95%85/"},{"name":"Numpy","slug":"Numpy","permalink":"https://hyemin-kim.github.io/tags/Numpy/"},{"name":"Python_Base","slug":"Python-Base","permalink":"https://hyemin-kim.github.io/tags/Python-Base/"},{"name":"Python - Base","slug":"Python-Base","permalink":"https://hyemin-kim.github.io/tags/Python-Base/"},{"name":"Hexo","slug":"Hexo","permalink":"https://hyemin-kim.github.io/tags/Hexo/"},{"name":"Typora","slug":"Typora","permalink":"https://hyemin-kim.github.io/tags/Typora/"},{"name":"Markdown","slug":"Markdown","permalink":"https://hyemin-kim.github.io/tags/Markdown/"},{"name":"Git","slug":"Git","permalink":"https://hyemin-kim.github.io/tags/Git/"},{"name":"Github","slug":"Github","permalink":"https://hyemin-kim.github.io/tags/Github/"},{"name":"Jupyter notebook","slug":"Jupyter-notebook","permalink":"https://hyemin-kim.github.io/tags/Jupyter-notebook/"}]}